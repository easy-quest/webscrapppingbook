**Инициализация нового «паука». ** После установки

платформы Scrapy необходимо выполнить небольшую

настройку для каждого *«паука»* \(spider\) — проекта Scrapy, который, как и обычный паук, занимается обходом сети. В этой

главе я буду называть «пауком» именно проект Scrapy, а

краулером — любую программу, которая занимается сбором

данных во Всемирной паутине, независимо от того, использует

она Scrapy или нет. 

Чтобы создать нового «паука» в текущем каталоге, нужно

ввести в командной строке следующую команду:

$ scrapy startproject wikiSpider

В результате в каталоге будет создан новый подкаталог, а в

нем — проект под названием wikiSpider. Внутри этого

каталога находится следующая файловая структура: scrapy.cfg

wikiSpider

— spiders

— \_\_init.py\_\_

— items.py

— middlewares.py

— pipelines.py

— settings.py

— \_\_init.py\_\_

Поначалу в эти файлы Python записывается код-заглушка, что позволяет быстро создать новый проект «паука». В

следующих разделах данной главы мы продолжим работать с

проектом wikiSpider. 

**Пишем простой веб-скрапер**

Чтобы создать веб-краулер, нужно добавить в дочерний

каталог 

**wikiSpider** 

новый 

файл

wikiSpider/wikiSpider/article.py. Затем в этом файле

article.py нужно написать следующее:

import scrapy

 

class ArticleSpider\(scrapy.Spider\):

name='article' 

 

def start\_requests\(self\):

urls = \[

'http://en.wikipedia.org/wiki/Pytho

n\_' 

'%28programming\_language%29', 

'https://en.wikipedia.org/wiki/Func

tional\_programming', 

'https://en.wikipedia.org/wiki/Mont

y\_Python'\]

return \[scrapy.Request\(url=url, 

callback=self.parse\)

for url in urls\]

 

def parse\(self, response\):

url = response.url



 

 

 

 

 

 

 

title 

=

response.css\('h1::text'\).extract\_first\(\)

print\('URL is: \{\}'.format\(url\)\)

print\('Title is: \{\}'.format\(title\)\)

Имя этого класса \(ArticleSpider\) отличается от имени

каталога \(**wikiSpider**\), что указывает на следующее: данный

класс отвечает за просмотр только страниц статей в рамках

более широкой категории **wikiSpider**, которую мы впоследствии

сможем использовать для поиска других типов страниц. 

Для больших сайтов с разными типами контента можно

создать отдельные элементы Scrapy для каждого типа

\(публикации в блогах, пресс-релизы, статьи и т.п.\). У каждого

из этих типов будут свои поля, но все они станут работать в

одном проекте Scrapy. Имя каждого «паука» должно быть

уникальным в рамках проекта. 

Следует обратить внимание еще на две важные вещи: функции start\_requests и parse. 

Функция start\_requests — это предопределенная Scrapy точка входа в программу, используемая для генерации

объектов Request, которые в Scrapy применяются для сбора

данных с сайта. 

Функция parse — это функция обратного вызова, определяемая пользователем, которая передается в объект

Request с помощью callback=self.parse. Позже мы

рассмотрим более мощные трюки, которые возможны

благодаря функции parse, но пока что она просто выводит

заголовок страницы. 

Для запуска «паука» article нужно перейти в каталог

**wikiSpider/wikiSpider** и выполнить следующую команду: $ scrapy runspider article.py

По умолчанию Scrapy выводит довольно подробные данные. 

Помимо отладочной информации, это будут примерно такие

строки:

2018-01-21 23:28:57 \[scrapy.core.engine\] DEBUG:

Crawled \(200\)

<GET 

https://en.wikipedia.org/robots.txt> 

\(referer: None\)

2018-01-21 

23:28:57

\[scrapy.downloadermiddlewares.redirect\]

DEBUG: 

Redirecting 

\(301\) 

to 

<GET

https://en.wikipedia.org/wiki/

Python\_%28programming\_language%29> from <GET

http://en.wikipedia.org/

wiki/Python\_%28programming\_language%29> 

2018-01-21 23:28:57 \[scrapy.core.engine\] DEBUG:

Crawled \(200\)

<GET

https://en.wikipedia.org/wiki/Functional\_progra

mming> 

\(referer: None\)

URL 

is:

https://en.wikipedia.org/wiki/Functional\_progra

mming

Title is: Functional programming

2018-01-21 23:28:57 \[scrapy.core.engine\] DEBUG:

Crawled \(200\)

<GET

https://en.wikipedia.org/wiki/Monty\_Python> 

\(referer: None\)

URL 

is:

https://en.wikipedia.org/wiki/Monty\_Python

Title is: Monty Python

![Image 39](images/000073.png)

Веб-скрапер проходит по трем страницам, указанным в

списке urls, собирает с них информацию и завершает работу. 

**«Паук» с правилами**

«Паук», созданный в предыдущем разделе, не очень-то похож

на веб-краулер, так как ограничен лишь списком

предоставленных ему URL. Он не умеет самостоятельно искать

новые страницы. Чтобы превратить этого «паука» в

полноценный 

веб-краулер, 

нужно 

задействовать

предоставляемый Scrapy класс CrawlSpider. 

**Где искать этот код в репозитории GitHub**

К сожалению, фреймворк Scrapy нельзя просто запустить из

редактора Jupyter, что затрудняет фиксацию внесения

изменений в код. Для представления примеров кода в тексте

главы мы сохранили веб-скрапер из предыдущего раздела в

файле article.py, а следующий пример создания «паука» Scrapy, перебирающего несколько страниц, — в файле articles.py \(обратите внимание на использование буквы s в конце слова

articles\). 

Последующие примеры также будут храниться в отдельных

файлах со своими именами, которые будут приводиться в

соответствующих разделах. Убедитесь, что используете

правильные имена файлов при запуске этих примеров. 

Следующий класс находится в файле articles.py в

репозитории GitHub:

from 

scrapy.contrib.linkextractors 

import

LinkExtractor

from scrapy.contrib.spiders import CrawlSpider, 

Rule

 

class ArticleSpider\(CrawlSpider\):

name = 'articles' 

allowed\_domains = \['wikipedia.org'\]

 

 

 

 

start\_urls 

=

\['https://en.wikipedia.org/wiki/' 

'Benevolent\_dictator\_for\_life'\]

rules = \[Rule\(LinkExtractor\(allow=r'.\*'\), 

callback='parse\_items', 

follow=True\)\]

 

def parse\_items\(self, response\):

url = response.url

 

 

 

 

 

 

 

 

title 

=

response.css\('h1::text'\).extract\_first\(\)

text = response.xpath\('//div\[@id="mw-

content-text"\]//text\(\)' 

\).extract\(\)

lastUpdated = response.css\('li\#footer-

info-lastmod::text' 

\).extract\_first\(\)

lastUpdated = lastUpdated.replace\(

'This page was last edited on ', 

''\)

print\('URL is: \{\}'.format\(url\)\)

print\('title is: \{\} '.format\(title\)\) print\('text is: \{\}'.format\(text\)\)

 

 

 

 

 

 

 

 

print\('Last 

updated:

\{\}'.format\(lastUpdated\)\)

Этот новый «паук» ArticleSpider является наследником

класса CrawlSpider. Вместо функции start\_requests он

предоставляет списки start\_urls и allowed\_domains, которые сообщают «пауку», с чего начать обход сети и следует

ли переходить по ссылке или игнорировать ее, в зависимости

от имени домена. 

Предоставляется и список rules, в котором содержатся

дальнейшие инструкции: по каким ссылкам переходить, а

какие — игнорировать \(в данном случае с помощью

регулярного выражения .\* мы разрешаем переходить по всем

URL\). 

Помимо заголовка и URL на каждой странице, здесь

добавлено извлечение еще пары элементов. С помощью

селектора XPath извлекается текстовый контент каждой

страницы. XPath часто используется для извлечения текста, включая содержимое дочерних тегов \(например, тега <a>, расположенного внутри текстового блока\). Если для этого

использовать CSS-селектор, то контент всех дочерних тегов

будет игнорироваться. 

Анализируется также строка с датой последнего изменения

из нижнего колонтитула страницы, которая сохраняется в

переменной lastUpdated. 

Чтобы запустить этот пример, нужно перейти в каталог

**wikiSpider/wikiSpider** и выполнить следующую команду: $ scrapy runspider articles.py

![Image 40](images/000024.png)

**Предупреждение: он будет работать вечно**

Этот «паук», как и предыдущий, запускается из командной

строки. Однако он не прекратит работу \(по крайней мере, будет

работать очень, очень долго\), пока вы не остановите его

выполнение, нажав Ctrl\+C или закрыв окно терминала. Прошу, пожалейте сервер «Википедии», не выполняйте данную

программу слишком долго. 

При запуске этот «паук» проходит по сайту **wikipedia.org**, переходя по всем ссылкам домена **wikipedia.org**, выводя

заголовки страниц и игнорируя все внешние ссылки \(ведущие

на другие сайты\):

2018-01-21 

01:30:36

\[scrapy.spidermiddlewares.offsite\]

DEBUG: 

Filtered 

offsite 

request 

to

'www.chicagomag.com':

<GET 

http://www.chicagomag.com/Chicago-

Magazine/June-2009/

Street-Wise/> 

2018-01-21 

01:30:36

\[scrapy.downloadermiddlewares.robotstxt\]

DEBUG: 

Forbidden 

by 

robots.txt: 

<GET

https://en.wikipedia.org/w/

index.php? 

title=Adrian\_Holovaty&action=edit&section=3> title is: Ruby on Rails

URL 

is:

https://en.wikipedia.org/wiki/Ruby\_on\_Rails

text is: \['Not to be confused with ', 'Ruby \(programming language\)', 

'.', '\\n', '\\n', 'Ruby on Rails', ... \]

Last updated: 9 January 2018, at 10:32. 

Пока данный веб-краулер работает неплохо, однако у него

есть несколько ограничений. Вместо того чтобы посещать

только страницы статей «Википедии», он вполне может

переходить на страницы, не относящиеся к статьям, такие как

эта:

title is: Wikipedia:General disclaimer

Рассмотрим внимательнее следующую строку кода, где

используются объекты Scrapy Rule и LinkExtractor: rules 

= 

\[Rule\(LinkExtractor\(allow=r'.\*'\), 

callback='parse\_items', 

follow=True\)\]

В данной строке содержится список объектов Scrapy Rule, определяющих правила, по которым фильтруются все

найденные ссылки. При наличии нескольких правил каждая

ссылка проверяется на соответствие им в порядке их

перечисления. Первое подходящее правило — то, которое

используется для определения способа обработки ссылки. Если

ссылка не соответствует ни одному из правил, то игнорируется. 

Правило может быть описано следующими четырьмя

аргументами:

•link\_extractor — единственный обязательный аргумент —

объект LinkExtractor; 

• callback — функция обратного вызова, которая должна

использоваться для анализа контента страницы; 

• cb\_kwargs — словарь аргументов, передаваемых в

функцию 

обратного 

вызова. 

Имеет 

вид

\{arg\_name1:arg\_value1,arg\_name2:arg\_value2\} 

и

может быть удобным инструментом для многократного

использования одних и тех же функций синтаксического

анализа в незначительно различающихся задачах; 

• follow — указывает на то, хотите ли вы, чтобы веб-краулер

обработал также страницы по ссылкам, найденным на

данной странице. Если функция обратного вызова не

указана, то по умолчанию используется значение True \(в

конце концов, при отсутствии действий с этой страницей

логично предположить, что вы по крайней мере захотите

применить ее для продолжения сбора данных с сайта\). Если

предусмотрена функция обратного вызова, то по умолчанию

используется значение False. 

Простой 

класс 

LinkExtractor 

предназначен

исключительно для распознавания и возврата ссылок, обнаруженных в HTML-контенте страницы на основе

предоставленных этому классу правил. Имеет ряд аргументов, которые можно использовать для того, чтобы принимать или

отклонять ссылки на основе CSS-селекторов и XPath, тегов

\(можно искать ссылки не только в тегах <a>\!\), доменов и др. 

От класса LinkExtractor можно унаследовать свой, в

котором можно добавить нужные аргументы. Подробнее об

извлечении 

ссылок 

см. 

в 

документации 

Scrapy

\(**https://doc.scrapy.org/en/latest/topics/link-extractors.html**\). 

Несмотря на всю гибкость свойств класса LinkExtractor, самыми распространенными аргументами, которые вы, скорее

всего, будете использовать, являются следующие:

•allow — разрешить проверку всех ссылок, соответствующих

заданному регулярному выражению; 

• deny — запретить проверку всех ссылок, соответствующих

заданному регулярному выражению. 

Используя два отдельных класса Rule и LinkExtractor с

общей функцией синтаксического анализа, можно создать

«паука», который бы собирал данные в «Википедии», идентифицируя все страницы статей и помечая остальные

страницы флагом \(articleMoreRules.py\):

from 

scrapy.contrib.linkextractors 

import

LinkExtractor

from scrapy.contrib.spiders import CrawlSpider, 

Rule

 

class ArticleSpider\(CrawlSpider\):

name = 'articles' 

allowed\_domains = \['wikipedia.org'\]

 

 

 

 

start\_urls 

=

\['https://en.wikipedia.org/wiki/' 

'Benevolent\_dictator\_for\_life'\]

rules = \[

Rule\(LinkExtractor\(allow='^\(/wiki/\)

\(\(?\!:\).\)\*$'\), 

callback='parse\_items', 

follow=True, 

cb\_kwargs=\{'is\_article': True\}\), Rule\(LinkExtractor\(allow='.\*'\), 

callback='parse\_items', 

cb\_kwargs=\{'is\_article': False\}\)

\]

 

 

 

 

 

def 

parse\_items\(self, 

response, 

is\_article\):

print\(response.url\)

 

 

 

 

 

 

 

 

title 

=

response.css\('h1::text'\).extract\_first\(\)

if is\_article:

url = response.url

 

 

 

 

 

 

 

 

 

 

 

 

text 

=

response.xpath\('//div\[@id="mw-content-text"\]' 

'//text\(\)'\).extract\(\)

lastUpdated =

response.css\('li\#footer-info-lastmod' 

'::text'\).extract\_first\(\)

lastUpdated =

lastUpdated.replace\('This page was ' 

'last edited on ', ''\)

print\('Title is: \{\}

'.format\(title\)\)

print\('title is: \{\}

'.format\(title\)\)

print\('text is: \{\}'.format\(text\)\)

else:

print\('This is not an article:

\{\}'.format\(title\)\)

Напомню, что правила применяются к каждой ссылке в том

порядке, в котором они представлены в списке. Все страницы

статей \(страницы, URL которых начинается с /wiki/ и не

содержит двоеточий\) передаются в функцию parse\_items с

аргументом is\_article=True. Все остальные ссылки \(не на

статьи\) передаются в функцию parse\_items с аргументом

is\_article=False. 

Конечно, если вы хотите собирать информацию только со

страниц статей и игнорировать все остальные, то такой подход

был бы непрактичным. Намного проще игнорировать все

страницы, которые не соответствуют структуре URL страниц со

статьями, и вообще не определять второе правило \(и

переменную is\_article\). Однако бывают случаи, когда

подобный подход может быть полезен — например, если

информация из URL или данные, собранные при крау линге, влияют на способ синтаксического анализа страницы. 

**Создание объектов Item**

Мы 

уже 

рассмотрели 

множество 

способов 

поиска, 

синтаксического анализа и краулинга сайтов с помощью

Scrapy, однако эта платформа также предоставляет полезные

инструменты для упорядочения собранных элементов и их

хранения в созданных пользователем объектах с четко

определенными полями. 

Чтобы упорядочить всю собираемую информацию, нужно

создать объект Article. Определим этот новый элемент с

именем Article в файле items.py. 

Когда мы впервые открываем файл items.py, он выглядит

так:

\# -\*- coding: utf-8 -\*-



\# Здесь определяются модели объектов, для

которых выполняется веб-скрапинг. 

\#

\# Подробнее см. документацию:

\#

http://doc.scrapy.org/en/latest/topics/items.ht

ml

 

import scrapy

 

class WikispiderItem\(scrapy.Item\):

\# Определяем поля для ваших объектов, как

здесь:

\# name = scrapy.Field\(\)

pass

Заменим этот предоставляемый по умолчанию код-заглушку Item на новый класс Article, который является

расширением scrapy.Item:

import scrapy

 

class Article\(scrapy.Item\):

url = scrapy.Field\(\)

title = scrapy.Field\(\)

text = scrapy.Field\(\)

lastUpdated = scrapy.Field\(\)

Мы определили три поля, в которые будут собираться

данные с каждой страницы: заголовок, URL и дату последнего

редактирования страницы. 

Если мы собираем данные для страниц разных типов, то

нужно определить каждый тип как отдельный класс в

items.py. Если собираемые объекты Item имеют очень

большие размеры или если мы захотим перенести в их Item дополнительные функции синтаксического анализа, то можно

разместить каждый такой объект в отдельном файле. Однако, поскольку наши объекты Item невелики, мне нравится хранить

их в одном файле. 

Обратите внимание на изменения, которые были внесены в

класс ArticleSpider в файле articleItems.py, чтобы

можно было создать новый объект Article:

from 

scrapy.contrib.linkextractors 

import

LinkExtractor

from scrapy.contrib.spiders import CrawlSpider, 

Rule

from wikiSpider.items import Article

 

class ArticleSpider\(CrawlSpider\):

name = 'articleItems' 

allowed\_domains = \['wikipedia.org'\]

 

 

 

 

start\_urls 

=

\['https://en.wikipedia.org/wiki/Benevolent' 

'\_dictator\_for\_life'\]

rules = \[

Rule\(LinkExtractor\(allow='\(/wiki/\)

\(\(?\!:\).\)\*$'\), 

callback='parse\_items', follow=True\), 

\]

 

def parse\_items\(self, response\):

article = Article\(\)

article\['url'\] = response.url article\['title'\] 

=

response.css\('h1::text'\).extract\_first\(\)

 

 

 

 

 

 

 

 

article\['text'\] 

=

response.xpath\('//div\[@id=' 

'"mw-content-

text"\]//text\(\)'\).extract\(\)

lastUpdated = response.css\('li\#footer-

info-lastmod::text' 

\).extract\_first\(\)

article\['lastUpdated'\] =

lastUpdated.replace\('This page was ' 

'last edited on ', ''\)

return article

При запуске этого файла с помощью команды:

$ scrapy runspider articleItems.py

получим обычные отладочные данные Scrapy, а также

список всех объектов статей в виде словаря Python: 2018-01-21 

22:52:38

\[scrapy.spidermiddlewares.offsite\] DEBUG:

Filtered 

offsite 

request 

to

'wikimediafoundation.org':

<GET

https://wikimediafoundation.org/wiki/Terms\_of\_U

se> 

2018-01-21 22:52:38 \[scrapy.core.engine\] DEBUG:

Crawled \(200\)

<GET

https://en.wikipedia.org/wiki/Benevolent\_dictat

or\_for\_life

\#mw-head> 

\(referer:

https://en.wikipedia.org/wiki/Benevolent\_

dictator\_for\_life\)

2018-01-21 

22:52:38 

\[scrapy.core.scraper\]

DEBUG: Scraped from

<200

https://en.wikipedia.org/wiki/Benevolent\_dictat

or\_for\_life> 

\{'lastUpdated': ' 13 December 2017, at 09:26.', 

'text': \['For the political term, see ', 

'Benevolent dictatorship', 

'.', 

... 

Использование объектов Item в Scrapy не только

обеспечивает хорошую упорядоченность кода и представление

объектов в удобно читаемом виде. У объектов Item есть

множество инструментов для вывода и обработки данных, о

которых пойдет речь в следующих разделах. 

**Вывод объектов Item**

В Scrapy объекты Item позволяют определить, какие

фрагменты информации из посещенных страниц следует

сохранять. Scrapy может сохранять эту информацию

различными способами, такими как файлы форматов CSV, JSON или XML, с помощью следующих команд:

$ 

scrapy 

runspider 

articleItems.py 

-o

articles.csv -t csv

$ 

scrapy 

runspider 

articleItems.py 

-o

articles.json -t json

$ 

scrapy 

runspider 

articleItems.py 

-o

articles.xml -t xml

Каждая 

из 

этих 

команд 

запускает 

веб-скрапер

articleItems и записывает полученные данные в заданном

файле в указанном формате. Если такого файла еще не

существует, то он будет создан. 

Вы могли заметить, что в «пауке» ArticleSpider, созданном в предыдущих примерах, переменная текстовых

данных представляет собой не одну строку, а их список. Каждая

строка в нем соответствует тексту, расположенному в одном из

элементов HTML, тогда как контент тега <divid="mw-content-text">, из которого мы собираем текстовые данные, представляет собой множество дочерних элементов. 

Scrapy 

хорошо 

управляет 

этими 

усложненными

значениями. Например, при сохранении данных в формате

CSV списки преобразуются в строки, а запятые экранируются, так что список текстовых фрагментов в формате CSV занимает

одну ячейку. 

В XML каждый элемент этого списка сохраняется в

отдельном дочернем теге:

<items> 

<item> 

<url>https://en.wikipedia.org/wiki/Benevole nt\_dictator\_for\_life</url> 

<title>Benevolent dictator for life</title> 

<text> 

<value>For the political term, see

</value> 

<value>Benevolent dictatorship</value> 

... 

</text> 

<lastUpdated> 13 December 2017, at 09:26. 

</lastUpdated> 

</item> 

.... 

В формате JSON списки так и сохраняются в виде списков. 

И конечно же, мы можем обрабатывать объекты Item самостоятельно, записывая их в файл или базу данных любым

удобным способом, просто добавив соответствующий код в

функцию синтаксического анализа в веб-краулере. 

**Динамический конвейер**

Несмотря на то что Scrapy является однопоточной

библиотекой, она способна выполнять и обрабатывать

несколько запросов асинхронно, благодаря чему работает

быстрее, чем веб-скраперы, рассмотренные ранее в этой книге. 

Впрочем, я продолжаю настаивать на своем мнении: когда речь

идет о веб-скрапинге, быстрее не всегда значит лучше. 

Веб-сервер сайта, на котором выполняется веб-скрапинг, должен обработать каждый ваш запрос. Важно иметь совесть и

трезво оценивать, насколько приемлема создаваемая вами

нагрузка на сервер \(и не только совесть, но и мудрость — это в

ваших собственных интересах, поскольку многие сайты имеют

возможность и желание заблокировать то, что посчитают

злостным веб-скрапингом\). Подробнее об этике веб-скрапинга, а также о важности соответствующей настройки веб-скраперов

см. в главе 18. 

С учетом всего этого использование динамического

конвейера в Scrapy позволяет еще более повысить скорость

работы веб-скрапера, так как вся обработка данных будет

выполняться в то время, пока веб-скрапер ожидает ответа на

запросы, вместо того чтобы дожидаться окончания обработки

данных и только потом выполнять очередной запрос. Такой

тип оптимизации иногда бывает просто необходим — если

обработка данных занимает много времени или требуется

выполнить вычисления, создающие значительную нагрузку на

процессор. 

Чтобы создать динамический конвейер, вернемся к файлу

settings.py \(см. начало главы\). В нем содержатся следующие

строки кода, заблокированные символами комментариев:

\# Configure item pipelines

\# 

See

http://scrapy.readthedocs.org/en/latest/topics/

item-pipeline.html

\#ITEM\_PIPELINES = \{

\# 'wikiSpider.pipelines.WikispiderPipeline':

300, 

\#\}

Уберем символы комментариев из последних трех строк и

получим следующее:

ITEM\_PIPELINES = \{

'wikiSpider.pipelines.WikispiderPipeline':

300, 

\}

Таким 

образом, 

мы 

создали 

класс 

Python

wikiSpider.pipelines.Wikispider 

Pipe 

line, 

который

будет служить для обработки данных, а также указали целое

число, соответствующее порядку запуска конвейера при

наличии нескольких классов обработки. Здесь можно

использовать любое целое число, однако обычно это цифры от

0 до 1000; конвейер запускается в порядке возрастания. 

Теперь нужно добавить класс конвейера в «паука» и

переписать нашего исходного «паука» таким образом, чтобы он

собирал данные, а конвейер выполнял нелегкую задачу по их

обработке. Было бы заманчиво создать в исходном «пауке»

метод parse\_items, который бы возвращал ответ, и позволить

конвейеру создавать объект Article:

def parse\_items\(self, response\):

return response

Однако фреймворк Scrapy такого не допускает: должен

возвращаться объект Item \(или Article, который является

расширением Item\). Итак, сейчас назначение parse\_items состоит в том, чтобы извлечь необработанные данные и

обработать по минимуму — лишь бы можно было передать их в

конвейер:

from 

scrapy.contrib.linkextractors 

import

LinkExtractor

from scrapy.contrib.spiders import CrawlSpider, 

Rule

from wikiSpider.items import Article

 

class ArticleSpider\(CrawlSpider\):

name = 'articlePipelines' 

allowed\_domains = \['wikipedia.org'\]

 

 

 

 

start\_urls 

=

\['https://en.wikipedia.org/wiki/Benevolent\_dict

ator\_for\_life'\]

rules = \[

Rule\(LinkExtractor\(allow='\(/wiki/\) \(\(?\!:\).\)\*$'\), 

callback='parse\_items', 

follow=True\), 

\]

 

def parse\_items\(self, response\):

article = Article\(\)

article\['url'\] = response.url

 

 

 

 

 

 

 

 

article\['title'\] 

=

response.css\('h1::text'\).extract\_first\(\)

 

 

 

 

 

 

 

 

article\['text'\] 

=

response.xpath\('//div\[@id=' 

'"mw-content-

text"\]//text\(\)'\).extract\(\)

article\['lastUpdated'\] =

response.css\('li\#' 

'footer-info-

lastmod::text'\).extract\_first\(\)

return article

Этот файл хранится в репозитории GitHub под именем

articlePipelines.py. 

Конечно, теперь нужно связать файл pipelines.py с

измененным 

«пауком», 

добавив 

конвейер. 

При

первоначальной инициализации проекта Scrapy был создан

файл wikiSpider/wikiSpider/pipelines.py:

\# -\*- coding: utf-8 -\*-

 

\# Define your item pipelines here

\#

\# Don't forget to add your pipeline to the ITEM\_PIPELINES setting

\# 

See:

http://doc.scrapy.org/en/latest/topics/item-

pipeline.html

 

class WikispiderPipeline\(object\):

def process\_item\(self, item, spider\):

return item

Этот класс-заглушку следует заменить нашим новым кодом

конвейера. В предыдущих разделах мы собирали два поля в

необработанном 

формате 

— 

lastUpdated 

\(плохо

отформатированный строковый объект, представляющий дату\) и text \(«грязный» массив строковых фрагментов\), — но для

них можно использовать дополнительную обработку. 

Вместо 

кода-заглушки 

в

wikiSpider/wikiSpider/pipelines.py 

поставим

следующий код:

from datetime import datetime

from wikiSpider.items import Article

from string import whitespace

 

class WikispiderPipeline\(object\):

def process\_item\(self, article, spider\):

dateStr = article\['lastUpdated'\]

article\['lastUpdated'\] =

article\['lastUpdated'\]

.replace\('This page was last edited

on', ''\)

article\['lastUpdated'\] =

article\['lastUpdated'\].strip\(\)

article\['lastUpdated'\] =

datetime.strptime\(

article\['lastUpdated'\], '%d %B %Y, at

%H:%M.'\)

article\['text'\] = \[line for line in

article\['text'\]

if line not in whitespace\]

 

 

 

 

 

 

 

 

article\['text'\] 

=

''.join\(article\['text'\]\)

return article

У класса WikispiderPipeline есть метод process\_item, который принимает объект Article, преобразует строку

lastUpdated в объект datetime Python, очищает текст и

трансформирует его из списка строк в одну строку. 

Метод process\_item является обязательным для любого

класса конвейера. В Scrapy этот метод используется для

асинхронной передачи объектов Item, собранных «пауком». 

Анализируемый объект Article, который возвращается в

данном случае, будет сохранен или выведен Scrapy, если, например, мы решим выводить элементы в формате JSON или

CSV, как это было сделано в предыдущем разделе. 

Теперь можно выбрать, где обрабатывать данные: в методе

parse\_items в «пауке» или в методе process\_items в

конвейере. 

В файле settings.py можно создать несколько

конвейеров, каждый из которых будет выполнять свою задачу. 

Однако Scrapy передает все элементы, независимо от их типа, во все конвейеры по порядку. Синтаксический анализ

элементов конкретных типов лучше реализовать в «пауке» и

только потом передавать данные в конвейер. Но если этот

анализ занимает много времени, то можно переместить его в

конвейер \(где он будет выполняться асинхронно\) и добавить

проверку типа элемента:

def process\_item\(self, item, spider\):

if isinstance\(item, Article\):

\# обработка объектов Article

При написании проектов Scrapy, особенно крупных, важно

учитывать, где и какую обработку вы собираетесь делать. 

**Ведение журнала Scrapy**

Отладочная информация, генерируемая Scrapy, бывает

полезна, однако, как вы могли заметить, часто чересчур

многословна. Вы можете легко настроить уровень ведения

журнала, добавив в файл settings.py проекта Scrapy следующую строку:

LOG\_LEVEL = 'ERROR' 

В Scrapy принята стандартная иерархия уровней ведения

журнала:

•CRITICAL; 

• ERROR; 

• WARNING; 

• DEBUG; 

• INFO. 

В случае уровня ведения журнала ERROR будут отображаться

только события типов CRITICAL и ERROR; при ведении

журнала на уровне INFO будут отображаться все события и т.д. 

Кроме управления ведением журнала через файл

settings.py, также можно управлять тем, куда попадают

записи из командной строки. Чтобы они выводились не на

терминал, а в отдельный файл журнала, нужно указать этот

файл при запуске Scrapy из командной строки:

$ scrapy crawl articles -s LOG\_FILE=wiki.log

Эта команда создает в текущем каталоге новый файл

журнала \(если такого еще нет\) и делает в него все журнальные

записи, оставляя терминал только для данных, выводимых

операторами Python, которые вы сами добавили в программу. 

**Дополнительные ресурсы**

Scrapy — мощный инструмент, способный решить многие

проблемы веб-краулинга. Он автоматически собирает все URL

и проверяет их на соответствие заданным правилам; гарантирует, что все адреса являются уникальными, при

необходимости нормализует относительные URL и рекурсивно

переходит на страницы более глубоких уровней. 

В этой главе мы лишь поверхностно рассмотрели

возможности Scrapy; я призываю вас заглянуть в

документацию 

по 

Scrapy

\(**https://doc.scrapy.org/en/latest/news.html**\), а также почитать

книгу *Learning Scrapy* Димитриоса Кузис-Лукаса \(Dimitrios Kouzis-Loukas\) 

\(издательство 

O’Reilly\)

\(**http://shop.oreilly.com/product/9781784399788.do**\), в которой

содержится исчерпывающее описание данного фреймворка. 

Scrapy — чрезвычайно обширная библиотека с множеством

функций. Они хорошо сочетаются между собой, но во многом

перекрывают друг друга, благодаря чему разработчик может

легко создать собственный стиль. Если есть то, что вы хотели

бы сделать с помощью Scrapy, но о чем не упоминается в этой

главе, — скорее всего, такой способ — или даже несколько —

существует\! 

**Глава 6. Хранение данных**

Несмотря на то что вывод данных на терминал доставляет

массу удовольствия, он не слишком полезен для агрегирования

и анализа данных. В большинстве случаев, чтобы работа

удаленных веб-скраперов приносила пользу, необходимо

иметь возможность сохранять получаемую от них

информацию. 

В данной главе мы рассмотрим три основных метода

управления данными, которых вам будет достаточно

практически для любого случая. Хотите запустить серверную

часть сайта или создать собственный API? Тогда, возможно, веб-скраперы должны записывать информацию в базу данных. 

Нужен быстрый и простой способ собирать документы из

Интернета и сохранять их на жестком диске? Вероятно, для

этого стоит создать файловый поток. Требуется время от

времени отправлять предупреждения или раз в день

передавать сводные данные? Тогда отправьте себе письмо по

электронной почте\! 

Возможность взаимодействовать с большими объемами

данных и хранить их невероятно важна не только для веб-скрапинга, но и практически для любого современного

программного обеспечения. В сущности, информация, представленная в данной главе, необходима для реализации

многих примеров, рассматриваемых в следующих разделах

книги. Если вы не знакомы с автоматизированным хранением

данных, то я настоятельно рекомендую вам хотя бы

просмотреть эту главу. 

**Медиафайлы**

Есть два основных способа хранения медиафайлов: сохранить

ссылку или скачать сам файл. Чтобы сохранить файл по ссылке, нужно сохранить URL, по которому находится этот файл. 

Данный способ имеет следующие преимущества:

• веб-скраперы работают намного быстрее и требуют гораздо

меньшей пропускной способности, если им не нужно

скачивать файлы; 

• сохраняя только URL, вы экономите место на своем

компьютере; 

• проще написать код, который не скачивает файлы, а лишь

сохраняет их URL; 

• избегая скачивания больших файлов, вы уменьшаете

нагрузку на сервер, на котором размещен сканируемый

ресурс. 

Но есть и недостатки. 

• Встраивание URL в ваш сайт или приложение называется

*горячими ссылками*; это легкий способ нажить себе

неприятности в Интернете. 

• Вы вряд ли захотите размещать на чужих серверах

мультимедийные ресурсы, которые используются в ваших

приложениях. 

• Файл, размещенный по определенному URL, может

измениться. Это может привести к нелепой ситуации, если, к примеру, разместить картинку, расположенную по этому

адресу, в публичном блоге. Если сохранить URL, рассчитывая

позже скачать и изучить сам файл, то однажды данный файл

могут удалить или заменить чем-нибудь совершенно

неподходящим. 

• Настоящие браузеры не просто получают HTML-код

страницы и двигаются дальше. Они также скачивают все

ресурсы, необходимые для отображения данной страницы. 

Скачивая файлы, вы делаете действия веб-скрапера больше

похожими на действия человека, который просматривает

сайт, — это может стать преимуществом. 

Принимая решение, сохранять ли сам файл или только его

URL, следует спросить себя: намерены ли вы просматривать

или читать этот файл чаще чем один-два раза? Или же со

временем у вас накопится целая база данных с такими

файлами, которая большую часть времени будет лежать на

диске мертвым грузом? Если вероятнее второй вариант, то

лучше просто сохранить URL. Если же первый, то читайте эту

главу дальше\! 

В библиотеке urllib, используемой для извлечения

контента веб-страниц, также содержатся функции для

получения содержимого файлов. Следующая программа с

помощью 

urllib.request.urlretrieve 

скачивает

изображения с удаленного URL:

from urllib.request import urlretrieve

from urllib.request import urlopen

from bs4 import BeautifulSoup

 

html = urlopen\('http://www.pythonscraping.com'\)

bs = BeautifulSoup\(html, 'html.parser'\)

imageLocation 

= 

bs.find\('a', 

\{'id':

'logo'\}\).find\('img'\)\['src'\]

urlretrieve \(imageLocation, 'logo.jpg'\) Эта программа скачивает картинку с логотипом, размещенную по адресу **http://pythonscraping.com**, и сохраняет

ее под именем logo.jpg в том же каталоге, из которого

запускается скрипт. 

Данный код отлично справляется с задачей, когда нужно

скачать только один файл с заранее известным именем и

расширением. 

Но 

большинство 

веб-скраперов 

не

ограничиваются скачиванием одного файла. Следующая

программа 

скачивает 

с 

главной 

страницы

**http://pythonscraping.com** все файлы, на которые указывает

ссылка в атрибуте src любого тега, если это внутренняя

ссылка:

import os

from urllib.request import urlretrieve

from urllib.request import urlopen

from bs4 import BeautifulSoup

 

downloadDirectory = 'downloaded' 

baseUrl = 'http://pythonscraping.com' 

 

def getAbsoluteURL\(baseUrl, source\):

if source.startswith\('http://www.'\):

url = 'http://\{\}'.format\(source\[11:\]\)

elif source.startswith\('http://'\):

url = source

elif source.startswith\('www.'\):

url = source\[4:\]

url = 'http://\{\}'.format\(source\)

else:

url = '\{\}/\{\}'.format\(baseUrl, source\) if baseUrl not in url:

return None

return url

 

def 

getDownloadPath\(baseUrl, 

absoluteUrl, 

downloadDirectory\):

path = absoluteUrl.replace\('www.', ''\)

path = path.replace\(baseUrl, ''\)

path = downloadDirectory\+path

directory = os.path.dirname\(path\)

 

if not os.path.exists\(directory\):

os.makedirs\(directory\)

 

return path

 

html = urlopen\('http://www.pythonscraping.com'\)

bs = BeautifulSoup\(html, 'html.parser'\)

downloadList = bs.find\_all\(src=True\)

 

for download in downloadList:

fileUrl = getAbsoluteURL\(baseUrl, 

download\['src'\]\)

if fileUrl is not None:

print\(fileUrl\)

 

urlretrieve\(fileUrl, 

getDownloadPath\(baseUrl, 

fileUrl, downloadDirectory\)\)

![Image 41](images/000076.png)


