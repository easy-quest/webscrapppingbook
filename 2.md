**Проход отдельного домена**

Даже если вам еще не приходилось слышать об игре «Шесть

шагов по “Википедии”», вы наверняка знаете о ее

предшественнице — «Шесть шагов до Кевина Бейкона». В обеих

играх цель состоит в том, чтобы установить взаимосвязь между

двумя мало связанными элементами \(в первом случае — между

статьями «Википедии», а во втором — между актерами, 

сыгравшими в одном фильме\) и построить цепь, содержащую

не более шести звеньев \(включая начальный и конечный

элементы\). 

Например, Эрик Айдл \(Eric Idle\) снялся в фильме «Дадли

Справедливый» \(Dudley Do-Right\) с Бренданом Фрейзером

\(Brendan Fraser\), который, в свою очередь, снялся с Кевином

Бейконом \(Kevin Bacon\) в «Воздухе, которым я дышу» \(The Air I Breathe\)4.  В этом случае цепь от Эрика Айдла до Кевина

Бейкона состоит всего из трех элементов. 

В этом разделе мы начнем разработку проекта, который

позволит строить цепи для «Шести шагов по “Википедии”»: например, 

начав 

со 

страницы 

Эрика 

Айдла

\(**https://en.wikipedia.org/wiki/Eric\_Idle**\), вы сможете найти

наименьшее количество переходов по ссылкам, которые

приведут 

вас 

на 

страницу 

Кевина 

Бейкона

\(**https://en.wikipedia.org/wiki/Kevin\_Bacon**\). 

**А как насчет нагрузки на сервер «Википедии»? **

По данным Фонда Викимедиа \(вышестоящей организации, отвечающей в том числе за «Википедию»\), за одну секунду

происходит примерно 2500 обращений к веб-ресурсам сайта, причем более 99 % из них относятся к «Википедии» \(см. 

раздел Traffic volume на странице Wikimedia in figures, **https://meta.wikimedia.org/wiki/Wikimedia\_in\_figures\_-\_Wikipedia\#Traffic\_volume**\). Из-за большого объема трафика

ваши веб-скраперы вряд ли сколько-нибудь заметно

повлияют на нагрузку сервера «Википедии». Тем не менее, если вы намерены активно использовать примеры кода, приведенные в этой книге, или будете разрабатывать

собственные проекты для веб-скрапинга «Википедии», я

призываю 

вас 

совершить 

необлагаемое 

налогом

пожертвование 

в 

Фонд 

Викимедиа

\(**https://wikimediafoundation.org/wiki/Ways\_to\_Give**\) — не

только в качестве компенсации нагрузки на сервер, но и

чтобы помочь сделать образовательные ресурсы более

доступными для других пользователей. 

Кроме того, имейте в виду: если вы планируете создать

большой проект с применением данных из «Википедии», то

стоит убедиться, что эти данные еще неоступны через API

«Википедии»

«\(**https://www.mediawiki.org/wiki/API:Main\_page**\). 

«Википедия» часто используется в качестве сайта для

демонстрации веб-скраперов и веб-краулеров, поскольку

данный сайт имеет простую структуру HTML-кода и

относительно стабилен. Однако эти же данные могут

оказаться более доступными через API. 

Вы уже, вероятно, знаете, как написать скрипт на Python, который бы получал произвольную страницу «Википедии» и

создавал список ссылок, присутствующих на этой странице: from urllib.request import urlopen

from bs4 import BeautifulSoup

 

html 

=

urlopen\('http://en.wikipedia.org/wiki/Kevin\_Bac

on'\)

bs = BeautifulSoup\(html, 'html.parser'\)

for link in bs.find\_all\('a'\):

if 'href' in link.attrs:

print\(link.attrs\['href'\]\)

Если вы посмотрите на созданный список ссылок, то

заметите, что в него вошли все ожидаемые статьи: Apollo 13, Philadelphia, Primetime Emmy Award и т.д. Однако есть и кое-что лишнее:

//wikimediafoundation.org/wiki/Privacy\_policy

//en.wikipedia.org/wiki/Wikipedia:Contact\_us

Дело в том, что на каждой странице «Википедии» есть

боковая панель, нижний и верхний колонтитулы с множеством

ссылок; также есть ссылки на страницы категорий, обсуждений

и другие страницы, которые не содержат полезных статей:

/wiki/Category:Articles\_with\_unsourced\_statemen

ts\_from\_April\_2014

/wiki/Talk:Kevin\_Bacon

Недавно мой друг, работая над аналогичным проектом по

веб-скрапингу «Википедии», заявил, что написал большую

функцию фильтрации, насчитывающую более 100 строк кода, с

целью определить, является ли внутренняя ссылка

«Википедии» ссылкой на страницу статьи. К сожалению, он не

уделил достаточно времени поиску закономерностей между

«ссылками на статьи» и «другими ссылками», иначе бы нашел

более красивое решение. Если вы внимательно посмотрите на

ссылки, которые ведут на страницы статей \(в отличие от других

внутренних страниц\), то заметите, что у всех таких ссылок есть

три общие черты:

• они находятся внутри тега div, у которого атрибут id имеет

значение bodyContent; 

• в их URL нет двоеточий; 

• их URL начинаются с /wiki/. 

Мы можем немного изменить код, включив в него эти

правила, и получить только нужные ссылки на статьи, воспользовавшись регулярным выражением ^\(/wiki/\) \(\(?\!:\).\)\*$"\):

from urllib.request import urlopen

from bs4 import BeautifulSoup

import re

 

html 

=

urlopen\('http://en.wikipedia.org/wiki/Kevin\_Bac

on'\)

bs = BeautifulSoup\(html, 'html.parser'\)

for 

link 

in 

bs.find\('div', 

\{'id':'bodyContent'\}\).find\_all\(

 

 

 

 

'a', 

href=re.compile\('^\(/wiki/\)

\(\(?\!:\).\)\*$'\)\):

if 'href' in link.attrs:

print\(link.attrs\['href'\]\)

Запустив этот код, мы получим список всех URL статей, на

которые ссылается статья «Википедии» о Кевине Бейконе. 

Скрипт, который находит все ссылки на статьи для одной

жестко заданной статьи «Википедии», конечно, интересен, однако с практической точки зрения довольно бесполезен. Вы

должны уметь, взяв этот код за основу, преобразовать его

примерно в такую программу. 

• Отдельная функция getLinks принимает URL статьи

«Википедии» в формате /wiki/<Название\_статьи> и

возвращает список всех URL, на которые ссылается эта

статья, в том же формате. 

• Основная функция, которая вызывает getLinks с

первоначальной статьей, выбирает из возвращенного

списка случайную ссылку и снова вызывает getLinks, пока

пользователь не остановит программу или пока не окажется, что на очередной странице нет ссылок. 

Вот полный код программы, которая это делает:

from urllib.request import urlopen

from bs4 import BeautifulSoup

import datetime

import random

import re

 

random.seed\(datetime.datetime.now\(\)\)

def getLinks\(articleUrl\):

 

 

 

 

html 

=

urlopen\('http://en.wikipedia.org\{\}'.format\(arti

cleUrl\)\)

bs = BeautifulSoup\(html, 'html.parser'\)

 

 

 

 

return 

bs.find\('div', 

\{'id':'bodyContent'\}\).find\_all\('a', 

href=re.compile\('^\(/wiki/\)

\(\(?\!:\).\)\*$'\)\)



links = getLinks\('/wiki/Kevin\_Bacon'\)

while len\(links\) > 0:

newArticle = links\[random.randint\(0, 

len\(links\)-1\)\].attrs\['href'\]

print\(newArticle\)

links = getLinks\(newArticle\)

Сразу после импорта необходимых библиотек программа

присваивает генератору случайных чисел начальное значение, равное текущему системному времени, чем обеспечивает

новый и интересный случайный путь по статьям «Википедии»

практически при каждом запуске программы. 

**Псевдослучайные числа и случайные начальные значения**

В предыдущем примере был задействован генератор

случайных чисел Python для случайного выбора статьи на

странице, по ссылке на которую будет продолжен обход

«Википедии». Однако случайные числа следует использовать

осторожно. 

Компьютеры хорошо справляются с вычислением правильных

решений, но откровенно слабы, когда нужно придумать что-то новое. По этой причине случайные числа могут стать

проблемой. Большинство алгоритмов генерации этих чисел

стремятся 

создать 

равномерно 

распределенную 

и

труднопредсказуемую числовую последовательность, однако

для запуска работы такого алгоритма необходимо

«начальное» число. Если оно каждый раз будет одним и тем

же, то алгоритм станет всякий раз генерировать одну и ту же

последовательность «случайных» чисел. Именно поэтому я

использовала значение системных часов в качестве

начального 

значения 

для 

создания 

новых

последовательностей случайных чисел и, следовательно, генерации последовательностей случайных статей. Благодаря

этому выполнять программу будет немного интереснее. 

Любопытно, что генератор псевдослучайных чисел Python работает по *алгоритму Мерсенна Твистера* \(Mersenne Twister\). 

Он генерирует труднопредсказуемые и равномерно

распределенные случайные числа, однако несколько

загружает процессор. Случайные числа — это хорошо, но за

все приходится платить\! 

Затем программа определяет функцию getLinks, которая

принимает URL статьи в формате /wiki/..., добавляет в

начало имя домена «Википедии» http://en.wikipedia.org и получает объект BeautifulSoup с HTML-кодом, который

находится по этому адресу. Затем функция извлекает список

тегов со ссылками на статьи, исходя из описанных ранее

параметров, и возвращает их. 

В основной части программы сначала создается список

тегов со ссылками на статьи \(переменная links\), в котором

содержатся ссылки, найденные на начальной странице

**https://en.wikipedia.org/wiki/Kevin\_Bacon**. 

Затем 

программа

выполняет цикл и находит тег со случайной ссылкой на статью, извлекает оттуда атрибут href, выводит страницу и получает

по извлеченному URL новый список ссылок. 

Разумеется, решение задачи «Шесть шагов по “Википедии”»

не ограничивается созданием веб-скрапера, переходящего с

![Image 28](images/000063.png)

одной страницы на другую. Нам также необходимо хранить и

анализировать полученные данные. Продолжение решения

этой задачи вы найдете в главе 6. 

**Обрабатывайте исключения\! **

По большей части обработка исключений в этих примерах

пропущена для краткости, однако следует помнить: здесь может

возникнуть множество потенциальных ловушек. Что, если, например, «Википедия» изменит имя тега bodyContent? Тогда

при попытке извлечь текст из тега программа выдаст

исключение AttributeError. 

Таким образом, несмотря на то, что эти сценарии достаточно

хороши в качестве тщательно контролируемых примеров, в

автономном коде готового приложения требуется обрабатывать

исключения гораздо лучше, чем позволяет описать объем

данной книги. Для получения дополнительной информации об

этом вернитесь к главе 1. 

**Сбор информации со всего сайта**

В предыдущем разделе мы прошли по сайту, переходя от одной

случайно выбранной ссылки к другой. Но как быть, если нужно

систематизировать сайт и составить каталог или просмотреть

все страницы? Сбор информации со всего сайта, особенно

большого, — процесс, требующий интенсивного использования

памяти. Лучше всего с этим справляются приложения, имеющие быстрый доступ к базе данных для хранения

результатов краулинга. Однако мы можем исследовать

поведение таких приложений, не выполняя их в полном

объеме. Подробнее об их выполнении с применением базы

данных см. в главе 6. 

**Темный и глубокий Интернет**

Вероятно, вам часто приходилось слышать о *глубоком*, *темном* или *скрытом Интер нете*, особенно в последних

новостях. Что имеется в виду? 

*Глубокий Интернет* — это любая часть Сети, выходящая за

пределы *видимого Интерне та*. Видимой называют ту часть

Интернета, которая индексируется поисковыми системами. 

Несмотря на большой разброс оценок, глубокий Интернет

почти наверняка составляет около 90 % всего Интернета. 

Поскольку Google не способен отправлять формы или

находить страницы, на которые не ссылается домен верхнего

уровня, а также не выполняет поиск сайтов, для которых это

запрещено в файле robots.txt, видимый Интернет

продолжает составлять относительно небольшую часть Сети. 

*Темный Интернет*, также известный как *Даркнет*, — нечто

совершенно иное. Он работает на той же сетевой аппаратной

инфраструктуре, но использует браузер Tor или другой

клиент, у которого протокол приложения работает поверх

HTTP, обеспечивая безопасный канал для обмена

информацией. В темном Интернете, как и на обычном сайте, 

тоже можно выполнять веб-скрапинг, однако эта тема

выходит за пределы данной книги. 

В отличие от темного, в глубоком Интернете веб-скрапинг

выполняется относительно легко. Многие инструменты, описанные в этой книге, научат вас, как выполнять веб-скрапинг и сбор информации во многих местах, недоступных

для ботов Google. 

В каких случаях сбор данных со всего сайта полезен, а в

каких — вреден? Веб-скраперы, которые перебирают весь сайт, хороши во многих случаях, включая следующие. 

• *Формирование карты сайта. * Несколько лет назад мне

встретилась задача: важный клиент хотел оценить затраты

на редизайн сайта, однако не хотел предоставлять моей

компании 

доступ 

ко 

внутренним 

компонентам

существующей системы управления контентом и у него не

было общедоступной карты сайта. Я воспользовалась веб-краулером, чтобы пройти по всему сайту, собрать все

внутренние ссылки и разместить страницы в структуре

папок, соответствующей той, что применялась на сайте. Это

позволило мне быстро обнаружить разделы сайта, о которых

я даже не подозревала, и точно подсчитать, сколько эскизов

страниц потребуется создать и какой объем контента

необходимо перенести. 

• *Сбор данных. * Другой клиент хотел собрать статьи \(истории, посты в блогах, новости и т.п.\), чтобы построить рабочий

прототип специализированной поисковой платформы. Это

исследование сайтов должно было быть не всеобъемлющим, однако достаточно обширным \(нам хотелось получать

данные лишь с нескольких сайтов\). Мне удалось создать веб-краулеры, которые рекурсивно обходили каждый сайт и

собирали данные только со страниц статей. 

Общий подход к полному сбору данных с сайта заключается

в том, чтобы начать со страницы верхнего уровня \(например, с

начальной страницы\) и построить список всех ее внутренних

ссылок. Затем обойти все страницы, на которые указывают эти

ссылки, и на каждой из них собрать дополнительные списки

ссылок, запускающие очередной этап сбора данных. 

Конечно же, в такой ситуации количество ссылок

стремительно растет. Если на каждой странице есть десять

внутренних ссылок, а глубина сайта составляет пять страниц

\(что довольно типично для сайта среднего размера\), то

необходимо проверить 105, то есть 100 000 страниц, чтобы с

уверенностью утверждать: сайт пройден полностью. Как ни

странно, хоть и «пять страниц в глубину и десять внутренних

ссылок на каждой странице» — довольно типичный размер

сайта, очень немногие сайты действительно насчитывают 100

000 и более страниц. Причина, конечно, состоит в том, что

подавляющее большинство внутренних ссылок являются

дубликатами. 

Во избежание повторного сбора данных с одной и той же

страницы крайне важно, чтобы все обнаруженные внутренние

ссылки имели согласованный формат и сохранялись в общем

множестве, что облегчило бы поиск во время работы

программы. *Множество* похоже на список, но его элементы не

располагаются в определенной последовательности. Кроме

того, в множестве содержатся исключительно уникальные

элементы, что идеально подходит для наших целей. Следует

проверять лишь те ссылки, которые являются «новыми», и

искать дополнительные ссылки только по ним:

from urllib.request import urlopen from bs4 import BeautifulSoup

import re

 

pages = set\(\)

def getLinks\(pageUrl\):

global pages

 

 

 

 

html 

=

urlopen\('http://en.wikipedia.org\{\}'.format\(page

Url\)\)

bs = BeautifulSoup\(html, 'html.parser'\)

 

 

 

 

for 

link 

in 

bs.find\_all\('a', 

href=re.compile\('^\(/wiki/\)'\)\):

if 'href' in link.attrs:

if link.attrs\['href'\] not in pages:

\# мы нашли новую страницу

newPage = link.attrs\['href'\]

print\(newPage\)

pages.add\(newPage\)

getLinks\(newPage\)

getLinks\(''\)

Для демонстрации полной работы этого веб-краулера я

ослабила требования к тому, как должны выглядеть

внутренние ссылки \(из предыдущих примеров\). Веб-скрапер не

ограничивается только страницами статей, а ищет все ссылки, начинающиеся с /wiki/, независимо от того, где они

располагаются на странице и содержат ли двоеточия. 

Напомню: в URL страниц статей отсутствуют двоеточия, в

отличие от адресов страниц загрузки файлов, обсу ждений и т.п. 

Изначально функция getLinks вызывается с пустым URL, то есть для начальной страницы «Википедии»: к пустому URL

![Image 29](images/000018.png)

внутри функции добавляется http://en.wikipedia.org. 

Затем функция просматривает каждую ссылку на первой

странице и проверяет, есть ли она в *глобальном множестве*

страниц \(множестве страниц, с которыми скрипт уже

встречался\). Если нет, то ссылка добавляется в список, выводится на экран и функция getLinks вызывается для нее

рекурсивно. 

**Осторожно с рекурсией**

Это предупреждение редко встречается в книгах по

программированию, но я считаю, что вы должны знать: если

оставить программу работать достаточно долго, то предыдущая

программа почти наверняка завершится аварийно. 

В Python установлен по умолчанию предел рекурсии

\(количество рекурсивных вызовов программы\), и он равен

1000. Поскольку сеть ссылок «Википедии» чрезвычайно

обширна, данная программа в итоге достигнет предела

рекурсии и остановится, если только не добавить счетчик

рекурсии или что-то еще, призванное помешать этому. 

Для «плоских» сайтов глубиной менее 1000 ссылок данный

метод обычно — за редким исключением — работает хорошо. 

Например, однажды мне встретилась ошибка в динамически

генерируемом URL, поскольку ссылка на следующую страницу

зависела от адреса текущей страницы. Это привело к

![Image 30](images/000046.png)

![Image 31](images/000077.png)

бесконечно 

повторяющимся 

путям, 

таким 

как

/blogs/blogs.../blogs/blog-post.php. 

Однако, как правило, этот рекурсивный метод должен

подходить для любого обычного сайта, с которым вы, скорее

всего, столкнетесь. 

**Сбор данных со всего сайта. ** Веб-краулеры были бы

довольно скучными программами, если бы только переходили

с одной страницы на другую. Полезными они могут быть, что-то делая на странице, на которую перешли. Рассмотрим

пример веб-скрапера, который бы выбирал заголовок, первый

абзац контента страницы и ссылку на режим редактирования

страницы \(если таковая существует\). 

Как всегда, чтобы выбрать лучший способ сделать это, сначала нужно просмотреть несколько страниц сайта и

определить его структуру. При просмотре нескольких страниц

«Википедии» \(как статей, так и страниц, не являющихся

статьями, 

например 

страницы 

с 

политикой

конфиденциальности\) становится ясно следующее. 

• У любой страницы \(независимо от статуса: статья, история

редактирования или любая другая страница\) есть заголовок, заключенный в теги h1 span, и это единственный тег h1 на

странице. 

• Как уже говорилось, весь основной текст находится внутри

тега div\#bodyCon tent. Но если вы хотите выделить

конкретную часть текста — например, получить доступ

только к первому абзацу, — то лучше использовать div\#mw-content-text p \(выбрать только тег первого абзаца\). Это

подходит для всех контентных страниц, кроме страниц

файлов 

\(например, 

![Image 32](images/000025.png)

![Image 33](images/000057.png)

**https://en.wikipedia.org/wiki/File:Orbit\_of\_274301\_Wikipedia.svg**\)

, на которых нет разделов основного текста. 

• Ссылки для редактирования существуют только на страницах

статей. Если такая ссылка есть, то она находится в теге

li\#ca-edit, в li\#ca-edit span a. 

Изменив исходный код нашего веб-краулера, мы можем

создать комбинированную программу для краулинга и сбора

данных \(или по крайней мере для вывода данных на экран\): from urllib.request import urlopen

from bs4 import BeautifulSoup

import re

 

pages = set\(\)

def getLinks\(pageUrl\):

global pages

 

 

 

 

html 

=

urlopen\('http://en.wikipedia.org\{\}'.format\(page

Url\)\)

bs = BeautifulSoup\(html, 'html.parser'\)

try:

print\(bs.h1.get\_text\(\)\)

print\(bs.find\(id ='mw-content-

text'\).find\_all\('p'\)\[0\]\)

 

 

 

 

 

 

 

 

print\(bs.find\(id='ca-

edit'\).find\('span'\)

.find\('a'\).attrs\['href'\]\)

except AttributeError:

print\('This page is missing something\! 

Continuing.'\)

![Image 34](images/000005.png)

 

 

 

 

 

for 

link 

in 

bs.find\_all\('a', 

href=re.compile\('^\(/wiki/\)'\)\):

if 'href' in link.attrs:

if link.attrs\['href'\] not in pages:

\# мы нашли новую страницу

newPage = link.attrs\['href'\]

print\('-'\*20\)

print\(newPage\)

pages.add\(newPage\)

getLinks\(newPage\)

getLinks\(''\)

Цикл for в этой программе, по сути, такой же, как и в

первоначальной программе сбора данных \(для ясности

добавлен вывод дефисов в качестве разделителей контента\). 

Поскольку никогда нельзя быть полностью уверенными в

том, что на каждой странице будут присутствовать все нужные

данные, операторы print расположены в соответствии с

вероятностью наличия этих данных на странице. Так, тег

заголовка h1 есть на каждой странице \(во всяком случае, насколько я могу судить\), вследствие чего мы сначала

пытаемся получить эти данные. Текстовый контент

присутствует на большинстве страниц \(за исключением

страниц файлов\), так что этот фрагмент данных извлекается

вторым. Кнопка редактирования есть только на страницах с

заголовками и текстовым контентом, и то не на всех. 

**Разные схемы для различных потребностей**

Очевидно, что, когда в блоке обработчика исключений

заключено нескольких строк, возникает некая опасность. 

Прежде всего, вы не можете сказать, какая именно строка

вызвала исключение. Кроме того, если по какой-либо причине

на странице есть кнопка редактирования, но нет заголовка, то

кнопка никогда не будет зарегистрирована. Однако во многих

случаях 

при 

наличии 

определенной 

вероятности

возникновения на сайте каждого из элементов этого достаточно

и непредвиденное отсутствие нескольких точек данных или

ведение подробных журналов не является проблемой. 

Вы могли заметить: в этом и во всех предыдущих примерах

мы не столько «собирали» данные, сколько «выводили» их. 

Очевидно, что данными, выводимыми в окно терминала, сложно манипулировать. Подробнее о хранении информации и

создании баз данных вы узнаете в главе 5. 

**Обработка перенаправлений**

Перенаправления позволяют веб-серверу использовать имя

текущего домена или URL для контента, находящегося в

другом месте. Существует два типа перенаправлений: перенаправления на стороне сервера, когда URL

изменяется до загрузки страницы; 

перенаправления на стороне клиента, иногда с

сообщением наподобие «через десять секунд вы

перейдете на другой сайт», когда сначала загружается

страница сайта, а потом происходит переход на новую

страницу. 

С перенаправлениями на стороне сервера вам обычно ничего

не нужно делать. Если вы используете библиотеку urllib для Python 3.x, то она обрабатывает перенаправления

автоматически\! В случае применения библиотеки запросов

проследите, чтобы флаг allow\_redirects имел значение

True:

r = requests.get\( 'http://github.com' , 

allow\_redirects=True\)

Просто помните, что иногда URL сканируемой страницы

может не совпадать с URL, по которому расположена эта

страница. 

Подробнее о перенаправлениях на стороне клиента, которые

выполняются с помощью JavaScript или HTML, см. в главе 12. 

**Сбор информации с нескольких сайтов**

Каждый раз, когда я делаю доклад о взломе веб-страниц, кто-нибудь обязательно спрашивает: «Как построить Google?» Мой

ответ всегда состоит из двух частей: «Во-первых, вам нужно

где-то взять много миллиардов долларов, чтобы купить

крупнейшие в мире хранилища данных и разместить их в

скрытых местах по всему миру. Во-вторых, вам нужно

разработать веб-краулер». 

В 1996 году, когда Google только начиналась, она состояла

всего из двух аспирантов из Стэнфорда, у которых был старый

сервер и веб-краулер, написанный на Python. Теперь, зная, как

![Image 35](images/000035.png)

работает веб-скрапинг, вы официально располагаете

инструментами, необходимыми для того, чтобы стать

следующим технологическим мультимиллиардером\! 

Заявляю совершенно серьезно: именно веб-краулеры —

основа того, что движет многими современными веб-технологиями, и для их использования не обязательно иметь

большое хранилище данных. Чтобы выполнить любой кросс-доменный анализ данных, необходимо разработать веб-краулеры, способные интерпретировать и хранить данные, собранные со множества интернет-страниц. 

Как и в предыдущем примере, веб-краулеры, которые мы

намерены создать, будут переходить по ссылкам от страницы к

странице, формируя карту сети. Однако на этот раз они не

станут игнорировать внешние ссылки, а, наоборот, будут

переходить по ним. 

**Впереди — неведомые воды**

Имейте в виду: код, представленный ниже, может переходить в

любую точку Интернета. Если мы что-то и узнали из «Шести

шагов по “Википедии”», так это то, что всего за несколько

переходов можно полностью уйти с сайта, например, http://www.sesamestreet.org, и попасть в гораздо менее

приятное место. 

Дети, получите разрешение у родителей, прежде чем запускать

этот код. Если у вас есть твердые убеждения или религиозные

ограничения, которые не позволяют вам посещать порносайты, 

то ознакомьтесь с данным кодом, однако будьте осторожны, запуская программу. 

Прежде чем приступать к написанию веб-краулера, который, хотите вы этого или нет, будет переходить по всем

внешним ссылкам, следует задать себе несколько вопросов. 

• Какие данные я намереваюсь собрать? Достаточно ли для

этого выполнить веб-скрапинг нескольких заранее

определенных сайтов \(что почти всегда проще сделать\), или

же мой краулер должен иметь возможность обнаруживать

новые сайты, о которых я, вероятно, не подозреваю? 

• Попав на определенный сайт, должен ли мой краулер сразу

переходить по следующей внешней ссылке на другой сайт, или же ему следует задержаться на какое-то время на этом и

изучить его более углубленно? 

• Существуют ли какие-либо условия, при которых я бы не

хотел выполнять веб-скрапинг текущего сайта? Интересует

ли меня контент на иностранных языках? 

• Как я буду защищаться от судебных исков, если мой веб-краулер привлечет внимание веб-мастера одного из сайтов, на которые попадет? \(Подробнее об этом см. в главе 18.\) Менее чем в 60 строках кода можно без труда уместить

гибкий набор функций Python, комбинация которых дает

различные типы веб-скраперов:

from urllib.request import urlopen

from urllib.parse import urlparse

from bs4 import BeautifulSoup

import re

import datetime

import random

 

pages = set\(\)

random.seed\(datetime.datetime.now\(\)\)

 

\# Получить список всех внутренних ссылок, 

найденных на странице. 

def getInternalLinks\(bs, includeUrl\):

 

 

 

 

includeUrl 

=

'\{\}://\{\}'.format\(urlparse\(includeUrl\).scheme, 

urlparse\(includeUrl\).netloc\)

internalLinks = \[\]

\# найти все ссылки, которые начинаются с

"/" 

for link in bs.find\_all\('a', 

href=re.compile\('^\(/|.\*'\+includeUrl\+'\)' 

\)\):

if link.attrs\['href'\] is not None:

if link.attrs\['href'\] not in

internalLinks:

if\(link.attrs\['href'\].startswit

h\('/'\)\):

internalLinks.append\(

includeUrl\+link.attrs\[' 

href'\]\)

else:

internalLinks.append\(link.a

ttrs\['href'\]\)

return internalLinks

 

\# Получить список всех внешних ссылок, найденных на странице. 

def getExternalLinks\(bs, excludeUrl\):

externalLinks = \[\]

\# Найти все ссылки, которые начинаются с

"http" или "www", 

\# не содержащие текущий URL. 

for link in bs.find\_all\('a', 

href=re.compile\('^\(http|www\)

\(\(?\!'\+excludeUrl\+'\).\)\*$'\)\):

if link.attrs\['href'\] is not None:

if link.attrs\['href'\] not in

externalLinks:

externalLinks.append\(link.attrs

\['href'\]\)

return externalLinks

 

def getRandomExternalLink\(startingPage\):

html = urlopen\(startingPage\)

bs = BeautifulSoup\(html, 'html.parser'\)

externalLinks = getExternalLinks\(bs, 

urlparse\(startingPage\).netloc\)

if len\(externalLinks\) == 0:

print\('No external links, looking

around the site for one'\)

 

 

 

 

 

 

 

 

 

 

 

 

domain 

=

'\{\}://\{\}'.format\(urlparse\(startingPage\).scheme, 

urlparse\(startingPage\).netloc\)

internalLinks =

getInternalLinks\(bs, domain\)

 

 

 

 

 

 

 

 

 

 

 

 

return

getRandomExternalLink\(internalLinks\[random.rand

int\(0, 

len\(int

ernalLinks\)-1\)\]\)

else:

 

 

 

 

 

 

 

 

 

 

 

 

return

externalLinks\[random.randint\(0, 

len\(externalLinks\)-1\)\]

 

def followExternalOnly\(startingSite\):

 

 

 

 

externalLink 

=

getRandomExternalLink\(startingSite\)

print\('Random external link is:

\{\}'.format\(externalLink\)\)

followExternalOnly\(externalLink\)

followExternalOnly\('http://oreilly.com'\)

Эта программа начинает с сайта **http://oreilly.com** и

случайным образом переходит от одной внешней ссылки к

другой. Вот пример результатов, которые она выводит: http://igniteshow.com/

http://feeds.feedburner.com/oreilly/news

http://hire.jobvite.com/CompanyJobs/Careers.asp

x?c=q319

http://makerfaire.com/

Не обязательно внешние ссылки обнаружатся на первой же

странице сайта. В этом случае для поиска внешних ссылок

используется метод, аналогичный тому, который применялся в

предыдущем примере веб-краулинга для рекурсивного

перехода в глубину сайта до тех пор, пока не будет найдена

внешняя ссылка. 

![Image 36](images/000041.png)

![Image 37](images/000049.png)

Принцип работы этой программы показан на рис. 3.1 в виде

блок-схемы. 

 

**Рис. 3.1. ** Блок-схема нашего сценария краулинга сайтов

**Не используйте учебные примеры программ в реальных**

**приложениях**

Еще раз напоминаю: ради экономии места и читабельности

примеры программ, приведенные в этой книге, не всегда

содержат необходимые проверки и обработки исключений, требуемые в готовом коде. Например, если веб-краулер не

обнаружил на сайте ни одной внешней ссылки \(что

маловероятно, но в случае достаточно долгой работы

программы в какой-то момент обязательно произойдет\), то

программа будет продолжать работу, пока не достигнет

предела рекурсии Python. 

Одним из простых способов повысить надежность этого веб-краулера было бы объединить его с кодом обработки

исключений при соединении, описанном в главе 1. Это

позволило бы в случае возникновения ошибки HTTP или

исключения на сервере при получении страницы выбрать

другой URL и перейти по нему. 

Прежде чем использовать этот код для каких-либо серьезных

целей, обязательно добавьте в него проверки для устранения

потенциальных ловушек. 

Разделение задачи на простые функции, такие как «найти

все внешние ссылки на странице», имеет преимущество: впоследствии код можно легко перестроить для выполнения

другой задачи веб-краулинга. Например, если цель состоит в

том, чтобы просмотреть сайт, найти все внешние ссылки и

сохранить их, то можно добавить следующую функцию. 

\# Составляет список из всех внешних URL, 

найденных на сайте. 

allExtLinks = set\(\)

allIntLinks = set\(\)

 

def getAllExternalLinks\(siteUrl\):

html = urlopen\(siteUrl\)

 

 

 

 

domain 

=

'\{\}://\{\}'.format\(urlparse\(siteUrl\).scheme, 

urlparse\(siteUrl\).netloc\)

bs = BeautifulSoup\(html, 'html.parser'\)

internalLinks = getInternalLinks\(bs, 

domain\)

externalLinks = getExternalLinks\(bs, 

domain\)

 

![Image 38](images/000007.png)

for link in externalLinks:

if link not in allExtLinks:

allExtLinks.add\(link\)

print\(link\)

for link in internalLinks:

if link not in allIntLinks:

allIntLinks.add\(link\)

getAllExternalLinks\(link\)

 

allIntLinks.add\('http://oreilly.com'\)

getAllExternalLinks\('http://oreilly.com'\)

Этот код можно представить как два совместно работающих

цикла: один собирает внутренние ссылки, а второй — внешние. 

Блок-схема программы выглядит примерно так \(рис. 3.2\). 

 

**Рис. 3.2. ** Блок-схема веб-краулера сайтов, который собирает все внешние ссылки

Привычка составлять схемы и диаграммы того, что должен

делать код, прежде чем писать его, фантастически полезна. По

мере усложнения ваших веб-краулеров она сэкономит вам кучу

времени и избавит от многих разочарований. 

4 Благодарю сайт The Oracle of Bacon \(http://oracleofbacon.org/\), с помощью

которого я удовлетворила свое любопытство относительно этой цепочки. 

**Глава 4. Модели веб-краулинга**

Писать чистый и масштабируемый код довольно сложно, даже

если у вас есть возможность контролировать входные и

выходные данные. Написание кода веб-краулера, которому

иногда приходится выполнять веб-скрапинг и сохранять

различные данные с разных групп сайтов никак не

контролируемых программистом часто представляет собой

невероятно сложную организационную задачу. 

Вам могут предложить собрать новостные статьи или

публикации из блогов, размещенных на различных сайтах, у

каждого из которых свои шаблоны и макеты. На одном сайте

тег h1 может содержать заголовок статьи, а на другом —

заголовок самого сайта, а заголовок статьи будет заключен в

тег <spanid="title">. 

Возможно, вам понадобится гибко управлять тем, для каких

сайтов нужно выполнить веб-скрапинг и как именно это

делать, а также способ быстро добавлять новые сайты или

изменять существующие — максимально быстро и без

необходимости писать много строк кода. 

Вас могут попросить собрать цены на товары с разных

сайтов, чтобы в итоге можно было сравнивать цены на один и

тот же товар. Возможно, они будут представлены в разных

валютах. Вдобавок, вероятно, потребуется объединить их с

внешними данными, полученными из какого-то другого

источника, не имеющего отношения к Интернету. 

Несмотря на то что вариантов применения веб-краулеров

бесчисленное множество, большие масштабируемые веб-краулеры, как правило, относятся к одному из нескольких

типов. Исследуя эти типы и распознавая ситуации, в которых

они используются, можно значительно повысить удобство

сопровождения и надежность своих веб-краулеров. 

В этой главе мы уделим основное внимание веб-краулерам, собирающим ограниченное количество «типов» данных \(таких

как обзоры ресторанов, новостные статьи, профили компаний\) с большого количества сайтов и хранящим эти данные в виде

объектов Python, которые читают и записывают в базу данных. 

**Планирование и определение объектов**

Одна из распространенных ловушек веб-скрапинга —

определение данных, которые разработчики намерены

собирать, исключительно на основании того, что есть у них

перед глазами. Например, желая собрать данные о товаре, можно было бы для начала заглянуть в магазин одежды и

решить, что для каждого интересующего нас товара

необходимо получить данные из следующих полей:

• наименование товара; 

• цена; 

• описание; 

• размеры; 

• цвета; 

• тип ткани; 

• рейтинг клиентов. 

Посмотрев на другой сайт, вы обнаружите, что у товара есть

SKU \(stock keeping unit, единицы хранения, или артикул, используемый для отслеживания и заказа товаров\). Вы

наверняка захотите собирать и эти данные, даже если их не

было на первом сайте\! И вы добавите в список данное поле:

• артикул. 

Для начала одежда вполне подойдет, однако необходимо

убедиться, что этот веб-скрапер можно будет распространить и

на другие типы товаров. Начав просматривать разделы товаров

на других сайтах, вы поймете, что также необходимо собрать

следующую информацию:

• твердая/мягкая обложка; 

• матовая/глянцевая печать; 

• количество отзывов клиентов; 

• ссылка на сайт производителя. 

Понятно, что такой подход неприемлем. Просто добавляя

атрибуты к типу товара всякий раз, когда на сайте встречается

новая информация, вы получите слишком большое количество

полей, которые нужно отслеживать. Мало того, каждый раз при

веб-скрапинге нового сайта придется подробно анализировать

поля, уже имеющиеся на нем, и те, что уже были накоплены

ранее, а также, возможно, добавлять новые \(изменяя тип

объекта Python и структуру базы данных\). Это приведет к

запутанному и трудночитаемому набору данных, а

следовательно, к проблемам при его использовании. 

Принимая решение о том, какие данные собирать, зачастую

лучше игнорировать сайты. Нельзя запустить проект, рассчитанный на то, чтобы стать большим и масштабируемым, посмотрев только на один сайт и спросив себя: «Что здесь

есть?» Вместо этого нужно задать другой вопрос: «Что мне

нужно?» — а затем найти способы поиска необходимой

информации. 

Возможно, в действительности вам нужно всего лишь

сравнивать цены на товары в нескольких магазинах и

отслеживать изменения этих цен с течением времени. В этом

случае вам необходима только та информация, которая бы

позволила однозначно идентифицировать товар:

• название товара; 

• производитель; 

• идентификационный номер товара \(если он есть и интересен

вам\). 

Важно отметить, что ни одно из этих свойств товара не

относится к конкретному магазину. Например, обзоры товаров, рейтинги, цены и даже описания одного и того же товара могут

отличаться в разных магазинах, и их стоит хранить отдельно. 

Другие данные \(цветовые варианты товара и материал, из

которого он сделан\) относятся к товару, но могут быть

разреженными — существовать не для всех единиц товара. 

Важно сделать шаг назад, составить контрольный список для

каждого свойства товара, которое вы считаете необходимым, и

задать себе следующие вопросы. 

• Поможет ли данная информация достичь целей проекта? 

Зайду ли я в тупик, если не получу ее, или это лишь данные

из разряда «пригодится», но, по большому счету, они ни на

что не влияют? 

• Если эти данные когда-нибудь *могут* пригодиться \(а могут и

нет\) — насколько сложно будет вернуться и собрать их? 

• Являются ли эти данные избыточными относительно уже

собранных? 

• Имеет ли логический смысл хранить данные именно в этом

объекте? \(Как уже упоминалось, сохранение описания в

качестве свойства товара не имеет смысла, если описание

одного и того же товара может быть разным на разных

сайтах.\)

Определившись, какие данные нужно собрать, важно задать

себе еще несколько вопросов, чтобы затем решить, как хранить

и обрабатывать эти данные в коде. 

• Эти данные разреженные или плотные? Свойственны ли они

любому товару, в любом списке или же существуют только

для небольшого множества товаров? 

• Насколько велик объем данных? 

• Особенно это касается больших данных: нужно ли будет

регулярно получать их каждый раз при выполнении анализа

или только в отдельных случаях? 

• Насколько переменчивы данные этого типа? Придется ли

регулярно добавлять новые атрибуты, изменять типы

\(например, часто могут добавляться ткани с новыми

узорами\), или же эти данные никогда не изменяются

\(допустим, размеры обуви\)? 

Предположим, вы хотите выполнить некий метаанализ

зависимости цены от свойств товара: например, от количества

страниц в книге или типа ткани, из которой сшита одежда; также, возможно, в будущем вы захотите добавить другие

атрибуты, от которых зависит цена. Просматривая подобные

вопросы, вы можете обнаружить, что эти данные являются

разреженными \(сравнительно немногие товары имеют хотя бы

один из указанных атрибутов\), и решить, что придется часто

добавлять или удалять атрибуты. В этом случае, вероятно, стоит создать тип товара, который выглядит следующим

образом:

• название товара; 

• производитель; 

• идентификационный номер товара \(если есть и нужен\); 

• атрибуты \(необязательный параметр; список или словарь\). 

Тип атрибута выглядит так:

• имя атрибута; 

• значение атрибута. 

Со временем это позволит гибко добавлять новые атрибуты

товара, не прибегая к необходимости изменять схему данных

или переписывать код. Определяясь с тем, как хранить эти

атрибуты в базе данных, вы можете решить записывать их в

поле attribute в формате JSON или же хранить каждый

атрибут в отдельной таблице, откуда извлекать их по

идентификатору товара. Подробнее о реализации этих типов

моделей баз данных см. в главе 6. 

Эти же вопросы можно задать и для другого типа

информации, которую вам нужно будет хранить. Чтобы

отслеживать цены, найденные для каждого товара, вам, вероятно, потребуется следующее:

• идентификатор товара; 

• идентификатор магазина; 

• цена; 

• дата или метка времени для данной цены. 

Что будет в случае, если атрибуты товара действительно

влияют на его цену? Например, за рубашку большего размера

магазин может выставить более высокую цену, чем за рубашку

меньшего, так как пошив требует больше труда или

материалов. В этом случае, возможно, стоит преобразовать

товар «рубашка» в список товаров, по одному для каждого

размера \(чтобы у каждого вида рубашки была независимая

цена\), или же создать новый тип элемента для хранения

информации об экземплярах товара, содержащий следующие

поля:

• идентификатор товара; 

• тип экземпляра \(в данном случае размер рубашки\). 

Тогда каждая цена будет выглядеть так:

• идентификатор экземпляра товара; 

• идентификатор магазина; 

• цена; 

• дата или метка времени для данной цены. 

Тема «товары и цены» может показаться слишком узкой, однако основные вопросы, которые нужно себе задать, и

логика, используемая при разработке этих объектов Python, применимы практически в любой ситуации. 

При веб-скрапинге новостных публикаций может

потребоваться следующая основная информация:

• заголовок; 

• автор; 

• дата; 

• контент. 

Но предположим, что некоторые статьи содержат «дату

изменения», или «связанные публикации», или «количество

перепостов в социальных сетях». Вам нужна эта информация? 

Она имеет отношение к вашему проекту? Как эффективно и

гибко хранить количество перепостов в социальных сетях, если

только некоторые новостные сайты используют все формы

социальных сетей, а со временем популярность разных

социальных сетей может увеличиваться или уменьшаться? 

При разработке нового проекта может возникнуть соблазн

сразу погрузиться в написание кода на Python, чтобы

немедленно приступить к веб-скрапингу. Но если оставить

создание модели данных на потом, то доступность и формат

данных часто будут определяться первым попавшимся сайтом. 

Однако модель данных является основой всего кода, который ее использует. Неправильный выбор модели легко

может привести к проблемам с написанием и сопровождением

кода или к затруднениям при извлечении и эффективном

применении полученных данных. Особенно это касается

работы с разными сайтами: и известными, и неизвестными. 

Притом жизненно важно все хорошенько обдумать и

спланировать, какие именно данные нужно собирать и как их

хранить. 

**Работа с различными макетами сайтов**

Одно из наиболее впечатляющих достижений поисковых

систем, таких как Google, состоит в возможности извлекать

релевантные и полезные данные с различных сайтов, не имея

предварительных знаний об их структуре. Мы, люди, способны

с первого взгляда определить, где у страницы заголовок, а где

— основ ной контент \(за исключением случаев крайне плохого

веб-дизайна\), однако заставить бот делать то же самое гораздо

труднее. 

К счастью, в большинстве случаев при веб-краулинге мы не

намерены собирать данные с сайтов, которые никогда прежде

не видели. Обычно речь идет максимум о нескольких десятках

сайтов, предварительно отобранных человеком. Это значит, что нам не понадобятся сложные алгоритмы или машинное

обучение для определения того, какой текст на странице

«больше всего похож на заголовок», а какой, скорее всего, является «основным контентом». Все эти элементы можно

задать вручную. 

Наиболее очевидный подход — написать отдельный веб-краулер или парсер страниц для каждого сайта. Каждый такой

краулер может принимать URL, строку или объект

BeautifulSoup и возвращать результат веб-скрапинга в виде

объекта Python. 

Ниже приведены пример класса Content \(представляющего

собой фрагмент контента сайта, например новостную

публикацию\) и две функции веб-скрапинга, которые

принимают объект BeautifulSoup и возвращают экземпляр

Content:

import requests

 

class Content:

def \_\_init\_\_\(self, url, title, body\):

self.url = url

self.title = title

self.body = body

 

def getPage\(url\):

req = requests.get\(url\)

 

 

 

 

return 

BeautifulSoup\(req.text, 

'html.parser'\)

 

def scrapeNYTimes\(url\):

bs = getPage\(url\)

title = bs.find\('h1'\).text

 

 

 

 

lines 

=

bs.select\('div.StoryBodyCompanionColumn div p'\)

body = '\\n'.join\(\[line.text for line in

lines\]\)

return Content\(url, title, body\)



def scrapeBrookings\(url\):

bs = getPage\(url\)

title = bs.find\('h1'\).text

body = bs.find\('div', \{'class', 'post-

body'\}\).text

return Content\(url, title, body\)

 

url = 'https://www.brookings.edu/blog/future-

development/2018/01/26/' 

'delivering-inclusive-urban-access-3-

uncomfortable-truths/' 

content = scrapeBrookings\(url\)

print\('Title: \{\}'.format\(content.title\)\)

print\('URL: \{\}\\n'.format\(content.url\)\)

print\(content.body\)

 

url 

=

'https://www.nytimes.com/2018/01/25/opinion/sun

day/' 

'silicon-valley-immortality.html' 

content = scrapeNYTimes\(url\)

print\('Title: \{\}'.format\(content.title\)\)

print\('URL: \{\}\\n'.format\(content.url\)\)

print\(content.body\)

Добавляя функции веб-скрапинга для других новостных

сайтов, вы, вероятно, заметите некоторую закономерность. В

сущности, функция синтаксического анализа любого сайта

делает одно и то же:

• находит элемент заголовка и извлекает оттуда текст

заголовка; 

• находит основной контент статьи; 

• при необходимости находит другие элементы контента; 

• возвращает объект Content, созданный с помощью ранее

найденных строк. 

Единственное, что здесь действительно зависит от сайта, —

это CSS-селекторы, используемые для получения каждого

элемента информации. Функции BeautifulSoup find и

find\_all принимают два аргумента: строку тега и словарь

атрибутов в формате «ключ — значение», вследствие чего эти

аргументы можно передавать как параметры, которые

определяют структуру сайта и расположение нужных данных. 

Чтобы было еще удобнее, вместо аргументов тегов и пар

«ключ — значение» можно использовать функцию

BeautifulSoup select, принимающую строку CSS-селектора для

каждого элемента информации, который вы хотите получить, и

разместить все эти селекторы в словарном объекте: class Content:

""" 

Общий родительский класс для всех статей/

страниц. 

""" 

def \_\_init\_\_\(self, url, title, body\):

self.url = url

self.title = title

self.body = body



def print\(self\):

""" 

Гибкая функция печати, управляющая выводом

данных. 

""" 

print\('URL: \{\}'.format\(self.url\)\)

print\('TITLE: \{\}'.format\(self.title\)\)

print\('BODY:\\n\{\}'.format\(self.body\)\)

 

class Website:

""" 

Содержит информацию о структуре сайта. 

""" 

def \_\_init\_\_\(self, name, url, titleTag, 

bodyTag\):

self.name = name

self.url = url

self.titleTag = titleTag

self.bodyTag = bodyTag

Обратите внимание: в классе Website хранится не

информация, собранная с разных страниц, а инструкции о том, *как* ее собирать. Так, здесь хранится не заголовок «Название

моей страницы», а лишь строка с тегом h1, который указывает

на то, где содержатся заголовки. Именно поэтому класс

называется Website \(его информация относится ко всему

сайту\), а не Content \(в котором содержится информация

только с одной страницы\). 

С помощью классов Content и Website можно написать

класс Crawler для веб-скрапинга заголовка и контента, 

размещенных по любому URL веб-страницы, принадлежащей

данному сайту:

import requests

from bs4 import BeautifulSoup

 

class Crawler:

def getPage\(self, url\):

try:

req = requests.get\(url\)

 

 

 

 

 

 

 

 

except

requests.exceptions.RequestException:

return None

return BeautifulSoup\(req.text, 

'html.parser'\)

 

def safeGet\(self, pageObj, selector\):

""" 

Служебная функция, используемая для

получения строки

содержимого из объекта BeautifulSoup и

селектора. 

Если объект для данного селектора не

найден, 

то возвращает пустую строку. 

""" 

 

 

 

 

 

 

 

 

selectedElems 

=

pageObj.select\(selector\)

if selectedElems is not None and

len\(selectedElems\) > 0:

return '\\n'.join\(

\[elem.get\_text\(\) for elem in selectedElems\]\)

return '' 

 

def parse\(self, site, url\):

""" 

Извлекает содержимое страницы с

заданным URL. 

""" 

bs = self.getPage\(url\)

if bs is not None:

title = self.safeGet\(bs, 

site.titleTag\)

body = self.safeGet\(bs, 

site.bodyTag\)

if title \!= '' and body \!= '':

content = Content\(url, title, 

body\)

content.print\(\)

А следующий код определяет объекты сайтов и запускает

весь процесс:

crawler = Crawler\(\)

 

siteData = \[

\['O\\'Reilly Media', 'http://oreilly.com', 

'h1', 'section\#product-description'\], 

\['Reuters', 'http://reuters.com', 'h1', 

'div.StandardArticleBody\_body\_1gnLA'\], 

\['Brookings', 'http://www.brookings.edu', 

'h1', 'div.post-body'\], 

\['New York Times', 'http://nytimes.com', 

'h1', 'div.StoryBodyCompanionColumn div p'\]

\]

websites = \[\]

for row in siteData:

websites.append\(Website\(row\[0\], row\[1\], 

row\[2\], row\[3\]\)\)

 

crawler.parse\(websites\[0\], 

'http://shop.oreilly.com/product/'\\

'0636920028154.do'\)

crawler.parse\(websites\[1\], 

'http://www.reuters.com/article/'\\

'us-usa-epa-pruitt-idUSKBN19W2D0'\)

crawler.parse\(websites\[2\], 

'https://www.brookings.edu/blog/'\\

'techtank/2016/03/01/idea-to-retire-old-

methods-of-policy-education/'\)

crawler.parse\(websites\[3\], 

'https://www.nytimes.com/2018/01/'\\

'28/business/energy-environment/oil-

boom.html'\)

На первый взгляд этот новый способ может показаться

удивительно простым, по сравнению с написанием отдельной

функции Python для каждого сайта. Однако представьте, что

произойдет при переходе от системы с четырьмя сайтами-источниками к системе с 20 или 200 источниками. 

Написать список строк относительно легко, и он не займет

много места. Такой список можно загрузить из базы данных

или CSV-файла, импортировать из удаленного источника или

передать кому-то, кто не является программистом, но умеет

заполнять формы и вводить новые сайты через

пользовательский интерфейс, и этот человек никогда не увидит

ни одной строки кода. 

Конечно, здесь есть недостаток: мы отказываемся от

определенной гибкости. В первом примере у каждого сайта

есть собственная функция, написанная в свободной форме, для

выбора и — если получение результата того требует —

синтаксического анализа HTML-кода. Во втором примере все

сайты должны иметь схожую структуру, в которой

гарантированно существуют определенные поля. Полученные

из них данные должны быть чистыми, а каждому интересу-ющему нас полю следует иметь уникальный и надежный CSS-селектор. 

Однако я считаю, что широкие возможности и

относительная гибкость данного подхода более чем

компенсируют его реальные или предполагаемые недостатки. 

В следующем разделе мы рассмотрим конкретные способы

применения и варианты расширения этой базовой схемы, которые позволят, например, справляться с отсутствующими

полями, собирать различные типы данных, проверять только

определенные части сайта и хранить более сложную

информацию о страницах. 

**Структурирование веб-краулеров**

Создание гибких и изменяемых типов разметки сайтов не

принесет особой пользы, если все равно придется вручную

искать каждую ссылку, чтобы выполнить для нее веб-скрапинг. 

В предыдущей главе были показаны различные способы

извлечения данных с сайтов и автоматического поиска новых

страниц. 

В этом разделе будет показано, как встроить эти методы в

хорошо структурированный и расширяемый веб-краулер, который бы автоматически собирал ссылки и находил нужные

данные. Здесь я представлю только три основные структуры

веб-краулера, хотя считаю их пригодными к применению в

большинстве ситуаций, которые вам, скорее всего, встретятся

при извлечении данных с реальных сайтов, — разве что

придется внести пару изменений. Я также надеюсь, что если

вам попадется необычный случай, связанный с нестандартной

задачей веб-краулинга, то эти структуры послужат для вас

источником вдохновения и позволят построить веб-краулер с

красивой и надежной структурой. 

**Веб-краулинг с помощью поиска**

Один из самых простых способов сбора данных с сайта — тот

же, что используют люди: с помощью панели поиска. Поиск на

сайте по ключевому слову или теме и последующий сбор

данных по списку результатов может показаться задачей, весьма зависящей от конкретного сайта, однако несколько

ключевых моментов делают ее на удивление примитивной. 

• Большинство сайтов получают список результатов поиска по

определенной теме, передавая ее в виде строки через

параметр 

URL, 

например: 

http://example.com? 

search=мояТема. Первую часть этого URL можно сохранить

как свойство объекта Website и потом просто каждый раз

добавлять к нему тему. 

• Выполнив поиск, большинство сайтов формируют страницы

результатов в виде легко идентифицируемого списка

ссылок, обычно заключенных в удобный тег наподобие

<spanclass="result">, точный формат которого тоже

можно сохранить в виде свойства объекта Website. 

• Каждая *ссылка на результат поиска* представляет собой либо

относительный URL \(такой как /articles/page.html\), либо 

абсолютный 

\(например, 

http://example.com/articles/page.html\). Независимо

от того, какой вариант вы ожидаете — абсолютный или

относительный, — URL можно сохранить как свойство

объекта Website. 

• Найдя и нормализовав URL на странице поиска, мы успешно

сводим задачу к примеру, рассмотренному в предыдущем

разделе, — извлечению данных со страницы сайта, имеющей заданный формат. 

Рассмотрим реализацию этого алгоритма в виде кода. Класс

Content здесь почти такой же, как в предыдущих примерах. 

Мы только добавили свойство URL, которое позволит

отслеживать, откуда взят этот контент:

class Content:

"""Общий родительский класс для всех

статей/страниц""" 

 

def \_\_init\_\_\(self, topic, url, title, 

body\):

self.topic = topic

self.title = title

self.body = body

self.url = url

 

def print\(self\):

""" 

Гибкая функция печати, управляющая

выводом данных

""" 

print\('New article found for topic:

\{\}'.format\(self.topic\)\)

print\('URL: \{\}'.format\(self.url\)\)

print\('TITLE: \{\}'.format\(self.title\)\)

print\('BODY:\\n\{\}'.format\(self.body\)\)

Мы также добавили к классу Website несколько новых

свойств. Свойство searchUrl определяет, по какому адресу

следует обратиться, чтобы получить результаты поиска, если

добавить к нему искомую тему. Свойство resultListing определяет блок, в котором заключена информация о каждом

результате поиска, а resultUrl — тег внутри этого блока, содержащий точный URL результата. Свойство absoluteUrl представляет собой значение логического типа, указывающее

на то, какими ссылками являются результаты поиска —

абсолютными или относительными. 

class Website:

"""Содержит информацию о структуре сайта""" 

 

def \_\_init\_\_\(self, name, url, searchUrl, 

resultListing, 

resultUrl, absoluteUrl, titleTag, 

bodyTag\):

self.name = name

self.url = url

self.searchUrl = searchUrl

self.resultListing = resultListing self.resultUrl = resultUrl

self.absoluteUrl=absoluteUrl

self.titleTag = titleTag

self.bodyTag = bodyTag

Мы немного расширили файл crawler.py — теперь в нем

содержатся данные Website, список тем для поиска и два

цикла, в которых перебираются все темы и сайты. В этом файле

также содержится функция search, переходящая на страницу

поиска заданного сайта с заданной темой и извлекающая

оттуда все найденные URL, перечисленные на странице

результатов поиска. 

import requests

from bs4 import BeautifulSoup

 

class Crawler:

 

def getPage\(self, url\):

try:

req = requests.get\(url\)

 

 

 

 

 

 

 

 

except

requests.exceptions.RequestException:

return None

return BeautifulSoup\(req.text, 

'html.parser'\)

 

def safeGet\(self, pageObj, selector\):

childObj = pageObj.select\(selector\)

if childObj is not None and

len\(childObj\) > 0:

return childObj\[0\].get\_text\(\) return '' 

 

def search\(self, topic, site\):

""" 

Поиск на заданном сайте по заданной

теме

и сохранение всех найденных страниц. 

""" 

bs = self.getPage\(site.searchUrl \+

topic\)

 

 

 

 

 

 

 

 

searchResults 

=

bs.select\(site.resultListing\)

for result in searchResults:

url = result.select\(site.resultUrl\)

\[0\].attrs\['href'\]

\# Проверить, является ли URL

относительным или абсолютным. 

if\(site.absoluteUrl\):

bs = self.getPage\(url\)

else:

bs = self.getPage\(site.url \+

url\)

if bs is None:

print\('Something was wrong with

that page or URL. Skipping\!'\)

return

title = self.safeGet\(bs, 

site.titleTag\)

body = self.safeGet\(bs, 

site.bodyTag\)

if title \!= '' and body \!= '':

content = Content\(topic, title, body, url\)

content.print\(\)

 

crawler = Crawler\(\)

 

siteData = \[

\['O\\'Reilly Media', 'http://oreilly.com', 

'https://ssearch.oreilly.com/?q=', 

'article.product-result', 'p.title a', 

True, 'h1', 

'section\#product-description'\], 

 

 

 

 

\['Reuters', 

'http://reuters.com', 

'http://www.reuters.com/search/news?blob=', 

'div.search-result-content', 'h3.search-

result-title a', False, 'h1', 

'div.StandardArticleBody\_body\_1gnLA'\], 

\['Brookings', 'http://www.brookings.edu', 

'https://www.brookings.edu/search/?s=', 

'div.list-content article', 

'h4.title a', True, 'h1', 'div.post-body'\]

\]

sites = \[\]

for row in siteData:

sites.append\(Website\(row\[0\], row\[1\], 

row\[2\], 

row\[3\], row\[4\], 

row\[5\], row\[6\], row\[7\]\)\)

 

topics = \['python', 'data science'\]

for topic in topics:

print\('GETTING INFO ABOUT: ' \+ topic\)

for targetSite in sites:

crawler.search\(topic, targetSite\)

Этот скрипт перебирает все темы из списка topics и, прежде чем приступить к веб-скрапингу по очередной теме, выводит предупреждение:

GETTING INFO ABOUT python

Затем скрипт просматривает все сайты из списка sites и

сканирует каждый из них по каждой теме. Всякий раз, успешно

находя информацию о странице, скрипт выводит ее в консоль: New article found for topic: python

URL: http://example.com/examplepage.html

TITLE: Page Title Here

BODY: Body content is here

Обратите внимание: скрипт перебирает все темы, проходя

по всем сайтам во внутреннем цикле. Почему бы не поступить

наоборот, сначала перебрав все темы на одном сайте, а затем —

на следующем? Цикл с перебором по темам позволяет более

равномерно распределить нагрузку на веб-серверы. Это

особенно важно, если наш список состоит из нескольких сотен

тем и нескольких десятков сайтов. Не стоит направлять на

один сайт сразу несколько десятков тысяч запросов; лучше

сделать десять запросов, подождать несколько минут, затем

сделать еще десять запросов, подождать еще несколько минут

и т.д. 

В итоге общее количество запросов не изменится, однако, как правило, лучше растянуть их на максимально возможное

время. Обратите внимание: структура наших циклов

обеспечивает простой способ сделать это. 

**Сбор данных с сайтов по ссылкам**

В предыдущей главе было показано несколько способов, позволяющих обнаруживать на веб-страницах внутренние и

внешние ссылки и затем использовать их для сбора данных с

сайта. В данном подразделе мы объединим эти базовые

методы и создадим более гибкий краулер сайтов, способный

переходить по любой ссылке, соответствующей заданному

URL-шаблону. 

Веб-краулер такого типа хорошо работает для проектов, в

которых нужно собрать данные со всего сайта, а не только из

определенных результатов поиска или списка страниц. Этот

метод хорошо работает и для страниц, имеющих между собой

мало общего или совсем ничего. 

В отличие от примера сбора данных со страниц с

результатами поиска, рассмотренного выше, данные типы

краулеров не требуют структурированного метода поиска

ссылок, поэтому атрибуты, описывающие страницу поиска, в

объекте Website не нужны. Однако, поскольку краулеру не

даны конкретные инструкции о том, где и как расположены

ссылки, которые он ищет, требуются некие правила, указывающие на то, какие страницы следует выбрать. Для

этого мы предоставляем targetPattern — регулярное

выражение, описывающее нужные URL — и создаем

логическую переменную absoluteUrl:

class Website:

 

 

 

 

def 

\_\_init\_\_\(self, 

name, 

url, 

targetPattern, absoluteUrl, titleTag, bodyTag\):

self.name = name

self.url = url

self.targetPattern = targetPattern

self.absoluteUrl = absoluteUrl

self.titleTag = titleTag self.bodyTag = bodyTag

 

class Content:

 

def \_\_init\_\_\(self, url, title, body\):

self.url = url

self.title = title

self.body = body

 

def print\(self\):

print\('URL: \{\}'.format\(self.url\)\)

print\('TITLE: \{\}'.format\(self.title\)\)

print\('BODY:\\n\{\}'.format\(self.body\)\)

Класс Content — тот же класс, что и в первом примере веб-краулера. 

Класс Crawler стартует с начальной страницы сайта, находит внутренние ссылки и анализирует контент каждой из

этих внутренних ссылок:

import re

 

class Crawler:

def \_\_init\_\_\(self, site\):

self.site = site

self.visited = \[\]

 

def getPage\(self, url\):

try:

req = requests.get\(url\)



 

 

 

 

 

 

 

except

requests.exceptions.RequestException:

return None

return BeautifulSoup\(req.text, 

'html.parser'\)

 

def safeGet\(self, pageObj, selector\):

 

 

 

 

 

 

 

 

selectedElems 

=

pageObj.select\(selector\)

if selectedElems is not None and

len\(selectedElems\) > 0:

return '\\n'.join\(\[elem.get\_text\(\)

for

elem in selectedElems\]\)

return '' 

 

def parse\(self, url\):

bs = self.getPage\(url\)

if bs is not None:

title = self.safeGet\(bs, 

self.site.titleTag\)

body = self.safeGet\(bs, 

self.site.bodyTag\)

if title \!= '' and body \!= '':

content = Content\(url, title, 

body\)

content.print\(\)

 

def crawl\(self\):

""" 

Получить ссылки с начальной страницы

сайта

""" 

bs = self.getPage\(self.site.url\)

targetPages = bs.find\_all\('a', 

href=re.compile\(self.site.targetPat

tern\)\)

for targetPage in targetPages:

targetPage =

targetPage.attrs\['href'\]

if targetPage not in self.visited:

self.visited.append\(targetPage\)

if not self.site.absoluteUrl:

targetPage = '\{\}

\{\}'.format\(self.site.url, targetPage\)

self.parse\(targetPage\)

 

reuters 

= 

Website\('Reuters', 

'https://www.reuters.com', 

'^\(/article/\)', 

False, 

'h1', 'div.StandardArticleBody\_body\_1gnLA'\)

crawler = Crawler\(reuters\)

crawler.crawl\(\)

Еще одно изменение, по сравнению с предыдущими

примерами: объект Website \(в данном случае переменная

reuters\), в свою очередь, является свойством объекта

Crawler. Это позволяет удобно хранить в краулере

посещенные страницы \(visited\), но также означает

необходимость создания для каждого сайта нового краулера, вместо того чтобы многократно использовать один и тот же

для проверки списка сайтов. 

Безотносительно того, хотите ли вы, чтобы веб-краулер не

зависел от конкретных сайтов, или желаете сделать его

атрибутом объекта Crawler, это структурное решение

необходимо принимать в соответствии с конкретными

потребностями. В общем случае годится любой из данных

подходов. 

Следует также отметить: веб-краулер станет получать

ссылки с начальной страницы, но не продолжит сбор данных

после того, как все эти страницы будут пройдены. Возможно, вы захотите написать веб-краулер, работающий по одной из

схем, описанных в главе 3, который будет искать ссылки на

каждой посещенной странице. Вы даже можете пройти по всем

URL на всех страницах \(а не только на имеющих заданную

структуру\), чтобы найти URL, соответствующие заданной

структуре. 

**Сбор данных со страниц нескольких типов**

В отличие от сбора данных с заданного множества страниц

проверка всех внутренних ссылок на сайте может вызвать

проблему, поскольку вы никогда точно не знаете, что получите. 

К счастью, есть несколько основных способов определения

типа страницы. 

• *По URL* — все публикации в блогах могут содержать URL

\(например, **http://example.com/blog/title-ofpost**\). 

*• По наличию или отсутствию определенных полей* — если на

странице есть дата, но нет имени автора, то эту страницу

можно классифицировать как пресс-релиз. При наличии у

страницы заголовка, основного изображения и цены, но при

отсутствии основного контента это может быть страница

товара. 

*• *

*По *

*наличию *

*на *

*странице *

*определенных *

*тегов, *

*идентифицирующих страницу, * — теги можно использовать, даже если мы не собираем данные внутри них. Веб-краулер

может 

искать 

элемент 

наподобие

<divid="relatedproducts">, чтобы идентифицировать

страницу как страницу товара, даже если вас не интересуют

сопутствующие товары. 

Чтобы отслеживать несколько типов страниц, вам

понадобится создать на Python несколько типов объектов

страниц. Это можно сделать следующими двумя способами. 

Если страницы похожи \(имеют в целом одинаковые типы

контента\), то можно добавить к существующему объекту веб-страницы атрибут pageType:

class Website:

def \_\_init\_\_\(self, name, url, titleTag, 

bodyTag, pageType\):

self.name = name

self.url = url

self.titleTag = titleTag

self.bodyTag = bodyTag

self.pageType = pageType

Если страницы хранятся в SQL-подобной базе данных, то

такой тип структуры страниц указывает на вероятное хранение

этих страниц в одной таблице, в которую будет добавлено поле

pageType. 

Если же страницы или контент, который вы ищете, заметно

различаются \(содержат поля разных типов\), то может

понадобиться создать отдельные объекты для каждого типа

страниц. Конечно, все веб-страницы будут иметь нечто общее:

URL и, вероятно, имя или заголовок. Это идеальная ситуация

для использования подклассов:

class Webpage:

def \_\_init\_\_\(self, name, url, titleTag\):

self.name = name

self.url = url

self.titleTag = titleTag

Веб-краулер не будет использовать этот объект напрямую, однако на него будут ссылаться типы страниц:

class Product\(Website\):

"""Содержит информацию для веб-скрапинга

страницы товара""" 

def \_\_init\_\_\(self, name, url, titleTag, 

productNumberTag, priceTag\):

Website.\_\_init\_\_\(self, name, url, 

TitleTag\)

self.productNumberTag =

productNumberTag

self.priceTag = priceTag

 

class Article\(Website\):

"""Содержит информацию для веб-скрапинга

страницы статьи""" 

def \_\_init\_\_\(self, name, url, titleTag, 

bodyTag, dateTag\):

Website.\_\_init\_\_\(self, name, url, 

titleTag\)

self.bodyTag = bodyTag

self.dateTag = dateTag

Страница Product расширяет базовый класс Website, добавляя к нему атрибуты productNumber и price, относящиеся лишь к товарам, а класс Article добавляет

атрибуты body и date, которые к товарам неприменимы. 

Эти два класса можно использовать, например, для веб-скрапинга интернет-магазина, на сайте которого содержатся не

только товары, но и публикации в блоге или пресс-релизы. 

**Размышления о моделях веб-краулеров**

Собирать информацию из Интернета — словно пить из

пожарного шланга. Ее слишком много, и не всегда понятно, что

именно вам нужно и как это получить. Ответ на эти вопросы

должен стать первым шагом в любом крупном \(а иногда и

небольшом\) веб-проекте. 

При сборе одних и тех же данных в разных областях или из

разных источников ваша цель почти всегда должна состоять в

том, чтобы попытаться нормализовать эти данные. Работать с

данными, имеющими одинаковые или сопоставимые поля, гораздо проще, чем с данными, формат которых полностью

определяется их первоначальным источником. 

Во многих случаях приходится создавать веб-скраперы, исходя из вероятного появления новых источников данных в

дальнейшем и с целью минимизировать издержки на

программирование, которые потребуются для добавления этих

новых источников. Даже если на первый взгляд сайт не

соотносится с вашей моделью, возможно, есть более тонкие

способы обеспечить данное соответствие. В долгосрочной

перспективе умение замечать эти базовые закономерности

позволит вам сэкономить время и деньги, а также избавит от

множества неприятностей. 

Кроме того, не следует игнорировать взаимосвязи между

фрагментами данных. Предположим, вы ищете информацию, у

которой в разных источниках есть свойства «тип», «размер»

или «тема». Как вы будете хранить, извлекать и осмысливать

эти атрибуты? 

Архитектура ПО — обширная и важная тема, освоению

которой можно посвятить всю карьеру. К счастью, программная архитектура для веб-скрапинга — гораздо более

ограниченный и управляемый набор навыков, приобретаемых

относительно легко. Занимаясь веб-скрапингом данных, вы, скорее всего, со временем заметите одни и те же постоянно

повторяющиеся базовые закономерности. Чтобы создать

хорошо структурированный веб-скрапер, не нужно много

тайных знаний, но потребуется уделить время обдумыванию

проекта в целом. 

**Глава 5. Scrapy**

В предыдущей главе мы рассмотрели некоторые методы и

схемы построения больших, масштабируемых и \(что самое

важное\!\) удобных в сопровождении веб-краулеров. Несмотря

на то что их несложно разработать вручную, есть множество

библиотек, фреймворков и даже инструментов с графическим

интерфейсом, которые сделают это за вас или по крайней мере

постараются немного упростить вам жизнь. 

В этой главе вы познакомитесь с одной из лучших платформ

для разработки веб-краулеров — Scrapy. Когда я работала над

первым изданием данной книги, Scrapy для Python 3.x еще не

была выпущена, поэтому ее упоминание в книге ограничилось

одним разделом. С тех пор библиотека стала поддерживать

Python 3.3\+, в ней появились дополнительные функции, и я с

удовольствием уделю ей не раздел, а целую главу. 

Одна из проблем создания веб-краулеров состоит в том, что

часто приходится выполнять одни и те же задачи: находить все

ссылки на странице, оценивать разницу между внутренними и

внешними ссылками, переходить на новые страницы. Эти

основные стандартные операции полезно знать и уметь писать

с нуля, но библиотека Scrapy способна многое из упомянутого

сделать автоматически. 

Конечно, Scrapy не читает мысли. Вам по-прежнему

необходимо описать шаблоны страниц, указать точку, с

которой следует начать работу, и построить правила для URL

искомых страниц. Но и в этих случаях библиотека

предоставляет чистую основу для построения четко

структурированного кода. 

**Установка Scrapy**

Scrapy предоставляет инструмент для скачивания библиотеки с

ее сайта \(**http://scrapy.org/download/**\), а также инструкции по

установке с помощью сторонних менеджеров, таких как pip. 

Из-за сравнительно большого размера и сложности Scrapy обычно нельзя установить, как другие фреймворки, с помощью

такой команды:

$ pip install Scrapy

Обратите внимание: я говорю «обычно», поскольку

теоретически можно использовать и эту команду. Однако на

практике я, как правило, сталкиваюсь с одной или

несколькими 

сложными 

проблемами, 

связанными 

с

зависимостями, несовпадением версий и неразрешимыми

ошибками. 

Если вы решили установить Scrapy с помощью pip, то

настоятельно 

рекомендую 

использовать 

виртуальное

окружение \(подробнее о виртуальных окружениях см. во врезке

«Хранение библиотек непосредственно в виртуальных

окружениях» на с. 28\). 

Я предпочитаю другой способ установки — с помощью

менеджера 

пакетов 

Anaconda

\(**https://docs.continuum.io/anaconda/**\). Это программный продукт, производимый компанией Continuum и предназначенный для

того, чтобы сглаживать острые углы при поиске и установке

популярных пакетов Python для обработки данных. В

следующих главах мы будем использовать многие другие

пакеты, которыми управляет Anaconda, такие как NumPy и

NLTK. 

После установки Anaconda можно установить Scrapy с

помощью следующей команды:

conda install -c conda-forge scrapy Если у вас возникнут проблемы или потребуется свежая

информация, то обратитесь к руководству по установке Scrapy \(**https://doc.scrapy.org/en/latest/intro/install.html**\). 

**Инициализация нового «паука». ** После установки

платформы Scrapy необходимо выполнить небольшую

настройку для каждого *«паука»* \(spider\) — проекта Scrapy, который, как и обычный паук, занимается обходом сети. В этой

главе я буду называть «пауком» именно проект Scrapy, а

краулером — любую программу, которая занимается сбором

данных во Всемирной паутине, независимо от того, использует

она Scrapy или нет. 

Чтобы создать нового «паука» в текущем каталоге, нужно

ввести в командной строке следующую команду:

$ scrapy startproject wikiSpider

В результате в каталоге будет создан новый подкаталог, а в

нем — проект под названием wikiSpider. Внутри этого

каталога находится следующая файловая структура: scrapy.cfg

wikiSpider

— spiders

— \_\_init.py\_\_

— items.py

— middlewares.py

— pipelines.py

— settings.py

— \_\_init.py\_\_

Поначалу в эти файлы Python записывается код-заглушка, что позволяет быстро создать новый проект «паука». В

следующих разделах данной главы мы продолжим работать с

проектом wikiSpider. 

**Пишем простой веб-скрапер**

Чтобы создать веб-краулер, нужно добавить в дочерний

каталог 

**wikiSpider** 

новый 

файл

wikiSpider/wikiSpider/article.py. Затем в этом файле

article.py нужно написать следующее:

import scrapy

 

class ArticleSpider\(scrapy.Spider\):

name='article' 

 

def start\_requests\(self\):

urls = \[

'http://en.wikipedia.org/wiki/Pytho

n\_' 

'%28programming\_language%29', 

'https://en.wikipedia.org/wiki/Func

tional\_programming', 

'https://en.wikipedia.org/wiki/Mont

y\_Python'\]

return \[scrapy.Request\(url=url, 

callback=self.parse\)

for url in urls\]

 

def parse\(self, response\):

url = response.url



 

 

 

 

 

 

 

title 

=

response.css\('h1::text'\).extract\_first\(\)

print\('URL is: \{\}'.format\(url\)\)

print\('Title is: \{\}'.format\(title\)\)

Имя этого класса \(ArticleSpider\) отличается от имени

каталога \(**wikiSpider**\), что указывает на следующее: данный

класс отвечает за просмотр только страниц статей в рамках

более широкой категории **wikiSpider**, которую мы впоследствии

сможем использовать для поиска других типов страниц. 

Для больших сайтов с разными типами контента можно

создать отдельные элементы Scrapy для каждого типа

\(публикации в блогах, пресс-релизы, статьи и т.п.\). У каждого

из этих типов будут свои поля, но все они станут работать в

одном проекте Scrapy. Имя каждого «паука» должно быть

уникальным в рамках проекта. 

Следует обратить внимание еще на две важные вещи: функции start\_requests и parse. 

Функция start\_requests — это предопределенная Scrapy точка входа в программу, используемая для генерации

объектов Request, которые в Scrapy применяются для сбора

данных с сайта. 

Функция parse — это функция обратного вызова, определяемая пользователем, которая передается в объект

Request с помощью callback=self.parse. Позже мы

рассмотрим более мощные трюки, которые возможны

благодаря функции parse, но пока что она просто выводит

заголовок страницы. 

Для запуска «паука» article нужно перейти в каталог

**wikiSpider/wikiSpider** и выполнить следующую команду: $ scrapy runspider article.py

По умолчанию Scrapy выводит довольно подробные данные. 

Помимо отладочной информации, это будут примерно такие

строки:

2018-01-21 23:28:57 \[scrapy.core.engine\] DEBUG:

Crawled \(200\)

<GET 

https://en.wikipedia.org/robots.txt> 

\(referer: None\)

2018-01-21 

23:28:57

\[scrapy.downloadermiddlewares.redirect\]

DEBUG: 

Redirecting 

\(301\) 

to 

<GET

https://en.wikipedia.org/wiki/

Python\_%28programming\_language%29> from <GET

http://en.wikipedia.org/

wiki/Python\_%28programming\_language%29> 

2018-01-21 23:28:57 \[scrapy.core.engine\] DEBUG:

Crawled \(200\)

<GET

https://en.wikipedia.org/wiki/Functional\_progra

mming> 

\(referer: None\)

URL 

is:

https://en.wikipedia.org/wiki/Functional\_progra

mming

Title is: Functional programming

2018-01-21 23:28:57 \[scrapy.core.engine\] DEBUG:

Crawled \(200\)

<GET

https://en.wikipedia.org/wiki/Monty\_Python> 

\(referer: None\)

URL 

is:

https://en.wikipedia.org/wiki/Monty\_Python

Title is: Monty Python

![Image 39](images/000073.png)

Веб-скрапер проходит по трем страницам, указанным в

списке urls, собирает с них информацию и завершает работу. 

 
