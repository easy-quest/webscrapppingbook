**Последнее замечание о JavaScript**

JavaScript используется на большинстве современных сайтов20. На

наше счастье, такое его применение по большей части не влияет на

веб-скрапинг страницы. JavaScript может ограничиваться, например, активацией механизмов контроля, управлением небольшим

разделом сайта или выпадающим меню. В тех случаях, когда это

влияет на веб-скрапинг сайта, код JavaScript легко выполняется с

помощью таких инструментов, как Selenium, позволяющих создать

простую HTML-страницу, которую вы уже научились читать в первой

части этой книги. 

Запомните: если на сайте задействован JavaScript, то это еще не

значит, что все традиционные инструменты веб-скрапинга можно

отбросить. Ведь главная цель JavaScript, в конце концов, формирование кода HTML и CSS для отображения в браузере или

динамического взаимодействия с сервером с помощью HTTP-запросов и ответов на них. При использовании Selenium код HTML и

CSS на странице можно прочитать и проанализировать так же, как

любой другой код сайта, а HTTP-запросы и ответы веб-скрапер

может отправить и обработать с помощью методов, описанных в

предыдущих главах, даже без применения Selenium. 

Кроме того, JavaScript может даже оказаться весьма уместным для

веб-скрапинга, поскольку его применение в качестве «системы

управления контентом на стороне браузера» может служить

полезным API для связи с внешним миром, позволяя получать

данные более непосредственно. Подробнее об этом вы узнаете в

главе 12. 

Если у вас все же возникнут проблемы с особенно запутанным

JavaScript-кодом, то в главе 14 вы найдете информацию о Selenium и

о непосредственном взаимодействии с динамическими сайтами, включая интерфейсы типа drag-and-drop. 

18 Более подробную статистику см. в статье The State of jQuery 2014

\(https://blog.jquery.com/2014/01/13/the-state-of-jquery-2014/\), опубликованной в блоге Дэйва

Метвина \(Dave Methvin\) 13 января 2014 года. 

19 W3Techs, Usage Statistics and Market Share of Google Analytics for Websites \(http://w3techs.com/technologies/details/ta-googleanalytics/all/all\). 

20 W3Techs, Usage of JavaScript for Websites \(http://w3techs.com/technologies/details/cp-javascript/all/all\). 

**Глава 12. Веб-краулинг с помощью API** JavaScript 

традиционно 

принято считать вселенским

проклятием веб-краулеров. Давным-давно были времена, когда вы могли быть уверены, что запрос, отправленный вами

на веб-сервер, получит те же данные, которые пользователь

увидит в своем браузере, сделав тот же запрос. 

По мере распространения методов генерации и загрузки

контента с помощью JavaScript и Ajax описанная выше

ситуация становится все менее привычной. В главе 11 мы

рассмотрели один из способов решения указанной проблемы: использование Selenium для автоматизации браузера и

извлечения данных. Это легко сделать. Это работает почти

всегда. 

Проблема в том, что, когда у вас в руках есть столь мощный

и эффективный «молоток», как Selenium, каждая задача веб-скрапинга начинает походить на гвоздь. 

В текущей главе вы узнаете о том, как, минуя весь этот

JavaScript \(не выполняя и даже не загружая его\!\), получить

прямой доступ к источнику данных — к интерфейсам API, которые генерируют эти данные. 

**Краткое введение в API**

Существует бесчисленное количество книг, докладов и

руководств о нюансах API REST, GraphQL21,  JSON и XML, однако

все они основаны на одной простой концепции. *API* определяет

стандартизованный синтаксис, который позволяет одной

программе взаимодействовать с другой, даже если они

написаны на разных языках или имеют разную структуру. 

Данный раздел посвящен веб-API \(в особенности

позволяющим веб-серверу взаимодействовать с браузером\), и

здесь мы будем понимать под API именно этот тип

интерфейсов. Но вы можете учесть, что в других контекстах *API* также является обобщенным термином, который может

обозначать, например, интерфейс, позволяющий программе на

Java взаимодействовать с программой на Python, работающей

на том же компьютере. API не всегда означает «интерфейс

через Интернет» и не обязательно должен включать в себя

какие-либо веб-технологии. 

Веб-API чаще всего используются разработчиками для

взаимодействия с широко разрекламированными и хорошо

документированными открытыми сервисами. Например, американский кабельный спортивный телевизионный канал

ESPN 

предоставляет 

API

\(**http://www.espn.com/apis/devcenter/docs/**\) для 

получения

информации о спорт сменах, счетах в играх и др. У Google в

разделе 

для 

разработчиков

\(**https://console.developers.google.com**\) есть десятки API для

языковых переводов, аналитики и геолокации. 

В документации всех этих API обычно описываются

маршруты или *конечные точки* в виде URL, которые можно

запрашивать, с изменяемыми параметрами, либо входящими в

состав этих URL, либо выступающими в роли GET-параметров. 

Например, в следующем URL pathparam является

параметром пути:

http://example.com/the-api-route/pathparam

А здесь pathparam является значением параметра param1: http://example.com/the-api-route? 

param1=pathparam

Оба метода передачи данных в API используются

достаточно широко, хотя, как и многие другие аспекты

информатики, являются предметом жарких философских

дискуссий о том, когда и где переменные следует передавать

через путь, а когда — через параметры. 

Ответ на API-запрос обычно возвращается в формате JSON

или XML. В настоящее время JSON гораздо популярнее, чем

XML, но последний иногда тоже встречается. Многие API позволяют выбирать тип ответа, обычно с помощью еще

одного параметра, определяющего, какой тип ответа вы хотите

получить. 

Вот пример ответа на API-запрос в формате JSON:

\{"user":\{"id": 123, "name": "Ryan Mitchell", 

"city": "Boston"\}\}

А вот ответ на API-запрос в формате XML:

<user><id>123</id><name>Ryan Mitchell</name> 

<city>Boston</city></user> 

Сайт **ip-api.com** \(**http://ip-api.com/**\) имеет понятный и

удобный API, который преобразует IP-адреса в реальные

физические адреса. Вы можете попробовать выполнить

простой запрос API, введя в браузере следующее22:

http://ip-api.com/json/50.78.253.58

В результате вы получите примерно такой ответ:

\{"ip":"50.78.253.58","country\_code":"US","count ry\_name":"United States", 

"region\_code":"MA","region\_name":"Massachuset ts","city":"Boston", 

"zip\_code":"02116","time\_zone":"America/New\_Y

ork","latitude":42.3496, 

"longitude":-71.0746,"metro\_code":506\}

Обратите внимание: в запросе есть параметр пути json. 

Чтобы получить ответ в формате XML или CSV, нужно заменить

его на соответствующий формат:

http://ip-api.com/xml/50.78.253.58

http://ip-api.com/csv/50.78.253.58

**API и HTTP-методы**

В предыдущем разделе мы рассмотрели API, отправляющие на

сервер GET-запрос для получения информации. Существует

четыре основных способа \(или метода\) запроса информации с

веб-сервера через HTTP:

•GET; 

• POST; 

• PUT; 

• DELETE. 

Технически типов запросов больше четырех \(например, еще

есть HEAD, OPTIONS и CONNECT\), но они редко используются в

API и маловероятно, что когда-либо встретятся вам. 

Подавляющее большинство API ограничиваются этими

четырьмя методами, а иногда даже какой-то их частью. 

Постоянно встречаются API, которые используют только GET

или только GET и POST. 

GET — тот запрос, который вы используете, когда посещаете

сайт, введя его адрес в адресной строке браузера. Обращаясь по

адресу **http://ip-api.com/json/50.78.253.58**, вы применяете

именно метод GET. Такой запрос можно представить как

команду: «Эй, веб-сервер, будь добр, выдай мне эту

информацию». 

Запрос GET по определению не вносит изменений в

содержимое базы данных сервера. Ничего не сохраняется и

ничего не изменяется. Информация только считывается. 

POST — запрос, который используется при заполнении

формы или отправке информации, предположительно

предназначенной для обработки серверным скриптом. Каждый

раз, авторизуясь на сайте, вы делаете POST-запрос, передавая

имя пользователя и \(как мы надеемся\) зашифрованный пароль. 

Делая POST-запрос через API, вы говорите серверу: «Будь

любезен, сохрани эту информацию в базе данных». 

Запрос PUT при взаимодействии с сайтами используется

реже, но время от времени встречается в API. Этот запрос

применяется для изменения объекта или информации. 

Например, в API можно задействовать запрос POST для

создания пользователя и запрос PUT для изменения его адреса

электронной почты23. 

Запросы DELETE, как нетрудно догадаться, служит для

удаления объекта. Например, если отправить запрос DELETE по

адресу **http://myapi.com/user/23**, то будет удален пользователь с

идентификатором 23. Методы DELETE нечасто встречаются в

открытых API, поскольку те в основном создаются для

распространения 

информации 

или 

чтобы 

позволить

пользователям создавать или публиковать информацию, но не

удалять ее из баз данных. 

В отличие от GET запросы POST, PUT и DELETE позволяют

передавать информацию в теле запроса, в дополнение к URL

или маршруту, с которого запрашиваются данные. 

Как и ответ, получаемый от веб-сервера, эти данные в теле

запроса обычно представляются в формате JSON или реже в

формате XML. Конкретный формат данных определяется

синтаксисом API. Например, при использовании API, который

добавляет комментарии к сообщениям в блоге, можно создать

следующий PUT-запрос:

http://example.com/comments?post=123

с таким телом запроса:

\{"title": "Great post about APIs\!", "body":

"Very informative. Really helped me out with a tricky technical challenge I was facing. Thanks

for taking the time to write such a detailed blog post about PUT requests\!", "author":

\{"name": 

"Ryan 

Mitchell", 

"website":

"http://pythonscraping.com", 

"company":

"O'Reilly Media"\}\}

Обратите внимание: идентификатор сообщения в блоге

\(123\) передается в качестве параметра в URL, а контент

создаваемого нами комментария — в теле запроса. Параметры

и данные могут передаваться и в параметре, и в теле запроса. 

Какие параметры обязательны и где передаются — опять-таки

определяется синтаксисом API. 

**Подробнее об ответах на API-запросы**

Как мы видели в примере с сайтом **ip-api.com** в начале данной

главы, важной особенностью API является то, что эти

интерфейсы возвращают хорошо отформатированные ответы. 

Наиболее распространенные форматы ответов —XML

\(eXtensible Markup Language — расширяемый язык разметки\) и

JSON \(JavaScript Object Notation — нотация объектов JavaScript\). 

В последние годы JSON стал намного популярнее, чем XML, по нескольким основным причинам. Во-первых, файлы JSON

обычно меньше, чем хорошо проработанные файлы XML. 

Сравните, например, следующие данные в формате XML, занимающие 98 символов:

<user><firstname>Ryan</firstname> 

<lastname>Mitchell</lastname> 

<username>Kludgist</username></user> А теперь посмотрите на те же данные в формате JSON:

\{"user":

\{"firstname":"Ryan","lastname":"Mitchell","user name":"Kludgist"\}\}

Это всего 73 символа, на целых 36 % меньше, чем те же

данные в формате XML. 

Конечно, 

вероятен 

аргумент, 

что 

XML 

можно

отформатировать так:

<user 

firstname="ryan" 

lastname="mitchell" 

username="Kludgist"></user> 

Но это не рекомендуется, поскольку такое представление не

поддерживает глубокое вложение данных. И все равно запись

занимает 71 символ — примерно столько же, сколько

эквивалентный JSON. 

Другая причина, по которой JSON так быстро становится

более популярным, чем XML, связана с изменением веб-

технологий. Раньше получателями API были по большей части

серверные скрипты на PHP или .NET. Сейчас вполне может

оказаться, что получать и отправлять вызовы API будет

фреймворк наподобие Angular или Backbone. Серверным

технологиям до определенной степени безразлично, в какой

форме к ним поступают данные. Однако библиотекам

JavaScript, таким как Backbone, проще обрабатывать JSON. 

Принято считать, что API возвращают ответ либо в формате

XML, либо в формате JSON, однако возможен любой другой

вариант. Тип ответа API ограничен только воображением

программиста, создавшего этот интерфейс. Еще один

типичный формат ответа — CSV \(как видно из примера с **ip-api.com**\). Отдельные API даже позволяют создавать файлы. 

Можно отправить на сервер запрос, по которому будет

сгенерировано изображение с наложенным на него заданным

текстом, или же запросить определенный файл XLSX или PDF. 

Некоторые API вообще не возвращают ответа. Например, если отправить на сервер запрос для создания комментария к

записи в блоге, то он может вернуть только HTTP-код ответа

200, что означает: «Я опубликовал комментарий; все в

порядке\!» Другие запросы могут возвращать минимальный

ответ наподобие такого:

\{"success": true\}

В случае ошибки вы можете получить такой ответ:

\{"error": \{"message": "Something super bad happened"\}\}

Или же, если API не очень хорошо сконфигурирован, вы

можете получить не поддающуюся анализу трассировку стека

или некий текст на английском. Отправляя запрос к API, как

правило, имеет смысл сначала убедиться, что получаемый

ответ действительно имеет формат JSON \(или XML, или CSV, или любой другой формат, который вы ожидаете получить\). 

**Синтаксический анализ JSON**

В данной главе мы рассмотрели различные типы API и их

функционирование, 

а 

также 

примеры JSON-ответов, 

полученных от этих API. Теперь посмотрим, как можно

анализировать и использовать эту информацию. 

В начале главы мы рассмотрели пример API **ip-api.com**, который преобразует IP-адреса в физические:

http://ip-api.com/json/50.78.253.58

Мы можем взять результат этого запроса и использовать

функции Python для синтаксического анализа JSON, чтобы его

декодировать:

import json

from urllib.request import urlopen

 

def getCountry\(ipAddress\):

 

 

 

 

response 

= 

urlopen\('http://ip-

api.com/json/'\+ipAddress\).read\(\)

.decode\('utf-8'\)

responseJson = json.loads\(response\)

return responseJson.get\('country\_code'\)

 

print\(getCountry\('50.78.253.58'\)\)

Эта программа выводит код страны для IP-адреса

50.78.253.58. 

Здесь используется библиотека синтаксического анализа

JSON, которая является частью базовой библиотеки Python. 

Чтобы ее подключить, нужно всего лишь поставить вверху

строку importjson\! В отличие от многих других языков, способных преобразовать строку JSON в специальный объект

JSON или узел JSON, в Python задействован более гибкий

подход: объекты JSON превращаются в словари, массивы JSON

— в списки, строки JSON — в строки и т.д. Таким образом, получить доступ и манипулировать значениями, хранящимися

в JSON, чрезвычайно легко. 

Ниже представлена краткая демонстрация того, как Python-библиотека JSON обрабатывает значения, которые встречаются

в строках JSON:

import json

 

jsonString 

= 

'\{"arrayOfNums":\[\{"number":0\}, 

\{"number":1\},\{"number":2\}\], 

"arrayOfFruits":

\[\{"fruit":"apple"\},\{"fruit":"banana"\}, 

\{"fruit":"pear"\}

\]\}' 

jsonObj = json.loads\(jsonString\)

 

print\(jsonObj.get\('arrayOfNums'\)\)

print\(jsonObj.get\('arrayOfNums'\)\[1\]\)

print\(jsonObj.get\('arrayOfNums'\)

\[1\].get\('number'\) \+

 

 

 

 

 

 

jsonObj.get\('arrayOfNums'\)

\[2\].get\('number'\)\)

print\(jsonObj.get\('arrayOfFruits'\)

\[2\].get\('fruit'\)\)

Результат выглядит так:

\[\{'number': 0\}, \{'number': 1\}, \{'number': 2\}\]

\{'number': 1\}

3

pear

Первая строка — список словарных объектов, вторая —

словарный объект, третья — целое число \(сумма целых чисел, доступных в словарях\), а четвертая — просто строка. 

**Недокументированные API**

До сих пор в данной главе мы обсуждали только

документированные API. Их разработчики предполагают, что

эти API будут открытыми, публикуют информацию о них и

рассчитывают на их использование другими разработчиками. 

Однако у подавляющего большинства API вообще нет никакой

открытой документации. 

Но зачем же создавать API без открытой документации? Как

упоминалось в начале этой главы, все дело в JavaScript. 

Традиционно веб-серверы для динамических сайтов при

каждом запросе страницы пользователем выполняли

следующие задачи:

• 

обрабатывали 

GET-запросы 

от 

пользователей, 

запрашивающих страницу сайта; 

• получали из базы данные, которые должны появляться на

этой странице; 

• вставляли эти данные в HTML-шаблон страницы; 

• 

отправляли 

этот 

отформатированный 

HTML-код

пользователю. 

По мере распространения фреймворков JavaScript многие из

задач по созданию HTML-кода, выполняемых сервером, были

перенесены в браузер. Сервер может отправить в браузер

пользователя жестко закодированный HTML-шаблон, но для

загрузки контента и размещения его в правильных местах

этого шаблона будут выполняться отдельные запросы Ajax. Все

это станет происходить на стороне браузера/клиента. 

Поначалу описанная ситуация представляла проблему для

веб-скраперов, привыкших делать запрос на HTML-страницу и

возвращать именно ее со всем уже имеющимся на ней

контентом. Вместо этого теперь веб-скраперы получают HTML-шаблон без какого-либо наполнения. 

Для решения данной проблемы был создан Selenium. Теперь

веб-скрапер может играть роль браузера для программиста, запрашивая HTML-шаблон, выполняя любой JavaScript, загружая все данные на их место и только *потом* производя

веб-скрапинг этих данных. Поскольку весь HTML-код был

загружен, задача, по сути, сводится к уже решенной ранее: к

синтаксическому анализу и форматированию существующего

HTML-кода. 

Однако, поскольку вся система управления контентом

\(которая раньше размещалась исключительно на веб-сервере\), по существу, переместилась в клиентский браузер, содержимое

даже самых простых сайтов может раздуться до нескольких

мегабайтов и дюжины HTTP-запросов. 

Кроме того, в случае применения Selenium загружаются все

«дополнения», которые не обязательно нужны пользователю: вызовы программ отслеживания, загрузка рекламы на боковых

панелях, вызовы программ отслеживания для нее. 

Изображения, CSS, внешние шрифты — все их необходимо

загрузить. Это может показаться отличным решением при

использовании браузера для просмотра веб-страниц, однако

если вы пишете веб-скрапер, который должен быстро

перемещаться по страницам, собирать конкретные данные и

создавать как можно меньше нагрузки на веб-сервер, то может

оказаться, что вы загружаете данных в 100 раз больше, чем

требуется. 

Но у всех этих JavaScript-, Ajax- и веб-модернизаций есть и

преимущество: поскольку серверы больше не преобразуют

данные в формат HTML, они часто играют роль тонкой

оболочки вокруг базы данных. Она просто извлекает данные из

базы и передает их на страницу через API. 

Разумеется, эти API не предназначены для использования

кем-то или чем-то, кроме самой веб-страницы, и, как

следствие, разработчики оставляют их без документации и

предполагают \(или надеются\), что никто не заметит этих API. 

Но они существуют. 

Например, сайт New York Times \(**http://nytimes.com**\) загружает все результаты поиска через JSON. Если вы пройдете

по ссылке:

https://query.nytimes.com/search/sitesearch/\#/p

ython

то получите последние новостные статьи по поисковому

запросу python. Выполнив веб-скрапинг этой страницы с

помощью urllib или библиотеки Requests, вы не найдете

результатов поиска. Они загружаются отдельно через

следующий вызов API:

https://query.nytimes.com/svc/add/v1/sitesearch

.json?q=python&spotlight=

![Image 63](images/000039.png)

true&facet=true

Если бы мы загрузили эту страницу с помощью Selenium, то

нам пришлось бы для каждого поиска сделать примерно 100

запросов и передать 600–700 Кбайт данных. Используя API напрямую, мы делаем только один запрос и передаем всего

лишь около 60 Кбайт красиво отформатированных данных —

именно тех, которые нам нужны. 

**Поиск недокументированных API**

В предыдущих главах мы использовали инспектор Chrome для

проверки контента HTML-страницы. Теперь применим этот

инспектор для несколько иной цели: исследования запросов и

ответов на вызовы функций, которые задействуются при

формировании страницы. 

Для этого откройте окно инспектора Chrome и перейдите на

вкладку **Network** \(Сеть\), как показано на рис. 12.1. 

 

**Рис. 12.1. ** Инструмент Chrome Network Inspector позволяет увидеть все вызовы функций, которые браузер делает и получает извне

Обратите внимание: это окно нужно открыть до загрузки

страницы. Будучи закрытым, оно не отслеживает сетевые

вызовы. 

При загрузке страницы вы увидите строку, изменяющуюся в

реальном времени, пока браузер обращается к веб-серверу с

целью получить дополнительную информацию, необходимую

для отображения страницы. Среди этих вызовов могут быть и

вызовы API. 

Поиск недокументированных API может потребовать

небольшого детективного расследования \(о том, как это

сделать, см. в подразделе «Автоматический поиск и

документирование API» на с. 223\), особенно на крупных сайтах

с большим количеством сетевых вызовов. Однако в

большинстве случаев вы увидите это сразу. 

Как правило, у вызовов API есть несколько свойств, отличающих их от других сетевых вызовов. 

• Вызовы API часто содержат JSON или XML. Вы можете

отфильтровать список таких запросов с помощью поля

поиска и фильтрации. 

• При GET-запросах URL обычно содержит значения

передаваемых параметров. Это удобно, если, например, вы

ищете вызов API, который возвращает результаты поиска

или загружает данные для конкретной страницы. Просто

отфильтруйте результаты по используемому поисковому

запросу, 

идентификатору 

страницы 

или 

другой

идентифицирующей информации. 

• Обычно вызовы API относятся к типу XHR. 

API не всегда заметны, особенно на больших сайтах с

большим количеством функций, которые делают сотни

вызовов при загрузке одной страницы. Однако со временем, приобретя немного практики, вы будете гораздо легче

находить эту метафорическую иголку в стоге сена. 

**Документирование недокументированных API** После обнаружения вызова API часто бывает полезно хотя бы

немного документировать его, особенно если ваши веб-скраперы будут в значительной степени опираться на данный

вызов. Вероятно, вы захотите загрузить несколько страниц

сайта, отфильтровывая нужный вызов API на вкладке **Network** \(Сеть\) в консоли инспектора. При этом вы можете заметить, как изменяется вызов от страницы к странице, и определить

поля, которые он принимает и возвращает. 

Каждый вызов API можно идентифицировать и

задокументировать, обратив внимание на следующие поля:

• используемый метод HTTP; 

• входные данные:

• параметры пути; 

• заголовки \(включая cookies\); 

• контент тела \(для вызовов PUT и POST\); 

• выходные данные:

• заголовки ответа \(включая набор cookie-файлов\); 

• тип тела ответа; 

• поля тела ответа. 

**Автоматический поиск и документирование API**

Работа по поиску и документированию API может показаться

несколько утомительной и рутинной. Вероятно, потому, что по

большей части так оно и есть. Некоторые сайты иногда

пытаются скрыть, как браузер получает от них данные, и это

несколько 

усложняет 

задачу, 

однако 

поиском 

и

документированием API должны заниматься программы. 

Я 

создала 

на 

GitHub 

репозиторий 

по 

адресу

**https://github.com/REMitchell/apiscraper**, в котором попыталась

выполнить некоторые базовые задачи из этой области. 

Для загрузки страниц, сбора данных со страниц одного

домена, анализа сетевого трафика, возникающего во время

загрузки страниц, и преобразования этих запросов в читаемые

вызовы API я использовала Selenium, ChromeDriver и

библиотеку BrowserMob Proxy. 

Для запуска данного проекта нам потребуется несколько

важных частей, и прежде всего — само программное

обеспечение. 

Клонируйте 

проект 

GitHub 

apiscraper

\(**https://github.com/REMitchell/apiscraper**\). Он должен включать

следующие файлы:

•sapicall.py — содержит атрибуты, определяющие вызов

API \(путь, параметры и т.д.\), а также логику, позволяющую

установить, являются ли два вызова API одинаковыми; 

• apiFinder.py — главный класс для веб-краулинга. 

Используются webservice.py и consoleservice.py для

запуска процесса поиска API; 

• browser.py — имеет только три метода: initialize, get и close, но выполняет весьма сложные действия, позволяющие связать прокси-сервер BrowserMob и Selenium. 

Прокручивает страницу с целью убедиться, что она

загружена целиком, сохраняет файлы HTTP Archive \(HAR\) в

заданном месте для обработки; 

• consoleservice.py — выполняет команды из консоли и

запускает основной класс APIFinder; 

• harParser.py — анализирует HAR-файлы и извлекает

вызовы API; 

• html\_template.html — предоставляет шаблон для

отображения вызовов API в браузере; 

• README.md — страница readme для Git. 

Скачайте с **https://bmp.lightbody.net/** двоичные файлы

BrowserMob Proxy и поместите их в каталог проекта **apiscraper**. 

На момент написания данной книги последней версией

BrowserMob Proxy была 2.1.4, вследствие чего этот скрипт

предполагает, что двоичные файлы находятся в каталоге

**browsermob-proxy-2.1.4/bin/browsermob-proxy** 

относительно

корневого каталога проекта. Если это не так, то вы можете

указать другой каталог во время выполнения или \(что, возможно, проще\) внести изменения в код в apiFinder.py. 

Скачайте 

ChromeDriver

\(**https://sites.google.com/a/chromium.org/chromedriver/downloads**\) и поместите его в каталог проекта **apiscraper**. 

Вам необходимо будет установить следующие библиотеки

Python:

•tldextract; 

• selenium; 

• browsermob-proxy. 

Когда эти операции по настройке будут завершены, можно

начинать собирать вызовы API. Для начала введите команду: $ python consoleservice.py -h

чтобы получить список параметров:

usage: consoleservice.py \[-h\] \[-u \[U\]\] \[-d \[D\]\]

\[-s \[S\]\] \[-c \[C\]\] \[-i \[I\]\] \[--p\]

 

optional arguments:

 

-h, --help show this help message and exit

-u \[U\] Target URL. If not provided, target directory will be scanned

for har files. 

-d \[D\] Target directory \(default is

"hars"\). If URL is provided, 

directory will store har files. 

If URL is not provided, 

directory will be scanned. 

-s \[S\] Search term

-c \[C\] File containing JSON formatted

cookies to set in driver \(with

target URL only\)

-i \[I\] Count of pages to crawl \(with

target URL only\)

--p Flag, remove unnecessary parameters \(may dramatically

increase runtime\)

Вызовы API, сделанные на одной странице, отличаются

общим поисковым запросом. Например, на странице

**http://target.com** можно найти API-запрос, возвращающий

данные о продукте, которыми заполняется страница продукта: $ 

python 

consoleservice.py 

-u

https://www.target.com/p/

rogue-one-a-star-wars-\\story-blu-ray-dvd-

digital-3-disc/-/A-52030319 -s

"Rogue One: A Star Wars Story" 

Эта команда возвращает информацию, включая URL, для

API-запроса, который возвращает данные о продукте для этой

страницы:

URL:

https://redsky.target.com/v2/pdp/tcin/52030319

METHOD: GET

AVG RESPONSE SIZE: 34834

SEARCH 

TERM 

CONTEXT:

c":"786936852318","product\_description":

\{"title":

"Rogue One: A Star Wars Story \(Blu-ray \+ DVD \+

Digital\) 3 Disc", 

"long\_description":... 

С помощью флага -i можно собирать данные с нескольких

страниц \(по умолчанию всего одна страница\), начиная с

предоставленного URL. Это может быть полезно для поиска по

определенным ключевым словам во всем сетевом трафике или

же, если опустить флаг ключевого слова -s, — для сбора всего

трафика API, возникающего при загрузке каждой страницы. 

Все собранные данные хранятся в виде HAR-файла, который

по умолчанию размещается в каталоге **/har** в корне проекта, хотя этот каталог можно изменить с помощью флага -d. 

Если URL не указан, то можно также выполнить поиск и

анализ среди предварительно собранных HAR-файлов, размещенных в каталоге. 

У этого проекта есть множество других функций, включая

следующие:

• удаление ненужных параметров \(параметров GET или POST, которые не влия ют на возвращаемое значение вызова API\); 

• поддержку нескольких форматов вывода API \(командная

строка, HTML, JSON\); 

• возможность различать параметры пути, указывающие на

определенный маршрут API, и те, которые просто играют

роль параметров GET-запроса для одного и того же

маршрута API. 

Дальнейшее развитие проекта также планируется, поскольку я и мои коллеги продолжаем использовать его для

веб-скрапинга и сбора данных API. 

**Объединение API с другими источниками данных**

Смысл многих современных веб-приложений состоит в том, чтобы брать существующие данные и представлять их в более

привлекательном виде, однако я бы поспорила с этим, сказав, что в большинстве случаев это неинтересно. Если вы

используете API в качестве единственного источника данных, уж лучше просто скопировать чужую базу данных, которая уже

![Image 64](images/000065.png)

существует и, по сути, опубликована. Было бы гораздо

интереснее взять два или более источника данных и по-новому

объединить их или же использовать API в качестве

инструмента, позволяющего взглянуть на извлеченные данные

под другим углом. 

Рассмотрим всего один пример того, как данные, полученные с помощью API, в сочетании с веб-скрапингом

позволяют увидеть, какие части света вносят наибольший

вклад в «Википедию». 

Если вы потратили много времени, читая «Википедию», то

вам, скорее всего, встречались страницы хронологии правок

статей, на которых отображается список последних изменений. 

Если пользователь авторизовался в «Википедии» и внес

изменения, то отображается его имя. Если же не

авторизовался, то записывается его IP-адрес, как показано на

рис. 12.2. 

 

**Рис. 12.2. ** IP-адрес анонимного редактора на странице хронологии правок статьи о Python в «Википедии»

На странице хронологии правок указан IP-адрес

121.97.110.145. Используя API ip-api.com, мы узнали, что на

момент написания данной книги этот IP-адрес находился в

![Image 65](images/000016.png)

Кесон-Сити на Филиппинах \(иногда географическое положение

IP-адресов может изменяться\). 

Сама по себе эта информация не особенно интересна. А что, если собрать много таких точек с географическими

координатами правок «Википедии» и данными о том, где они

находятся? Несколько лет назад я так и сделала, использовав

библиотеку 

Google 

GeoChart

\(**https://developers.google.com/chart/interactive/docs/gallery/geoch** **art**\), 

и 

построила 

интересную 

диаграмму

\(**http://www.pythonscraping.com/pages/wikipedia.html**\), показывающую, из каких точек земного шара вносятся

изменения в англоязычную «Википедию», а также в

«Википедию» на других языках \(рис. 12.3\). 

 

**Рис. 12.3. ** Визуализация правок «Википедии», построенная с помощью библиотеки Google GeoChart

Создать простейший скрипт, который бы собирал данные из

«Википедии», находил страницы хронологии правок, а затем

извлекал оттуда IP-адреса, несложно. Представленный ниже

скрипт, использующий модифицированный код из главы 3, делает именно это:

from urllib.request import urlopen from bs4 import BeautifulSoup

import json

import datetime

import random

import re

 

random.seed\(datetime.datetime.now\(\)\)

def getLinks\(articleUrl\):

 

 

 

 

html 

=

urlopen\('http://en.wikipedia.org\{\}'.format\(arti

cleUrl\)\)

bs = BeautifulSoup\(html, 'html.parser'\)

 

 

 

 

return 

bs.find\('div', 

\{'id':'bodyContent'\}\).find\_all\('a', 

href=re.compile\('^\(/wiki/\)\(\(?\!:\).\)\*$'\)\)

 

def getHistoryIPs\(pageUrl\):

\# Страницы хронологии изменений имеют такой

формат:

\# http://en.wikipedia.org/w/index.php? 

title=Title\_in\_URL&action=history. 

pageUrl = pageUrl.replace\('/wiki/', ''\)

 

 

 

 

historyUrl 

=

'http://en.wikipedia.org/w/index.php?title=

\{\}&action=history' 

.format\(pageUrl\)

 

 

 

 

print\('history 

url 

is:

\{\}'.format\(historyUrl\)\)

html = urlopen\(historyUrl\)

bs = BeautifulSoup\(html, 'html.parser'\)

\# Находит только ссылки с классом "mw-anonuserlink", 

\# в которых указаны лишь IP-адреса, а не

имена пользователей. 

 

 

 

 

ipAddresses 

= 

bs.find\_all\('a', 

\{'class':'mw-anonuserlink'\}\)

addressList = set\(\)

for ipAddress in ipAddresses:

addressList.add\(ipAddress.get\_text\(\)\)

return addressList

 

links 

=

getLinks\('/wiki/Python\_\(programming\_language\)'\)

 

while\(len\(links\) > 0\):

for link in links:

print\('-'\*20\)

 

 

 

 

 

 

 

 

historyIPs 

=

getHistoryIPs\(link.attrs\['href'\]\)

for historyIP in historyIPs:

print\(historyIP\)

 

newLink = links\[random.randint\(0, 

len\(links\)-1\)\].attrs\['href'\]

links = getLinks\(newLink\)

В этой программе используются две основные функции: getLinks \(уже знакомая нам по главе 3\) и новая функция

getHistoryIPs, которая ищет контент всех ссылок с классом

mw-anonuserlink 

\(указывающим 

на 

анонимного

пользователя с IP-адресом, а не на пользователя с именем\) и

возвращает его в виде множества Python. 

В этом коде также используется несколько произвольная \(но

тем не менее эффективная для данного примера\) схема поиска

статей, для которых можно извлечь хронологию правок. 

Согласно данной схеме сначала извлекаются хронологии

правок всех статей «Википедии», на которые ссылается

начальная страница \(в нашем случае статья о языке

программирования Python\). Затем случайным образом

выбирается новая начальная страница и извлекаются все

страницы хронологии правок статей, на которые она

ссылается. Так продолжается до тех пор, пока мы не попадем

на страницу без ссылок. 

Теперь, когда у нас есть код, извлекающий IP-адреса в виде

строк, можно объединить его с функцией getCountry из

предыдущего раздела, чтобы преобразовать эти IP-адреса в

названия стран. Мы немного изменили код getCountry, чтобы

учесть недействительные или искаженные IP-адреса, которые

приведут к ошибке 404 \(страница не найдена\):

def getCountry\(ipAddress\):

try:

response = urlopen\(

 

 

 

 

 

 

 

 

 

 

 

 

'http://ip-

api.com/json/\{\}'.format\(ipAddress\)\).read\(\).deco

de\('utf-8'\)

except HTTPError:

return None

responseJson = json.loads\(response\)

return responseJson.get\('country\_code'\)

 

links 

=

getLinks\('/wiki/Python\_\(programming\_language\)'\)

 

while\(len\(links\) > 0\):

for link in links:

print\('-'\*20\)

 

 

 

 

 

 

 

 

historyIPs 

=

getHistoryIPs\(link.attrs\["href"\]\)

for historyIP in historyIPs:

country = getCountry\(historyIP\)

if country is not None:

print\('\{\} is from

\{\}'.format\(historyIP, country\)\)

 

newLink = links\[random.randint\(0, 

len\(links\)-1\)\].attrs\['href'\]

links = getLinks\(newLink\)

Ниже представлен пример выполнения этой программы:

-------------------

history 

url 

is:

http://en.wikipedia.org/w/index.php? 

title=Programming\_

paradigm&action=history

68.183.108.13 is from US

86.155.0.186 is from GB

188.55.200.254 is from SA

108.221.18.208 is from US

141.117.232.168 is from CA

76.105.209.39 is from US

182.184.123.106 is from PK

212.219.47.52 is from GB

72.27.184.57 is from JM

49.147.183.43 is from PH

209.197.41.132 is from US

174.66.150.151 is from US

**Дополнительные сведения об API**

В данной главе показано несколько способов использования

современных API для доступа к данным в Интернете для

разработки более быстрых и мощных веб-скраперов. Если вы

хотите не только задействовать, но и создавать API или же

больше узнать о теории их построения и синтаксиса, то я

рекомендую книгу *RESTful Web APIs* Леонарда Ричардсона

\(Leonard Richardson\), Майка Амундсена \(Mike Amundsen\) и

Сэма 

Руби 

\(Sam 

Ruby\) 

\(издательство 

O’Reilly\)

\(**http://bit.ly/RESTful-Web-APIs**\). 

Она 

представляет 

собой

подробный обзор теории и практики использования API в

Интернете. Кроме того, у Майка Амундсена есть увлекательная

серия видеороликов *Designing APIs for the Web* \(издательство

O’Reilly\) \(**http://oreil.ly/1GOXNhE**\), где рассказывается о том, как

разрабатывать собственные API. Это полезно уметь делать, если вы решите открыть доступ к собранным вами данным и

представить их в удобном формате. 

В то время как многие жалуются на вездесущий JavaScript и

динамические сайты, из-за которых традиционные методы

«захвата и анализа» HTML-страниц устаревают, я, со своей

стороны, приветствую наших новых повелителей роботов. 

Поскольку динамические сайты меньше полагаются на HTML-страницы, рассчитанные на просмотр человеком, и в большей

степени — на строго отформатированные файлы JSON, это

полезно всем, кто стремится получить чистые, хорошо

отформатированные данные. 

Интернет больше не является коллекцией HTML-страниц со

случайными украшениями в виде мультимедиа и CSS. Это

коллекция из файлов сотен типов и форматов, сотнями

передаваемых по сети с целью сформировать страницы, которые вы просматриваете через браузер. Настоящая

хитрость в том, чтобы чаще выглядывать за пределы страницы, которую вы видите, и получать данные непосредственно из ее

источника. 

21 См.: *Бэнкс А., Порселло Е. * GraphQL: язык запросов для современных веб-приложений. — СПб.: Питер, 2019. 

22 Этот API преобразует IP-адреса в географические объекты, которые мы еще

будем использовать в данной главе. 

23 В действительности во многих API вместо запросов PUT для обновления

информации используются запросы POST. Независимо от того, создается новый

объект или всего лишь изменяется старый, вопрос структуры API-запроса остается. 

Тем не менее все равно имеет смысл знать разницу между этими типами запросов, поскольку вы часто будете сталкиваться с запросами PUT в популярных API. 

**Глава 13. Обработка изображений и**

**распознавание текста**

Машинное зрение — обширнейшая область с далеко идущими

планами: от самодвижущихся автомобилей Google до торговых

автоматов, способных отличать фальшивые купюры от

настоящих. В этой главе основное внимание будет уделено

лишь одному небольшому аспекту данной области: распознаванию текста, а именно тому, как с помощью

различных библиотек Python распознать и использовать

текстовые изображения, найденные в Интернете24. 

Использование изображений вместо текста — обычная

методика, применяемая в тех случаях, когда вы не хотите, чтобы этот текст находили и читали боты. Она часто

встречается в контактных формах, когда адрес электронной

почты частично или полностью заменяется изображением. В

зависимости от того, насколько искусно это сделано, оно может

быть даже незаметно для людей, просматривающих страницу. 

Но ботам такие изображения читать трудно, и данного приема

бывает достаточно, чтобы большинство спамеров не смогли

получить ваш адрес электронной почты. 

В методиках капчи, конечно, применяется тот факт, что

пользователи могут прочитать «секретные» изображения, а

большинство ботов — нет. Одни изображения капчи сложнее, другие проще, и далее мы рассмотрим данную проблему. 

Однако капча не единственный элемент Интернета, где

необходимо помочь веб-скраперу преобразовать изображение

в текст. Даже в наше время многие документы сканируются из

печатных копий и в таком виде размещаются на сайтах, из-за

чего часто являются недоступными, «спрятанными на видном

месте». Если нет возможности преобразовать изображение в

текст, то остается единственный способ сделать эти документы

доступными — ввести их вручную, и, конечно же, на него

никогда не хватает времени. 

Преобразование изображений в текст называется

*оптическим распознаванием символов* \(optical character recognition, OCR\). Есть несколько крупных библио тек, выполняющих распознавание текста, и множество других

библиотек, которые их поддерживают или построены на их

основе. Временами система библиотек представляется

довольно сложной, поэтому советую вам прочитать следующий

раздел, прежде чем пытаться выполнить какое-либо из

упражнений данной главы. 

**Обзор библиотек**

Python — фантастически эффективный язык для обработки и

чтения изображений, машинного обучения на основе

изображений и даже создания изображений. Есть множество

библиотек Python для обработки изображений, но мы

сосредоточимся только на двух: Pillow и Tesseract. 

В области обработки изображений и OCR в Интернете эти

две библиотеки образуют мощный взаимодополняющий дуэт. 

*Pillow* выполняет первый проход, очищает и фильтрует

изображения, а *Tesseract* пытается сопоставить фигуры, найденные на этих изображениях, со своей библиотекой

известного текста. 

В данной главе описываются установка и основы

использования этих библио тек, а также приводится несколько

примеров их совместного применения. Я также расскажу об

углубленном обучении Tesseract, чтобы вы могли сами обучить

Tesseract дополнительным шрифтам и языкам для решения

задач OCR \(или даже распознавания капчи\), с которыми

можете столкнуться в Интернете. 

**Pillow**

Возможно, Pillow не самая полнофункциональная библиотека

для обработки изображений, но в ней есть все функции, которые вам, скорее всего, понадобятся, и даже больше — если

только вы не планируете переписать Photoshop на Python, в

таком случае вы читаете не ту книгу\! У Pillow также есть

преимущество: это одна из самых хорошо документированных

сторонних библиотек, и она чрезвычайно проста в

использовании без дополнительной настройки. 

В дополнение к Python Imaging Library \(PIL\) для Python 2.x, на которой основана Pillow, эта библиотека включает в себя

поддержку Python 3.x. Как и ее предшественница, Pillow позволяет легко импортировать изображения и обрабатывать

их с помощью различных фильтров, масок и даже пиксельных

преобразований:

from PIL import Image, ImageFilter

 

kitten = Image.open\('kitten.jpg'\)

blurryKitten 

=

kitten.filter\(ImageFilter.GaussianBlur\)

blurryKitten.save\('kitten\_blurred.jpg'\)

blurryKitten.show\(\)

В этом примере изображение kitten.jpg открывается в

используемой 

по 

умолчанию 

программе 

просмотра

изображений, где к нему добавляется размытие, после чего

размытое 

изображение 

сохраняется 

в 

файле

kitten\_blurred.jpg в том же каталоге. 

Мы будем использовать Pillow для предварительной

обработки изображений, чтобы упростить их машинное

распознавание, однако, как уже упоминалось, помимо простых

операций фильтрации, эта библиотека позволяет делать

многое другое. Дополнительную информацию о ней см. в

документации по Pillow \(**http://pillow.readthedocs.org/**\). 


