**Очистка задним числом**

Код делает только то, что мы можем \(или хотим\) сделать. 

Кроме того, может возникнуть необходимость обрабатывать

набор данных, созданный не нами, или даже набор данных, который вообще непонятно как очистить, сначала его не

увидев. 

В подобной ситуации многие программисты по привычке

рвутся сразу писать скрипт, который решил бы все проблемы. 

Однако есть инструменты сторонних разработчиков, такие как

OpenRefine, позволяющие не только легко и быстро очищать

данные, но и удобно просматривать и использовать эти данные

людям, далеким от программирования. 

**OpenRefine**

OpenRefine \(**http://openrefine.org/**\) — это проект с открытым

исходным кодом, основанный в 2009 году компанией Metaweb. 

В 2010 году Google приобрела Metaweb, сменив название

проекта с Freebase Gridworks на Google Refine. В 2012 году

Google прекратила поддержку Refine и снова изменила ее

название на OpenRefine, и теперь любой желающий может

внести свой вклад в развитие проекта. 

**Установка**

Продукт OpenRefine необычен тем, что, хотя его интерфейс

запускается из браузера, технически это самостоятельное

приложение, которое необходимо скачать и установить. На

веб-странице 

загрузки 

OpenRefine

\(**http://openrefine.org/download.html**\) есть версии приложения

для Linux, Windows и macOS. 

![Image 52](images/000017.png)

Если вы пользователь Mac и не смогли открыть файл, то

выберите команду System Preferences–>Security & Privacy–

>General 

\(Установки 

систе мы–>Защищенность 

& 

Конфиденциальность–>Общие\). В разделе Allow apps downloaded from \(Разрешить скачивание приложений\) выберите вариант Anywhere \(Везде\). К сожалению, после

перехода от Google к открытому исходному коду проект

OpenRefine, похоже, утратил легитимность в глазах Apple. 

Чтобы использовать OpenRefine, нужно сохранить данные в

виде CSV-файла \(при необходимости освежить знания о том, как это делается, см. раздел «Хранение данных в формате CSV»

на с. 111\). Кроме того, если данные хранятся в базе, то их

можно экспортировать в CSV-файл. 

**Использование OpenRefine**

В следующих примерах мы будем использовать данные, взятые

из таблицы сравнения текстовых редакторов в «Википедии»

\(**https://en.wikipedia.org/wiki/Comparison\_of\_text\_editors**\) \(рис. 

8.1\). Несмотря на то что данная таблица относительно хорошо

отформатирована, в нее в течение длительного времени

вносили правки разные люди, поэтому ее форматирование

слегка отличается. Кроме того, поскольку данные таблицы

предназначены для чтения людьми, а не машинами, некоторые

варианты форматирования \(например, **Free** вместо **$0.00**\) неприемлемы для использования в качестве входных данных

программы. 

![Image 53](images/000045.png)

![Image 54](images/000075.png)

 

**Рис. 8.1**. Данные из таблицы сравнения текстовых редакторов в «Википедии», открытые в

главном окне OpenRefine

Первое, на что следует обратить внимание в OpenRefine, —

каждый заголовок столбца снабжен стрелкой. Она открывает

меню инструментов, с помощью которых можно фильтровать, сортировать, преобразовывать или удалять данные этого

столбца. 

**Фильтрациюданных** можно выполнить двумя способами: с помощью фильтров или фасетов. Фильтры хороши при

фильтрации данных путем регулярных выражений: например, 

«Показать только те строки, у которых в столбце “Язык

программирования” присутствует три и более названия языков

программирования, разделенных запятыми» \(рис. 8.2\). 

 

**Рис. 8.2. ** Регулярное выражение .\+,.\+,.\+ выбирает значения, которые содержат как минимум

три элемента, разделенных запятыми

![Image 55](images/000048.png)

Фильтры легко комбинировать, редактировать и добавлять, манипулируя блоками, предлагаемыми в правом столбце, а

также можно комбинировать с фасетами. 

Фасеты — отличное средство для включения данных в

выбранное множество или исключения из него на основании

контента всего столбца. \(Например, «Показать все продукты с

лицензией GPL или MIT, которые появились после 2005 года»

\(рис. 8.3\).\) Фасеты располагают собственными инструментами

фильтрации. Так, для фильтрации по числовому значению есть

ползунки, позволяющие выбирать диапазон значений, который мы хотим включить в выборку. 

 

**Рис. 8.3. ** При таком варианте выбора отображаются все текстовые редакторы с лицензиями

GPL или MIT, впервые появившиеся после 2005 года

Как бы вы ни фильтровали данные, их всегда можно

экспортировать в один из форматов, поддерживаемый

OpenRefine. В число этих форматов входят CSV, HTML \(HTML-таблица\), Excel и некоторые другие. 

**Очистка. ** Фильтрация данных может быть успешной только

при условии, что данные изначально сравнительно чисты. 

Например, в рассмотренном в предыдущем разделе примере

фасета текстовый редактор с датой выпуска 01-01-2006 не был

бы выбран, так как фасет «Первый официальный выпуск» искал

значение 2006 и игнорировал все значения, которые от него

отличались. 

Преобразование данных в OpenRefine выполняется с

помощью языка выражений OpenRefine, называемого GREL

\(Google Refine Expression Language, в аббревиатуре сохранилось

старое название OpenRefine — Google Refine\). Этот язык

используется для создания коротких лямбда-функций, которые

трансформируют значения ячеек на основе простых правил. 

Например:

if\(value.length\(\) \!= 4, "invalid", value\) Будучи примененной к столбцу **First stable release** \(Первая

стабильная версия\), эта функция сохраняет все ячейки с датой, значения которых имеют формат YYYY, и помечает все

остальные как invalid \(рис. 8.4\). 

![Image 56](images/000060.png)

 

**Рис. 8.4. ** Проект после вставки оператора GREL \(предварительные результаты размещены

под оператором\)

Чтобы применить оператор GREL, следует нажать

указывающую вниз стрелку, расположенную рядом с

заголовком какого-либо столбца, и выбрать команду **Edit cells**–

> **Transform** \(Редактирование ячеек–>Преобразование\). 

Но если вы думаете, что маркировка всех неидеальных

значений как недействительных облегчит их обнаружение, то

сильно заблуждаетесь. Лучше попытаться по максимуму спасти

информацию из плохо отформатированных значений. Это

можно сделать с помощью функции GREL match:

value.match\(".\*\(\[0-9\]\{4\}\).\*"\).get\(0\)

Эта функция пытается сопоставить строковое значение с

заданным регулярным выражением. Если данное выражение

соответствует строке, то функция возвращает массив. Его

значениями являются все подстроки, которые соответствуют

«группе захвата» в регулярном выражении \(обозначены в нем

круглыми скобками, в данном примере это \[0-9\]\{4\}\). 

Данный код, в сущности, находит все значения с четырьмя

цифрами, идущими подряд, и возвращает первый такой

фрагмент. Обычно этого достаточно, чтобы извлечь годы из

текста или плохо отформатированной даты. Еще одним

преимуществом описанного способа является то, что в случае

несуществующей даты он возвращает null. \(При выполнении

операций с переменной null GREL не выдает исключение

нулевого указателя.\)

Редактируя ячейки и GREL, можно выполнять и многие

другие преобразования данных. Полное руководство по этому

языку доступно на странице OpenRefine на GitHub \(**https://github.com/OpenRefine/OpenRefine/wiki/Documentation-For-Users**\). 

**Глава 9. Чтение и запись текстов на**

**естественных языках**

До сих пор данные, с которыми мы работали, в основном

представляли собой числа или перечислимые значения. В

большинстве случаев мы просто сохраняли данные, никак их

после этого не анализируя. В данной главе мы затронем

сложную проблему английского языка12. 

Каким образом Google узнает, что вам нужно, когда вы

вводите в строку поиска изображений слова cutekitten \(симпатичный котенок\)? По тексту, который окружает

изображения этих самых котят. Каким образом YouTube узнает, что вам нужно показать определенный скетч «Монти Пайтона», если вы ввели в строку поиска слова deadparrot \(мертвый

попугай\)? По заголовку и описанию, которые сопровождают

каждое загруженное видео. 

На самом деле, даже если ввести что-то вроде

deceasedbirdmontypython \(умершая птица монти пайтон\), система все равно сразу же выдаст вам тот же скетч «Мертвый

попугай», хотя на самой странице скетча нет слов deceased \(умерший\) или bird \(птица\). Google знает, что hot dog — это

еда, а boiling puppy — вовсе нет. Каким образом? Все дело в

статистике\! 

Вам может показаться, что анализ текста не имеет никакого

отношения к вашему проекту, однако понимание концепций, лежащих в его основе, может быть чрезвычайно полезным для

всех видов машинного обучения и вообще для возможности

моделирования в виде алгоритмических и вероятностных

задач. 

Например, музыкальный сервис Shazam определяет, содержит ли данный аудиофрагмент конкретную песню, даже

если в нем присутствуют фоновые помехи или искажения. 

Google 

разрабатывает 

автоматические 

подписи 

к

изображениям, основываясь только на самом изображении13. 

Сравнивая известные изображения, скажем, хот-догов с

другими их изображениями, поисковая система постепенно

учится распознавать, как выглядит эта еда, и находить похожие

детали на других изображениях, которые ей показывают. 

**Обобщение данных**

В главе 8 было показано, как разделить текстовый контент на

*n*-граммы — наборы фраз длиной *n* слов. В простейшем случае

*n*-граммы можно использовать для определения того, какие

наборы слов или фразы чаще всего встречаются в данном

текстовом фрагменте. Вдобавок метод подходит для создания

естественных на вид обобщений этих данных — если вернуться

к исходному тексту и извлечь из него предложения, заключающие в себе некоторые из наиболее распространенных

фраз. 

Одним из примеров текста, который мы будем использовать

для этого, является инаугурационная речь девятого президента

Соединенных Штатов Америки Уильяма Генри Харрисона

\(William Henry Harrison\). Он установил два рекорда в истории

института президентов США: самая длинная инаугурационная

речь и самое короткое время пребывания в должности — 32

дня. 

Мы 

возьмем 

полный 

текст 

этой 

речи

\(**http://pythonscraping.com/files/inaugurationSpeech.txt**\) в качестве

источника данных для нескольких примеров в этой главе. 

Слегка изменив код для поиска *n*-грамм, описанный в главе

8, можно создать код, который находил бы множества 2-грамм

и возвращал объект Counter, содержащий все 2-граммы: from urllib.request import urlopen

from bs4 import BeautifulSoup

import re

import string

from collections import Counter

 

def cleanSentence\(sentence\):

sentence = sentence.split\(' '\)

 

 

 

 

sentence 

=

\[word.strip\(string.punctuation\+string.whitespac

e\)

for word in sentence\]

sentence = \[word for word in sentence if

len\(word\) > 1

or \(word.lower\(\) == 'a' or word.lower\(\)

== 'i'\)\]

return sentence

 

def cleanInput\(content\):

content = content.upper\(\)

content = re.sub\('\\n', ' ', content\)

content = bytes\(content, "UTF-8"\)

content = content.decode\("ascii", "ignore"\) sentences = content.split\('. '\)

return \[cleanSentence\(sentence\) for

sentence in sentences\]

 

def getNgramsFromSentence\(content, n\):

output = \[\]

for i in range\(len\(content\)-n\+1\):

output.append\(content\[i:i\+n\]\)

return output

 

def getNgrams\(content, n\):

content = cleanInput\(content\)

ngrams = Counter\(\)

ngrams\_list = \[\]

for sentence in content:

newNgrams = \[' '.join\(ngram\) for ngram

in

getNgramsFromSentence\(sentence, 2\)\]

ngrams\_list.extend\(newNgrams\)

ngrams.update\(newNgrams\)

return\(ngrams\)

 

content = str\(

urlopen\('http://pythonscraping.com/files/

inaugurationSpeech.txt'\)

.read\(\), 'utf-8'\)

ngrams = getNgrams\(content, 2\)

print\(ngrams\)

На выходе получим, в частности, следующее:

Counter\(\{'OF THE': 213, 'IN THE': 65, 'TO THE':

61, 'BY THE': 41, 

'THE CONSTITUTION': 34, 'OF OUR': 29, 'TO BE':

26, 'THE PEOPLE': 24, 

'FROM THE': 24, 'THAT THE': 23,... 

Из этих 2-грамм *the constitution* представляется довольно

частой темой речи, но 2-граммы *of the*, *in the* и *to the* едва ли

заслуживают внимания. Можно ли каким-либо достаточно

точным 

автоматическим 

способом 

избавиться 

от

нежелательных слов? 

К счастью, есть люди, которые тщательно изучают разницу

между «интересными» и «неинтересными» словами, и их

работа поможет нам решить эту задачу. Марк Дэвис \(Mark Davies\), профессор лингвистики в Университете Бригама Янга, собирает «Корпус современного американского английского

языка» \(**http://corpus.byu.edu/coca/**\) — коллекцию из более 450

миллионов слов из популярных публикаций, выпущенных в

США за последние десять лет. 

Список из 5000 наиболее часто встречающихся слов

доступен бесплатно. К счастью, этого более чем достаточно, чтобы их можно было использовать в качестве базового

фильтра, позволяющего отсеять наиболее распространенные 2-граммы. Благодаря добавлению функции isCommon уже

первые 100 слов значительно улучшили наш результат: def isCommon\(ngram\):

commonWords = \['THE', 'BE', 'AND', 'OF', 

'A', 'IN', 'TO', 'HAVE', 'IT', 'I', 

'THAT', 'FOR', 'YOU', 'HE', 'WITH', 

'ON', 'DO', 'SAY', 'THIS', 'THEY', 

'IS', 'AN', 'AT', 'BUT', 'WE', 'HIS', 

'FROM', 'THAT', 'NOT', 'BY', 

'SHE', 'OR', 'AS', 'WHAT', 'GO', 

'THEIR', 'CAN', 'WHO', 'GET', 'IF', 

'WOULD', 'HER', 'ALL', 'MY', 'MAKE', 

'ABOUT', 'KNOW', 'WILL', 'AS', 

'UP', 'ONE', 'TIME', 'HAS', 'BEEN', 

'THERE', 'YEAR', 'SO', 'THINK', 

'WHEN', 'WHICH', 'THEM', 'SOME', 'ME', 

'PEOPLE', 'TAKE', 'OUT', 'INTO', 

'JUST', 'SEE', 'HIM', 'YOUR', 'COME', 

'COULD', 'NOW', 'THAN', 'LIKE', 

'OTHER', 'HOW', 'THEN', 'ITS', 'OUR', 

'TWO', 'MORE', 'THESE', 'WANT', 

'WAY', 'LOOK', 'FIRST', 'ALSO', 'NEW', 

'BECAUSE', 'DAY', 'MORE', 'USE', 

'NO', 'MAN', 'FIND', 'HERE', 'THING', 

'GIVE', 'MANY', 'WELL'\]

for word in ngram:

if word in commonWords:

return True

return False

В итоге мы получили следующие 2-граммы, которые

встречаются в тексте более двух раз:

Counter\(\{'UNITED 

STATES': 

10, 

'EXECUTIVE

DEPARTMENT': 4, 'GENERAL GOVERNMENT': 4, 

'CALLED UPON': 3, 'CHIEF MAGISTRATE': 3, 

'LEGISLATIVE BODY': 3, 'SAME CAUSES': 3, 

'GOVERNMENT SHOULD': 3, 'WHOLE COUNTRY': 3,... 

Неудивительно, что первыми в списке идут *United States* \(Соединенные Штаты\) и *executive department* \(исполнительная

власть\), — этого следовало ожидать от инаугурационной речи

президента. 

Важно подчеркнуть: мы используем для фильтрации

результатов 

сравнительно 

свежий 

список 

слов, 

распространенных в наше время. Это может оказаться

неуместным, если учесть, что текст был написан в 1841 году. 

Однако, поскольку мы использовали примерно первые 100 слов

данного 

списка 

\(которые 

можно 

считать 

менее

подверженными временным изменениям, чем, скажем, последние 100 слов\) и, похоже, получили удовлетворительные

результаты, то, вероятно, можем сэкономить усилия и не

создавать список слов, наиболее распространенных в 1841 году

\(хотя это могло бы быть интересным\). 

Теперь, когда мы извлекли из текста некоторые ключевые

темы, как это поможет написать его резюме? Один из способов

— найти первое предложение, содержащее каждую из

«популярных» *n*-грамм. Теоретически, выбрав для каждой из

них 

первый 

такой 

экземпляр, 

мы 

получим

удовлетворительный обзор всего текста. Первые пять самых

популярных 2-грамм дают следующие пункты. 

• *The Constitution of the United States is the instrument containing* *this grant of power to the several departments composing the* *Government*. 

*• Such a one was afforded by the executive department constituted by* *the Constitution. The General Government has seized upon none of* *the reserved rights of the States. *

*• The General Government has seized upon none of the reserved rights* *of the States*. 

*• Called from a retirement which I had supposed was to continue for* *the residue of my life to fill the chief executive office of this great and* *free nation, I appear before you, fellow-citizens, to take the oaths* *which the constitution prescribes as a necessary qualification for the* *performance of its duties; and in obedience to a custom coeval with* *our government and what I believe to be your expectations I proceed* *to present to you a summary of the principles which will govern me* *in the discharge of the duties which I shall be called upon to* *perform*. 

*• The presses in the necessary employment of the Government should* *never be used to “clear the guilty or to varnish crime” *. 

Конечно, такое резюме вряд ли в обозримом будущем

напечатают в CliffsNotes, но, учитывая, что размер исходного

документа составляет 217 предложений, а четвертое *\(Called* *from a retirement...\)* довольно хорошо отражает основную тему, это не так уж плохо для первой попытки. 

Для получения «самых важных» предложений из более

длинного или разно образного текста, возможно, стоит

рассмотреть 3- или даже 4-граммы. В данном случае только

одна 3-грамма используется несколько раз, и это *exclusive* *metallic currency* — едва ли характерная фраза для

инаугурационной речи президента. Для более длинных

оборотов имеет смысл использовать 3-граммы. 

Другой подход заключается в поиске предложений, содержащих самые популярные *n*-граммы. Конечно, это будут

более длинные предложения и в случае возникновения

проблем можно поискать предложения с наибольшим

процентом слов, которые являются популярными *n*-граммами, или создать собственную метрику оценки, комбинируя

несколько приемов. 

**Модели Маркова**

Возможно, вам приходилось слышать о текстовых генераторах

Маркова. Они стали популярными в развлекательных

приложениях, таких как сайт That can be my next tweet\! \(«Это

мог 

бы 

быть 

мой 

следующий 

твит\!»\)

\(**http://yes.thatcan.be/my/next/tweet/**\), а также применяются для

создания спам-писем, которые выглядят как настоящие и

призваны обмануть системы обнаружения спама. 

Все эти текстовые генераторы основаны на модели Маркова, которая часто используется для анализа больших множеств

случайных событий, где каждое дискретное событие может

сопровождаться другим дискретным событием с определенной

вероятностью. 

Например, можно построить модель Маркова для системы

прогноза погоды, как показано на рис. 9.1. 

В этой модели вероятность того, что следующий день после

солнечного тоже будет солнечным, составляет 70 %, пасмурным

— 20 %, а дождливым — 10 %. Если день дождливый, то

вероятность того, что на следующий день будет дождь, составляет 50 %, вероятность ясной погоды — 25 %, пасмурной

— также 25 %. 

Вы могли заметить, что модель Маркова имеет следующие

свойства. 

• Все вероятности, исходящие из одного узла, в сумме всегда

составляют ровно 100 %. Независимо от того, насколько

сложна система, всегда должна быть 100-процентная

вероятность того, что на следующем этапе она может

привести еще куда-либо. 

• Хотя в данной модели в любой момент существует лишь три

возможных варианта погоды, ее можно расширить, создав

бесконечный список состоя ний погоды. 

![Image 57](images/000010.png)

 

**Рис. 9.1. ** Модель Маркова, описывающая теоретическую систему прогноза погоды

• Узел, в который вы перейдете, определяется только

состоянием текущего узла. Если вы находитесь в узле

«Ясно», то не имеет значения, были предыдущие 100 дней

ясными или дождливыми, — вероятность ясной погоды на

следующий день все равно составляет 70 %. 

• Может оказаться так, что достичь одних узлов будет труднее, чем других. Стоящая за этим математика достаточно

сложна, однако нетрудно заметить: в данной системе

«Дождь» в любой момент времени является гораздо менее

вероятным состоянием, чем «Ясно» или «Пасмурно»

\(поскольку сумма стрелок, указывающих на «Дождь», меньше 100 %\). 

Очевидно, что это простая система и модели Маркова могут

быть сколь угодно большими. Алгоритм ранжирования страниц

Google частично основан на данной модели: сайты

представлены в виде узлов, а входящие и исходящие ссылки —

в виде связей между ними. «Вероятность» попасть в

определенный узел соответствует относительной популярности

сайта. Другими словами, если бы наша метеорологическая

система представляла собой чрезвычайно маленький

Интернет, то «дождливый» сайт имел бы низкий рейтинг

страниц, а «пасмурный» — высокий. 

Учитывая это, вернемся к более конкретному примеру: анализу и написанию текста. 

Снова использовав инаугурационную речь Уильяма Генри

Харрисона, проанализированную в предыдущем примере, мы

можем написать следующий код, который будет генерировать

на основе структуры этого текста цепи Маркова произвольной

длины \(сейчас длина цепи равна 100\):

from urllib.request import urlopen

from random import randint

 

def wordListSum\(wordList\):

sum = 0

for word, value in wordList.items\(\):

sum \+= value

return sum

 

def retrieveRandomWord\(wordList\):

 

 

 

 

randIndex 

= 

randint\(1, 

wordListSum\(wordList\)\)

for word, value in wordList.items\(\):

randIndex -= value

if randIndex <= 0:

return word

 

def buildWordDict\(text\):

\# удаляем разрывы строк и кавычки

text = text.replace\('\\n', ' '\); 

text = text.replace\('"', ''\); 

 

\# Убедимся, что знаки препинания

рассматриваются как отдельные "слова", 

\# чтобы они тоже включались в цепь Маркова. 

punctuation = \[',','.',';',':'\]

for symbol in punctuation:

text = text.replace\(symbol, ' \{\}

'.format\(symbol\)\); 

 

words = text.split\(' '\)

\# убираем пустышки

words = \[word for word in words if word \!=

''\]

 

wordDict = \{\}

for i in range\(1, len\(words\)\):

if words\[i-1\] not in wordDict:

\# Создаем новый словарь для этого

слова. 

wordDict\[words\[i-1\]\] = \{\}

if words\[i\] not in wordDict\[words\[i-

1\]\]:

wordDict\[words\[i-1\]\]\[words\[i\]\] = 0

wordDict\[words\[i-1\]\]\[words\[i\]\] \+= 1

return wordDict

 

text 

=

str\(urlopen\('http://pythonscraping.com/files/in

augurationSpeech.txt'\)

.read\(\), 'utf-8'\)

wordDict = buildWordDict\(text\)

 

\# Генерируем цепь Маркова длиной 100. 

length = 100

chain = \['I'\]

for i in range\(0, length\):

 

 

 

 

newWord 

=

retrieveRandomWord\(wordDict\[chain\[-1\]\]\)

chain.append\(newWord\)

 

print\(' '.join\(chain\)\)

Результат выполнения этого кода меняется при каждом

запуске программы. Вот пример совершенно бессмысленного

текста, который он сгенерировал:

I sincerely believe in Chief Magistrate to make

all necessary sacrifices and oppression of the remedies which we may have occurred to me in the arrangement and disbursement of the

democratic claims them , consolatory to have been best political power in fervently

commending every other addition of legislation

, by the interests which violate that the Government 

would 

compare 

our 

aboriginal

neighbors the people to its accomplishment . 

The latter also susceptible of the Constitution

not much mischief , disputes have left to betray . The maxim which may sometimes be an impartial and to prevent the adoption or

Что же делает этот код? 

Функция buildWordDict принимает строку текста, полученную из Интернета. Выполняет некую очистку и

форматирование: удаляет кавычки и ставит пробелы вокруг

остальных знаков препинания, чтобы они фактически

рассматривались как отдельные слова. Затем создает

двумерный словарь \(словарь словарей\) такого вида:

\{word\_a : \{word\_b : 2, word\_c : 1, word\_d : 1\}, 

word\_e : \{word\_b : 5, word\_d : 2\},...\}

В этом примере словаря в тексте было обнаружено четыре

вхождения слова word\_a, после двух из которых следовало

слово word\_b, после одного — word\_c и еще после одного —

word\_d. Слово word\_e встретилось семь раз: пять раз после

него стояло word\_b и дважды — word\_d. 

Построив узловую модель этого результата, мы увидим, что

из узла word\_a выходит стрелка с пометкой «50 %», указывающая на word\_b \(которое в тексте встречается после

word\_a два раза из четырех\), стрелка с пометкой «25 %», указывающая на word\_c, и еще одна стрелка с пометкой «25

%», указывающая на word\_d. 

Создав такой словарь, его затем можно использовать в

качестве справочной таблицы, чтобы увидеть, куда переходить

дальше, независимо от того, на каком слове в тексте мы

остановились14. В данном примере словаря словарей, если мы

сейчас находимся на слове word\_e, можем передать словарь

\{word\_b:5,word\_d:2\} в функцию retrieveRandomWord. Эта

функция, в свою очередь, извлечет из словаря случайное слово, взвешенное по числу его вхождений в текст. 

Начав со случайного исходного слова \(в данном случае

вездесущего I — «Я»\), мы можем легко перемещаться по цепи

Маркова, генерируя столько слов, сколько захотим. 

Вдобавок цепи Маркова имеют тенденцию быть тем

«реалистичнее», чем больше текста собирается, особенно из

источников с похожим авторским стилем. В этом примере для

создания цепи использовались 2-граммы \(в которых

следующее слово определяется по предыдущему\), однако

можно задействовать 3-граммы или *n*-граммы высшего

порядка, где каждое следующее слово определяется двумя или

более словами. 

Несмотря на интересные результаты и большие выгоды, которые можно получить, накопив много мегабайтов текста в

процессе веб-скрапинга, из-за подобного применения бывает

трудно заметить практическую пользу цепей Маркова. Ранее в

этом разделе отмечалось, что цепи Маркова моделируют то, как сайты переходят с одной страницы на другую. Большие

коллекции подобных ссылок, представленных в виде

указателей, позволяют строить веб-графы, полезные для

хранения, отслеживания и анализа. То есть цепочки Маркова

помогают сформировать основу для того, чтобы вы могли

представить процесс веб-краулинга, а также для описания

логики действия самих веб-краулеров. 

**Шесть шагов по «Википедии»: заключение. ** В главе 3 мы

создали веб-скрапер, который собирает ссылки одной статьи

«Википедии» на следующую, начиная со статьи о Кевине

Бейконе, а в главе 6 добавили сохранение ссылок в базе

данных. Почему я снова об этом вспоминаю? Потому что, оказывается, задача выбора пути по ссылкам от начальной до

заданной конечной страницы \(то есть поиск цепочки страниц

от **https://en.wikipedia.org/wiki/Kevin\_Bacon** до **https://en.wikipedia.org/wiki/Eric\_Idle**\) — то же самое, что и построение

цепочки Маркова, в которой заданы первое и последнее слово. 

Задачи такого рода относятся к задачам *направленных*

*графов*, где наличие связи A –> Б еще не означает наличие

связи Б –> A. После слова «футбол» часто стоит слово «игрок», однако вы обнаружите, что после слова «игрок» слово «футбол»

встречается гораздо реже. Хоть в статье о Кевине Бейконе в

«Википедии» есть ссылка на статью о его родном городе

Филадельфии, статья о Филадельфии не отвечает взаимностью

— в ней нет ссылки на страницу актера. 

И наоборот, исходная игра «Шесть шагов до Кевина

Бейкона» представляет собой задачу *ненаправленного графа*. 

Если известно, что Кевин Бейкон снимался в фильме

«Коматозники» с Джулией Робертс \(Julia Roberts\), то Джулия

Робертс тоже снималась в «Коматозниках» с Кевином

Бейконом, поэтому здесь отношения являются двусторонними

\(у них нет «направления»\). Задачи ненаправленных графов, как

правило, встречаются в информатике реже, чем задачи

направленных графов, но и те и другие достаточно сложны в

вычислительном отношении. 

Несмотря на большую проделанную работу по этим видам

задач и множеству их разновидностей, одним из лучших и

самых распространенных способов найти кратчайшие пути в

направленном графе — и, таким образом, найти пути между

статьей «Википедии» о Кевине Бейконе и любой другой статьей

«Вики педии» — остается поиск в ширину. 

При *поиске в ширину* сначала выполняется поиск всех

ссылок, ведущих непосредственно на начальную страницу. 

Если среди них нет конечной страницы \(той, которую мы

ищем\), то выполняется поиск среди ссылок второго уровня —

тех страниц, на которые ссылается страница, связанная

ссылкой с начальной. Этот процесс продолжается до тех пор, пока не будет достигнута предельная глубина \(в данном случае

6\) или найдена конечная страница. 

Полная реализация поиска в ширину с помощью таблицы

ссылок, описанной в главе 6, выглядит так:

import pymysql

 

conn 

= 

pymysql.connect\(host='127.0.0.1', 

unix\_socket='/tmp/mysql.sock', 

 

 

 

 

user='', 

passwd='', 

db='mysql', 

charset='utf8'\)

cur = conn.cursor\(\)

cur.execute\('USE wikipedia'\)

 

def getUrl\(pageId\):

cur.execute\('SELECT url FROM pages WHERE id

= %s', \(int\(pageId\)\)\)

return cur.fetchone\(\)\[0\]

 

def getLinks\(fromPageId\):

cur.execute\('SELECT toPageId FROM links

WHERE fromPageId = %s', 

\(int\(fromPageId\)\)\)

if cur.rowcount == 0:

return \[\]

return \[x\[0\] for x in cur.fetchall\(\)\]

 

def searchBreadth\(targetPageId, paths=\[\[1\]\]\):

newPaths = \[\]

for path in paths:

links = getLinks\(path\[-1\]\)

for link in links:

if link == targetPageId:

return path \+ \[link\]

else:

newPaths.append\(path\+\[link\]\)

return searchBreadth\(targetPageId, 

newPaths\)

 

nodes = getLinks\(1\)

targetPageId = 28624

pageIds = searchBreadth\(targetPageId\)

for pageId in pageIds:

print\(getUrl\(pageId\)\)

Здесь getUrl — вспомогательная функция, извлекающая

URL из базы данных по идентификатору страницы. Функция

getLinks работает аналогично, принимая fromPageId —

целочисленный идентификатор текущей страницы — и

извлекая список целочисленных ID всех страниц, на которые

ссылается текущая страница. 

Основная функция, searchBreadth, рекурсивно строит

список всех возможных путей от страницы поиска и

останавливается, когда находит путь, ведущий к конечной

странице. 

• Функция начинает работу с одного пути — \[1\]. Перейдя по

нему, пользователь остается на конечной странице с

идентификатором 1 \(Кевин Бейкон\) и не переходит по

ссылкам. 

• Для каждого пути в списке \(при первом проходе есть только

один путь, поэтому данный шаг короткий\) функция находит

все ссылки, которые ведут со страницы, представляющей

последнюю страницу пути. 

• Для каждой из этих исходящих ссылок алгоритм проверяет, соответствуют ли они targetPageId. Если такое

совпадение найдено, возвращается данный путь. 

• При отсутствии совпадения новый путь добавляется в новый

список \(теперь более длинных\) путей, состоящий из старого

пути и новой исходящей ссылки с текущей страницы. 

• Если targetPageId на этом уровне вообще не найден, то

выполняется рекурсия и searchBreadth вызывается с тем

же targetPageId и новым, более длинным списком путей. 

После того как будет найден список идентификаторов

страниц, содержащий путь между двумя страницами, все его ID

преобразуются в соответствующие URL и выводятся на экран. 

Ниже показан результат поиска ссылки между страницей

Кевина Бейкона \(которая в этой базе данных имеет

идентификатор 

1\) 

и 

страницей 

Эрика 

Айдла 

\(с

идентификатором страницы 28 624\):

/wiki/Kevin\_Bacon

/wiki/Primetime\_Emmy\_Award\_for\_Outstanding\_Lead

\_Actor\_in\_a\_

Miniseries\_or\_a\_Movie

/wiki/Gary\_Gilmore

/wiki/Eric\_Idle

Если преобразовать это в отношения ссылок, то получим

следующее: Кевин Бейкон–>Прайм-таймовая премия «Эмми»–

>Гэри Гилмор–>Эрик Айдл. 

Помимо решения задач о шести шагах и построения

моделей того, какие слова обычно следуют за другими в

предложениях, направленные и ненаправленные графы могут

применяться для моделирования различных ситуаций, возника ющих при веб-скрапинге. Какие сайты ссылаются на

другие характерные сайты? Какие исследовательские работы

связаны ссылками? Какие продукты обычно предлагают с

другими продуктами в интернет-магазинах? В чем сила той

или иной ссылки? Взаимна ли эта связь? 

Распознавание этих фундаментальных типов отношений

может быть чрезвычайно полезным для построения моделей, визуализаций и прогнозов на основе данных, собранных в

процессе веб-скрапинга. 

**Natural Language Toolkit**

До сих пор в этой главе основное внимание уделялось

статистическому анализу слов в тексте. Какие слова наиболее

популярны, а какие не свойственны данному тексту? Какие

слова обычно идут после тех или иных слов? Какие группы они

образуют? Чего нам еще не хватает, так это возможности

понять, что именно представляют собой данные слова. 

*Natural Language Toolkit \(NLTK\)* — набор библиотек Python, предназначенных для идентификации и маркировки частей

речи, встречающихся в тексте, написанном на естественном

английском языке. NLTK начали создавать в 2000 году, и за

последние 15 лет десятки разработчиков из разных стран

внесли свой вклад в этот проект. У NLTK огромный функционал

\(чему посвящены целые книги\), однако в текущем разделе мы

рассмотрим лишь несколько способов применения данной

библиотеки15. 

![Image 58](images/000040.png)

**Установка и настройка**

Модуль nltk можно установить так же, как и другие модули

Python: либо скачать пакет напрямую через сайт NLTK, либо

воспользоваться одним из многочисленных сторонних

инсталляторов с ключевым словом **nltk**. Подробные инструкции

по 

установке 

модуля 

см. 

на 

сайте 

NLTK

\(**http://www.nltk.org/install.html**\). 

После установки NLTK рекомендуется скачать его

предустановленные текстовые репозитории, чтобы было

удобнее пользоваться данными функциями. Для этого нужно

ввести в командной строке Python следующее:

>>> import nltk

>>> nltk.download\(\)

В результате запустится NLTK Downloader \(рис. 9.2\). 

 

**Рис. 9.2. ** NLTK Downloader позволяет просматривать и загружать дополнительные пакеты и

текстовые библиотеки, связанные с модулем nltk

При первом знакомстве с корпусом NLTK я рекомендую

установить все доступные пакеты. Вы сможете легко удалить

лишние пакеты в любой момент. 

**Статистический анализ с помощью NLTK**

NLTK отлично подходит для получения статистической

информации о количестве слов, частоте и разнообразии слов в

разделах текста. Если вам нужны лишь относительно простые

вычисления \(такие как количество уникальных слов, используемых в разделе текста\), то импорт nltk может

оказаться излишним — все же это большой модуль. Но если вам

нужно провести сравнительно обширный анализ текста, то у

вас под рукой будут функции, способные вычислить

практически любую необходимую метрику. 

Анализ с применением NLTK всегда начинается с объекта

Text. Объекты Text создаются с помощью всего пары простых

строк на Python:

from nltk import word\_tokenize

from nltk import Text

 

tokens = word\_tokenize\('Here is some not very interesting text'\)

text = Text\(tokens\)

На вход функции word\_tokenize подается любая текстовая

строка Python. На случай, если у вас нет подходящих длинных

строк, но вы все равно хотели бы поэкспериментировать с

этими функциями, в библиотеку NLTK встроено несколько

книг, к которым можно получить доступ с помощью функции

import:

from nltk.book import \*

В результате будет импортировано девять книг:

\*\*\* Introductory Examples for the NLTK Book \*\*\*

Loading text1, ..., text9 and sent1, ..., sent9

Type the name of the text or sentence to view it. 

Type: 'texts\(\)' or 'sents\(\)' to list the

materials. 

text1: Moby Dick by Herman Melville 1851

text2: Sense and Sensibility by Jane Austen 1811

text3: The Book of Genesis

text4: Inaugural Address Corpus

text5: Chat Corpus

text6: Monty Python and the Holy Grail

text7: Wall Street Journal

text8: Personals Corpus

text9: The Man Who Was Thursday by G . K . 

Chesterton 1908

Во всех следующих примерах мы будем работать с text6, Monty Python and the Holy Grail \(«Монти Пайтон и священный

Грааль», киносценарий 1975 года\). 

К текстовым объектам применимы все те же манипуляции, что и к обычным массивам Python: мы рассматриваем текст

как массив, состоящий из слов этого текста. Используя данное

свойство, можно подсчитать количество уникальных слов в

тексте и сравнить его с общим количеством слов \(как мы

помним, объекты set в Python содержат только уникальные

значения\):

>>> len\(text6\)/len\(set\(text6\)\) 7.833333333333333

Как видим, каждое слово в этом скрипте использовалось в

среднем около восьми раз. Мы также можем поместить текст в

объект распределения частот, чтобы определить наиболее

распространенные слова и частоту появления различных слов:

>>> from nltk import FreqDist

>>> fdist = FreqDist\(text6\)

>>> fdist.most\_common\(10\)

\[\(':', 1197\), \('.', 816\), \('\!', 801\), \(',', 731\), \("'", 421\), \('\[', 319\), 

\('\]', 

312\), 

\('the', 

299\), 

\('I', 

255\), 

\('ARTHUR', 225\)\]

>>> fdist\["Grail"\]

34

Поскольку это скрипт, то в нем могут появляться отдельные

специфические элементы, свойственные именно скриптам. 

Так, слово ARTHUR, написанное заглавными буквами, часто

встречается потому, что ставится перед каждой репликой

короля Артура. Кроме того, перед каждой новой репликой

стоит двоеточие \(:\), которое служит разделителем между ней и

именем персонажа. Используя данный факт, легко подсчитать, что в фильме 1197 реплик\! 

То, что в предыдущих главах мы называли 2-граммами, NLTK называет *биграммами* \(иногда также 3-граммы называют

*триграммами*, но я предпочитаю не биграммы и триграммы, а

2-граммы и 3-граммы\). Создание, поиск и перечисление 2-грамм выполняются очень легко:

>>> from nltk import bigrams

>>> bigrams = bigrams\(text6\)

>>> bigramsDist = FreqDist\(bigrams\)

>>> bigramsDist\[\('Sir', 'Robin'\)\]

18

Для нахождения всех 2-грамм *Sir Robin* нужно разбить эту 2-грамму на кортеж \( *Sir*, *Robin*\), чтобы она соответствовала

представлению 2-грамм в распределении частот. Вдобавок

существует модуль trigrams, который работает точно так же. 

Для общего случая можно импортировать модуль ngrams:

>>> from nltk import ngrams

>>> fourgrams = ngrams\(text6, 4\)

>>> fourgramsDist = FreqDist\(fourgrams\)

>>> fourgramsDist\[\('father', 'smelt', 'of', 

'elderberries'\)\]

1

Здесь функция ngrams вызывается для разбиения

текстового объекта на *n*-граммы любого размера, который

определяется вторым параметром функции. В данном случае

мы разбиваем текст на 4-граммы. Затем можно

продемонстрировать, что фраза *father smelt of elderberries* \(«отец

пахнет бузиной»\) встречается в скрипте ровно один раз. 

Распределения частот, текстовые объекты и *n*-граммы также

могут перебираться и обрабатываться в цикле. Например, в

следующем коде выводятся на экран все 4-граммы, которые

начинаются со слова *coconut* \(кокос\):

from nltk.book import \*

from nltk import ngrams

fourgrams = ngrams\(text6, 4\)

for fourgram in fourgrams:

if fourgram\[0\] == 'coconut': print\(fourgram\)

В состав библиотеки NLTK входит обширный набор

инструментов и объектов, предназначенных для упорядочения, подсчета, сортировки и измерения параметров больших

объемов текста. Мы лишь едва затронули варианты их

применения, однако большинство этих инструментов хорошо

спроектированы и интуитивно вполне понятны для тех, кто

знаком с Python. 

**Лексикографический анализ с помощью NLTK**

До сих пор мы сравнивали и классифицировали все слова в

тексте только по их собственному значению. Мы не делали

различий между омонимами и не учитывали контекст, в

котором использовались эти слова. 

Некоторые считают, что омонимы не стоит учитывать, так

как они редко представляют интерес. Однако вы будете

удивлены, насколько часто они встречаются. Большинство

носителей английского языка обычно даже не замечают, что то

или иное слово является омонимом, и тем более не считают, будто в другом контексте это слово можно спутать с другим. 

Предложение *He was objective in achieving his objective of writing* *an objective philosophy, primarily using verbs in the objective case*16

вполне понятно для человека, но веб-скрапер может посчитать, что одно и то же слово используется здесь четыре раза, и

просто отбросить какую-либо информацию о значении

каждого из этих слов. 

Кроме определения частей речи, может быть полезно

различать способы употребления слов. Например, можно

находить названия компаний, составленные из обычных

английских слов, или анализировать чьи-либо мнения о

компании. Смысл предложений «Продукты ACME — это

хорошо» и «Продукты ACME — это неплохо», в сущности, один

и тот же, несмотря на то что в одном из них используется слово

«хорошо», а в другом — «плохо». 

**Теги Penn Treebank**

В NLTK по умолчанию применяется популярная система

обозначения частей речи, разработанная в Пенсильванском

университете 

в 

рамках 

проекта 

Penn 

Treebank

\(**https://catalog.ldc.upenn.edu/LDC99T42**\). 

Хотя 

одни

обозначения в ней имеют определенный смысл \(так, CC — это

coordinating conjunction, соединительный союз\), другие

способны сбить с толку \(например, RP означает particle —

«частица»\). Ниже приводится таблица обозначений, использованных в данном подразделе. 

CC Coordinating conjunction

Соединительный союз

CD Cardinal number

Количественное числительное

DT Determiner

Определяющее слово

EX Existential “there” 

Бытийное there

FW Foreign word

Иностранное слово

IN Preposition, subordinating

conjunction

Предлог, подчинительный союз

JJ Adjective

Прилагательное

JJR Adjective, comparative

Прилагательное в сравнительной степени

JJS Adjective, superlative

Прилагательное в превосходной степени

LS List item marker

Обозначение элемента списка

MD Modal

Модальный

NN Noun, singular or mass

Существительное в единственном числе или

неисчисляемое

NNS Noun, plural

Существительное во множественном числе

NNP Proper noun, singular

Имя собственное в единственном числе

NNPS Proper noun, plural

Имя собственное во множественном числе

PDT Predeterminer

Предетерминатив

POS Possessive ending

Притяжательное окончание

PRP Personal pronoun

Личное местоимение

PRP$ Possessive pronoun

Притяжательное местоимение

RB

Adverb

Наречие

RBR Adverb, comparative

Наречие, сравнительное

RBS Adverb, superlative

Наречие, превосходная степень

RP

Particle

Частица

SYM Symbol

Символ

TO

“to” 

Слово «В»

UH

Interjection

Междометие

VB

Verb, base form

Глагол в неопределенной форме

VBD Verb, past tense

Глагол в прошедшем времени

VBG Verb, gerund or present

Глагол, герундий или причастие настоящего

participle

времени

VBN Verb, past participle

Глагол, причастие прошедшего времени

VBP Verb, non-third-person

Глагол настоящего времени, единственного числа, 

singular present

не третьего лица

VBZ Verb, third person singular

Глагол настоящего времени, единственного числа, 

present

третьего лица

WDT wh-determiner

wh-определяющее слово

WP Wh-pronoun

Wh-местоимение

WP$ Possessive wh-pronoun

Притяжательное wh-местоимение

WRB Wh-adverb

Wh-наречие

Помимо измерения параметров языка, с помощью NLTK

можно определять значения слов на основе контекста и

собственных обширных словарей. В простейшем случае NLTK

позволяет идентифицировать части речи:

>>> from nltk.book import \*

>>> from nltk import word\_tokenize

>>> text = word\_tokenize\('Strange women lying in ponds distributing swords'\\

'is no basis for a system of government.'\)

>>> from nltk import pos\_tag

>>> pos\_tag\(text\)

\[\('Strange', 

'NNP'\), 

\('women', 

'NNS'\), 

\('lying', 'VBG'\), \('in', 'IN'\)

, \('ponds', 'NNS'\), \('distributing', 'VBG'\), \('swords', 'NNS'\), \('is' 

, 'VBZ'\), \('no', 'DT'\), \('basis', 'NN'\), 

\('for', 'IN'\), \('a', 'DT'\), 

\('system', 'NN'\), \('of', 'IN'\), \('government', 

'NN'\), \('.', '.'\)\]

Каждое слово выделено в *кортеж*, содержащий само слово, и 

тег, 

идентифицирующий 

данную 

часть 

речи

\(дополнительную информацию об этих тегах см. в предыдущей

врезке\). На первый взгляд такой поиск может показаться

простым, однако в следующем примере видно, как сложно

правильно выполнить эту задачу:

>>> text = word\_tokenize\('The dust was thick so he had to dust'\)

>>> pos\_tag\(text\)

\[\('The', 'DT'\), \('dust', 'NN'\), \('was', 'VBD'\), 

\('thick', 'JJ'\), \('so

', 'RB'\), \('he', 'PRP'\), \('had', 'VBD'\), \('to', 

'TO'\), \('dust', 'VB'\)\]

Обратите внимание: слово *dust* использовано в

предложении дважды: один раз как существительное \(dust\), другой раз — как глагол \(to dust\)17. NLTK правильно распознает

оба варианта применения, основываясь на контексте этих слов

в предложении. NLTK идентифицирует части речи с помощью

контекстно-свободной 

грамматики, 

существующей 

в

английском языке. *Контекстно-свободные грамматики* —

наборы правил, по которым можно определить, какие

элементы могут следовать за теми или иными элементами в

упорядоченных списках. В данном случае эти правила

определяют, какие части речи могут идти за другими частями

речи. Всякий раз, когда в тексте встречается неоднозначное

слово, такое как *dust*, применяются правила контекстно-свободной 

грамматики 

и 

выбирается 

часть 

речи, 

соответствующая этим правилам. 

**Виды машинного обучения**

NLTK позволяет создавать совершенно новые контекстно-свободные грамматики — например, при обучении

иностранному 

языку. 

Если 

вручную 

разметить

соответствующими тегами Penn Treebank большие разделы

текста на этом языке, то их можно передать обратно в NLTK и

обучить программу правильно размечать другой текст, с

которым ей предстоит столкнуться. Такой тип обучения —

необходимый элемент любого машинного обучения, о

котором мы еще поговорим в главе 14, когда будем обучать

веб-скраперы распознавать символы капчи. 

Зачем нам знать, является ли слово глаголом или

существительным в данном контексте? Вероятно, это имеет

смысл 

в 

научно-исследовательской 

лаборатории 

по

информатике, но как может помочь при веб-скрапинге? 

Одна из распространенных задач веб-скрапинга связана с

поиском. Например, при веб-скрапинге текста с сайта вы

можете захотеть найти слово *google*, но только в значении

глагола «гуглить», а не как имя собственное. Или же, возможно, вам нужны только упоминания компании Google и при их

поиске вы не хотите полагаться на правильное употребление

заглавных букв. В таких случаях чрезвычайно полезна функция

pos\_tag:

from nltk import word\_tokenize, sent\_tokenize, pos\_tag

sentences = sent\_tokenize\('Google is one of the

best companies in the world.'\\

' I constantly google myself to see what I\\'m up to.'\)

nouns = \['NN', 'NNS', 'NNP', 'NNPS'\]

 

for sentence in sentences:

if 'google' in sentence.lower\(\):

 

 

 

 

 

 

 

 

taggedWords 

=

pos\_tag\(word\_tokenize\(sentence\)\)

for word in taggedWords:

if word\[0\].lower\(\) == 'google' 

and word\[1\] in nouns:

print\(sentence\)

Данная программа выводит только те предложения, в

которых содержится слово *google* \(или *Google*\) в значении

существительного, а не глагола. Конечно, мы могли бы

выразиться точнее и потребовать, чтобы выводились только

экземпляры Google, помеченные как *NNP* \(имя собственное\). 

Но даже NLTK время от времени допускает ошибки, поэтому

иногда имеет смысл оставить немного места для маневра, в

зависимости от конкретного приложения. 

Функция pos\_tag из библиотеки NLTK позволяет устранить

большую часть неоднозначностей естественного языка. Если

искать в тексте не просто конкретное слово или фразу, а *с*

*учетом* тега, то можно значительно повысить точность и

эффективность поиска веб-скрапера. 


