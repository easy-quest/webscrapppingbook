**Рис. 3.1. ** Блок-схема нашего сценария краулинга сайтов

**Не используйте учебные примеры программ в реальных**

**приложениях**

Еще раз напоминаю: ради экономии места и читабельности

примеры программ, приведенные в этой книге, не всегда

содержат необходимые проверки и обработки исключений, требуемые в готовом коде. Например, если веб-краулер не

обнаружил на сайте ни одной внешней ссылки \(что

маловероятно, но в случае достаточно долгой работы

программы в какой-то момент обязательно произойдет\), то

программа будет продолжать работу, пока не достигнет

предела рекурсии Python. 

Одним из простых способов повысить надежность этого веб-краулера было бы объединить его с кодом обработки

исключений при соединении, описанном в главе 1. Это

позволило бы в случае возникновения ошибки HTTP или

исключения на сервере при получении страницы выбрать

другой URL и перейти по нему. 

Прежде чем использовать этот код для каких-либо серьезных

целей, обязательно добавьте в него проверки для устранения

потенциальных ловушек. 

Разделение задачи на простые функции, такие как «найти

все внешние ссылки на странице», имеет преимущество: впоследствии код можно легко перестроить для выполнения

другой задачи веб-краулинга. Например, если цель состоит в

том, чтобы просмотреть сайт, найти все внешние ссылки и

сохранить их, то можно добавить следующую функцию. 

\# Составляет список из всех внешних URL, 

найденных на сайте. 

allExtLinks = set\(\)

allIntLinks = set\(\)

 

def getAllExternalLinks\(siteUrl\):

html = urlopen\(siteUrl\)

 

 

 

 

domain 

=

'\{\}://\{\}'.format\(urlparse\(siteUrl\).scheme, 

urlparse\(siteUrl\).netloc\)

bs = BeautifulSoup\(html, 'html.parser'\)

internalLinks = getInternalLinks\(bs, 

domain\)

externalLinks = getExternalLinks\(bs, 

domain\)

 

![Image 38](images/000007.png)

for link in externalLinks:

if link not in allExtLinks:

allExtLinks.add\(link\)

print\(link\)

for link in internalLinks:

if link not in allIntLinks:

allIntLinks.add\(link\)

getAllExternalLinks\(link\)

 

allIntLinks.add\('http://oreilly.com'\)

getAllExternalLinks\('http://oreilly.com'\)

Этот код можно представить как два совместно работающих

цикла: один собирает внутренние ссылки, а второй — внешние. 

Блок-схема программы выглядит примерно так \(рис. 3.2\). 

 

**Рис. 3.2. ** Блок-схема веб-краулера сайтов, который собирает все внешние ссылки

Привычка составлять схемы и диаграммы того, что должен

делать код, прежде чем писать его, фантастически полезна. По

мере усложнения ваших веб-краулеров она сэкономит вам кучу

времени и избавит от многих разочарований. 

4 Благодарю сайт The Oracle of Bacon \(http://oracleofbacon.org/\), с помощью

которого я удовлетворила свое любопытство относительно этой цепочки. 

**Глава 4. Модели веб-краулинга**

Писать чистый и масштабируемый код довольно сложно, даже

если у вас есть возможность контролировать входные и

выходные данные. Написание кода веб-краулера, которому

иногда приходится выполнять веб-скрапинг и сохранять

различные данные с разных групп сайтов никак не

контролируемых программистом часто представляет собой

невероятно сложную организационную задачу. 

Вам могут предложить собрать новостные статьи или

публикации из блогов, размещенных на различных сайтах, у

каждого из которых свои шаблоны и макеты. На одном сайте

тег h1 может содержать заголовок статьи, а на другом —

заголовок самого сайта, а заголовок статьи будет заключен в

тег <spanid="title">. 

Возможно, вам понадобится гибко управлять тем, для каких

сайтов нужно выполнить веб-скрапинг и как именно это

делать, а также способ быстро добавлять новые сайты или

изменять существующие — максимально быстро и без

необходимости писать много строк кода. 

Вас могут попросить собрать цены на товары с разных

сайтов, чтобы в итоге можно было сравнивать цены на один и

тот же товар. Возможно, они будут представлены в разных

валютах. Вдобавок, вероятно, потребуется объединить их с

внешними данными, полученными из какого-то другого

источника, не имеющего отношения к Интернету. 

Несмотря на то что вариантов применения веб-краулеров

бесчисленное множество, большие масштабируемые веб-краулеры, как правило, относятся к одному из нескольких

типов. Исследуя эти типы и распознавая ситуации, в которых

они используются, можно значительно повысить удобство

сопровождения и надежность своих веб-краулеров. 

В этой главе мы уделим основное внимание веб-краулерам, собирающим ограниченное количество «типов» данных \(таких

как обзоры ресторанов, новостные статьи, профили компаний\) с большого количества сайтов и хранящим эти данные в виде

объектов Python, которые читают и записывают в базу данных. 

**Планирование и определение объектов**

Одна из распространенных ловушек веб-скрапинга —

определение данных, которые разработчики намерены

собирать, исключительно на основании того, что есть у них

перед глазами. Например, желая собрать данные о товаре, можно было бы для начала заглянуть в магазин одежды и

решить, что для каждого интересующего нас товара

необходимо получить данные из следующих полей:

• наименование товара; 

• цена; 

• описание; 

• размеры; 

• цвета; 

• тип ткани; 

• рейтинг клиентов. 

Посмотрев на другой сайт, вы обнаружите, что у товара есть

SKU \(stock keeping unit, единицы хранения, или артикул, используемый для отслеживания и заказа товаров\). Вы

наверняка захотите собирать и эти данные, даже если их не

было на первом сайте\! И вы добавите в список данное поле:

• артикул. 

Для начала одежда вполне подойдет, однако необходимо

убедиться, что этот веб-скрапер можно будет распространить и

на другие типы товаров. Начав просматривать разделы товаров

на других сайтах, вы поймете, что также необходимо собрать

следующую информацию:

• твердая/мягкая обложка; 

• матовая/глянцевая печать; 

• количество отзывов клиентов; 

• ссылка на сайт производителя. 

Понятно, что такой подход неприемлем. Просто добавляя

атрибуты к типу товара всякий раз, когда на сайте встречается

новая информация, вы получите слишком большое количество

полей, которые нужно отслеживать. Мало того, каждый раз при

веб-скрапинге нового сайта придется подробно анализировать

поля, уже имеющиеся на нем, и те, что уже были накоплены

ранее, а также, возможно, добавлять новые \(изменяя тип

объекта Python и структуру базы данных\). Это приведет к

запутанному и трудночитаемому набору данных, а

следовательно, к проблемам при его использовании. 

Принимая решение о том, какие данные собирать, зачастую

лучше игнорировать сайты. Нельзя запустить проект, рассчитанный на то, чтобы стать большим и масштабируемым, посмотрев только на один сайт и спросив себя: «Что здесь

есть?» Вместо этого нужно задать другой вопрос: «Что мне

нужно?» — а затем найти способы поиска необходимой

информации. 

Возможно, в действительности вам нужно всего лишь

сравнивать цены на товары в нескольких магазинах и

отслеживать изменения этих цен с течением времени. В этом

случае вам необходима только та информация, которая бы

позволила однозначно идентифицировать товар:

• название товара; 

• производитель; 

• идентификационный номер товара \(если он есть и интересен

вам\). 

Важно отметить, что ни одно из этих свойств товара не

относится к конкретному магазину. Например, обзоры товаров, рейтинги, цены и даже описания одного и того же товара могут

отличаться в разных магазинах, и их стоит хранить отдельно. 

Другие данные \(цветовые варианты товара и материал, из

которого он сделан\) относятся к товару, но могут быть

разреженными — существовать не для всех единиц товара. 

Важно сделать шаг назад, составить контрольный список для

каждого свойства товара, которое вы считаете необходимым, и

задать себе следующие вопросы. 

• Поможет ли данная информация достичь целей проекта? 

Зайду ли я в тупик, если не получу ее, или это лишь данные

из разряда «пригодится», но, по большому счету, они ни на

что не влияют? 

• Если эти данные когда-нибудь *могут* пригодиться \(а могут и

нет\) — насколько сложно будет вернуться и собрать их? 

• Являются ли эти данные избыточными относительно уже

собранных? 

• Имеет ли логический смысл хранить данные именно в этом

объекте? \(Как уже упоминалось, сохранение описания в

качестве свойства товара не имеет смысла, если описание

одного и того же товара может быть разным на разных

сайтах.\)

Определившись, какие данные нужно собрать, важно задать

себе еще несколько вопросов, чтобы затем решить, как хранить

и обрабатывать эти данные в коде. 

• Эти данные разреженные или плотные? Свойственны ли они

любому товару, в любом списке или же существуют только

для небольшого множества товаров? 

• Насколько велик объем данных? 

• Особенно это касается больших данных: нужно ли будет

регулярно получать их каждый раз при выполнении анализа

или только в отдельных случаях? 

• Насколько переменчивы данные этого типа? Придется ли

регулярно добавлять новые атрибуты, изменять типы

\(например, часто могут добавляться ткани с новыми

узорами\), или же эти данные никогда не изменяются

\(допустим, размеры обуви\)? 

Предположим, вы хотите выполнить некий метаанализ

зависимости цены от свойств товара: например, от количества

страниц в книге или типа ткани, из которой сшита одежда; также, возможно, в будущем вы захотите добавить другие

атрибуты, от которых зависит цена. Просматривая подобные

вопросы, вы можете обнаружить, что эти данные являются

разреженными \(сравнительно немногие товары имеют хотя бы

один из указанных атрибутов\), и решить, что придется часто

добавлять или удалять атрибуты. В этом случае, вероятно, стоит создать тип товара, который выглядит следующим

образом:

• название товара; 

• производитель; 

• идентификационный номер товара \(если есть и нужен\); 

• атрибуты \(необязательный параметр; список или словарь\). 

Тип атрибута выглядит так:

• имя атрибута; 

• значение атрибута. 

Со временем это позволит гибко добавлять новые атрибуты

товара, не прибегая к необходимости изменять схему данных

или переписывать код. Определяясь с тем, как хранить эти

атрибуты в базе данных, вы можете решить записывать их в

поле attribute в формате JSON или же хранить каждый

атрибут в отдельной таблице, откуда извлекать их по

идентификатору товара. Подробнее о реализации этих типов

моделей баз данных см. в главе 6. 

Эти же вопросы можно задать и для другого типа

информации, которую вам нужно будет хранить. Чтобы

отслеживать цены, найденные для каждого товара, вам, вероятно, потребуется следующее:

• идентификатор товара; 

• идентификатор магазина; 

• цена; 

• дата или метка времени для данной цены. 

Что будет в случае, если атрибуты товара действительно

влияют на его цену? Например, за рубашку большего размера

магазин может выставить более высокую цену, чем за рубашку

меньшего, так как пошив требует больше труда или

материалов. В этом случае, возможно, стоит преобразовать

товар «рубашка» в список товаров, по одному для каждого

размера \(чтобы у каждого вида рубашки была независимая

цена\), или же создать новый тип элемента для хранения

информации об экземплярах товара, содержащий следующие

поля:

• идентификатор товара; 

• тип экземпляра \(в данном случае размер рубашки\). 

Тогда каждая цена будет выглядеть так:

• идентификатор экземпляра товара; 

• идентификатор магазина; 

• цена; 

• дата или метка времени для данной цены. 

Тема «товары и цены» может показаться слишком узкой, однако основные вопросы, которые нужно себе задать, и

логика, используемая при разработке этих объектов Python, применимы практически в любой ситуации. 

При веб-скрапинге новостных публикаций может

потребоваться следующая основная информация:

• заголовок; 

• автор; 

• дата; 

• контент. 

Но предположим, что некоторые статьи содержат «дату

изменения», или «связанные публикации», или «количество

перепостов в социальных сетях». Вам нужна эта информация? 

Она имеет отношение к вашему проекту? Как эффективно и

гибко хранить количество перепостов в социальных сетях, если

только некоторые новостные сайты используют все формы

социальных сетей, а со временем популярность разных

социальных сетей может увеличиваться или уменьшаться? 

При разработке нового проекта может возникнуть соблазн

сразу погрузиться в написание кода на Python, чтобы

немедленно приступить к веб-скрапингу. Но если оставить

создание модели данных на потом, то доступность и формат

данных часто будут определяться первым попавшимся сайтом. 

Однако модель данных является основой всего кода, который ее использует. Неправильный выбор модели легко

может привести к проблемам с написанием и сопровождением

кода или к затруднениям при извлечении и эффективном

применении полученных данных. Особенно это касается

работы с разными сайтами: и известными, и неизвестными. 

Притом жизненно важно все хорошенько обдумать и

спланировать, какие именно данные нужно собирать и как их

хранить. 

**Работа с различными макетами сайтов**

Одно из наиболее впечатляющих достижений поисковых

систем, таких как Google, состоит в возможности извлекать

релевантные и полезные данные с различных сайтов, не имея

предварительных знаний об их структуре. Мы, люди, способны

с первого взгляда определить, где у страницы заголовок, а где

— основ ной контент \(за исключением случаев крайне плохого

веб-дизайна\), однако заставить бот делать то же самое гораздо

труднее. 

К счастью, в большинстве случаев при веб-краулинге мы не

намерены собирать данные с сайтов, которые никогда прежде

не видели. Обычно речь идет максимум о нескольких десятках

сайтов, предварительно отобранных человеком. Это значит, что нам не понадобятся сложные алгоритмы или машинное

обучение для определения того, какой текст на странице

«больше всего похож на заголовок», а какой, скорее всего, является «основным контентом». Все эти элементы можно

задать вручную. 

Наиболее очевидный подход — написать отдельный веб-краулер или парсер страниц для каждого сайта. Каждый такой

краулер может принимать URL, строку или объект

BeautifulSoup и возвращать результат веб-скрапинга в виде

объекта Python. 

Ниже приведены пример класса Content \(представляющего

собой фрагмент контента сайта, например новостную

публикацию\) и две функции веб-скрапинга, которые

принимают объект BeautifulSoup и возвращают экземпляр

Content:

import requests

 

class Content:

def \_\_init\_\_\(self, url, title, body\):

self.url = url

self.title = title

self.body = body

 

def getPage\(url\):

req = requests.get\(url\)

 

 

 

 

return 

BeautifulSoup\(req.text, 

'html.parser'\)

 

def scrapeNYTimes\(url\):

bs = getPage\(url\)

title = bs.find\('h1'\).text

 

 

 

 

lines 

=

bs.select\('div.StoryBodyCompanionColumn div p'\)

body = '\\n'.join\(\[line.text for line in

lines\]\)

return Content\(url, title, body\)



def scrapeBrookings\(url\):

bs = getPage\(url\)

title = bs.find\('h1'\).text

body = bs.find\('div', \{'class', 'post-

body'\}\).text

return Content\(url, title, body\)

 

url = 'https://www.brookings.edu/blog/future-

development/2018/01/26/' 

'delivering-inclusive-urban-access-3-

uncomfortable-truths/' 

content = scrapeBrookings\(url\)

print\('Title: \{\}'.format\(content.title\)\)

print\('URL: \{\}\\n'.format\(content.url\)\)

print\(content.body\)

 

url 

=

'https://www.nytimes.com/2018/01/25/opinion/sun

day/' 

'silicon-valley-immortality.html' 

content = scrapeNYTimes\(url\)

print\('Title: \{\}'.format\(content.title\)\)

print\('URL: \{\}\\n'.format\(content.url\)\)

print\(content.body\)

Добавляя функции веб-скрапинга для других новостных

сайтов, вы, вероятно, заметите некоторую закономерность. В

сущности, функция синтаксического анализа любого сайта

делает одно и то же:

• находит элемент заголовка и извлекает оттуда текст

заголовка; 

• находит основной контент статьи; 

• при необходимости находит другие элементы контента; 

• возвращает объект Content, созданный с помощью ранее

найденных строк. 

Единственное, что здесь действительно зависит от сайта, —

это CSS-селекторы, используемые для получения каждого

элемента информации. Функции BeautifulSoup find и

find\_all принимают два аргумента: строку тега и словарь

атрибутов в формате «ключ — значение», вследствие чего эти

аргументы можно передавать как параметры, которые

определяют структуру сайта и расположение нужных данных. 

Чтобы было еще удобнее, вместо аргументов тегов и пар

«ключ — значение» можно использовать функцию

BeautifulSoup select, принимающую строку CSS-селектора для

каждого элемента информации, который вы хотите получить, и

разместить все эти селекторы в словарном объекте: class Content:

""" 

Общий родительский класс для всех статей/

страниц. 

""" 

def \_\_init\_\_\(self, url, title, body\):

self.url = url

self.title = title

self.body = body



def print\(self\):

""" 

Гибкая функция печати, управляющая выводом

данных. 

""" 

print\('URL: \{\}'.format\(self.url\)\)

print\('TITLE: \{\}'.format\(self.title\)\)

print\('BODY:\\n\{\}'.format\(self.body\)\)

 

class Website:

""" 

Содержит информацию о структуре сайта. 

""" 

def \_\_init\_\_\(self, name, url, titleTag, 

bodyTag\):

self.name = name

self.url = url

self.titleTag = titleTag

self.bodyTag = bodyTag

Обратите внимание: в классе Website хранится не

информация, собранная с разных страниц, а инструкции о том, *как* ее собирать. Так, здесь хранится не заголовок «Название

моей страницы», а лишь строка с тегом h1, который указывает

на то, где содержатся заголовки. Именно поэтому класс

называется Website \(его информация относится ко всему

сайту\), а не Content \(в котором содержится информация

только с одной страницы\). 

С помощью классов Content и Website можно написать

класс Crawler для веб-скрапинга заголовка и контента, 

размещенных по любому URL веб-страницы, принадлежащей

данному сайту:

import requests

from bs4 import BeautifulSoup

 

class Crawler:

def getPage\(self, url\):

try:

req = requests.get\(url\)

 

 

 

 

 

 

 

 

except

requests.exceptions.RequestException:

return None

return BeautifulSoup\(req.text, 

'html.parser'\)

 

def safeGet\(self, pageObj, selector\):

""" 

Служебная функция, используемая для

получения строки

содержимого из объекта BeautifulSoup и

селектора. 

Если объект для данного селектора не

найден, 

то возвращает пустую строку. 

""" 

 

 

 

 

 

 

 

 

selectedElems 

=

pageObj.select\(selector\)

if selectedElems is not None and

len\(selectedElems\) > 0:

return '\\n'.join\(

\[elem.get\_text\(\) for elem in selectedElems\]\)

return '' 

 

def parse\(self, site, url\):

""" 

Извлекает содержимое страницы с

заданным URL. 

""" 

bs = self.getPage\(url\)

if bs is not None:

title = self.safeGet\(bs, 

site.titleTag\)

body = self.safeGet\(bs, 

site.bodyTag\)

if title \!= '' and body \!= '':

content = Content\(url, title, 

body\)

content.print\(\)

А следующий код определяет объекты сайтов и запускает

весь процесс:

crawler = Crawler\(\)

 

siteData = \[

\['O\\'Reilly Media', 'http://oreilly.com', 

'h1', 'section\#product-description'\], 

\['Reuters', 'http://reuters.com', 'h1', 

'div.StandardArticleBody\_body\_1gnLA'\], 

\['Brookings', 'http://www.brookings.edu', 

'h1', 'div.post-body'\], 

\['New York Times', 'http://nytimes.com', 

'h1', 'div.StoryBodyCompanionColumn div p'\]

\]

websites = \[\]

for row in siteData:

websites.append\(Website\(row\[0\], row\[1\], 

row\[2\], row\[3\]\)\)

 

crawler.parse\(websites\[0\], 

'http://shop.oreilly.com/product/'\\

'0636920028154.do'\)

crawler.parse\(websites\[1\], 

'http://www.reuters.com/article/'\\

'us-usa-epa-pruitt-idUSKBN19W2D0'\)

crawler.parse\(websites\[2\], 

'https://www.brookings.edu/blog/'\\

'techtank/2016/03/01/idea-to-retire-old-

methods-of-policy-education/'\)

crawler.parse\(websites\[3\], 

'https://www.nytimes.com/2018/01/'\\

'28/business/energy-environment/oil-

boom.html'\)

На первый взгляд этот новый способ может показаться

удивительно простым, по сравнению с написанием отдельной

функции Python для каждого сайта. Однако представьте, что

произойдет при переходе от системы с четырьмя сайтами-источниками к системе с 20 или 200 источниками. 

Написать список строк относительно легко, и он не займет

много места. Такой список можно загрузить из базы данных

или CSV-файла, импортировать из удаленного источника или

передать кому-то, кто не является программистом, но умеет

заполнять формы и вводить новые сайты через

пользовательский интерфейс, и этот человек никогда не увидит

ни одной строки кода. 

Конечно, здесь есть недостаток: мы отказываемся от

определенной гибкости. В первом примере у каждого сайта

есть собственная функция, написанная в свободной форме, для

выбора и — если получение результата того требует —

синтаксического анализа HTML-кода. Во втором примере все

сайты должны иметь схожую структуру, в которой

гарантированно существуют определенные поля. Полученные

из них данные должны быть чистыми, а каждому интересу-ющему нас полю следует иметь уникальный и надежный CSS-селектор. 

Однако я считаю, что широкие возможности и

относительная гибкость данного подхода более чем

компенсируют его реальные или предполагаемые недостатки. 

В следующем разделе мы рассмотрим конкретные способы

применения и варианты расширения этой базовой схемы, которые позволят, например, справляться с отсутствующими

полями, собирать различные типы данных, проверять только

определенные части сайта и хранить более сложную

информацию о страницах. 

**Структурирование веб-краулеров**

Создание гибких и изменяемых типов разметки сайтов не

принесет особой пользы, если все равно придется вручную

искать каждую ссылку, чтобы выполнить для нее веб-скрапинг. 

В предыдущей главе были показаны различные способы

извлечения данных с сайтов и автоматического поиска новых

страниц. 

В этом разделе будет показано, как встроить эти методы в

хорошо структурированный и расширяемый веб-краулер, который бы автоматически собирал ссылки и находил нужные

данные. Здесь я представлю только три основные структуры

веб-краулера, хотя считаю их пригодными к применению в

большинстве ситуаций, которые вам, скорее всего, встретятся

при извлечении данных с реальных сайтов, — разве что

придется внести пару изменений. Я также надеюсь, что если

вам попадется необычный случай, связанный с нестандартной

задачей веб-краулинга, то эти структуры послужат для вас

источником вдохновения и позволят построить веб-краулер с

красивой и надежной структурой. 

**Веб-краулинг с помощью поиска**

Один из самых простых способов сбора данных с сайта — тот

же, что используют люди: с помощью панели поиска. Поиск на

сайте по ключевому слову или теме и последующий сбор

данных по списку результатов может показаться задачей, весьма зависящей от конкретного сайта, однако несколько

ключевых моментов делают ее на удивление примитивной. 

• Большинство сайтов получают список результатов поиска по

определенной теме, передавая ее в виде строки через

параметр 

URL, 

например: 

http://example.com? 

search=мояТема. Первую часть этого URL можно сохранить

как свойство объекта Website и потом просто каждый раз

добавлять к нему тему. 

• Выполнив поиск, большинство сайтов формируют страницы

результатов в виде легко идентифицируемого списка

ссылок, обычно заключенных в удобный тег наподобие

<spanclass="result">, точный формат которого тоже

можно сохранить в виде свойства объекта Website. 

• Каждая *ссылка на результат поиска* представляет собой либо

относительный URL \(такой как /articles/page.html\), либо 

абсолютный 

\(например, 

http://example.com/articles/page.html\). Независимо

от того, какой вариант вы ожидаете — абсолютный или

относительный, — URL можно сохранить как свойство

объекта Website. 

• Найдя и нормализовав URL на странице поиска, мы успешно

сводим задачу к примеру, рассмотренному в предыдущем

разделе, — извлечению данных со страницы сайта, имеющей заданный формат. 

Рассмотрим реализацию этого алгоритма в виде кода. Класс

Content здесь почти такой же, как в предыдущих примерах. 

Мы только добавили свойство URL, которое позволит

отслеживать, откуда взят этот контент:

class Content:

"""Общий родительский класс для всех

статей/страниц""" 

 

def \_\_init\_\_\(self, topic, url, title, 

body\):

self.topic = topic

self.title = title

self.body = body

self.url = url

 

def print\(self\):

""" 

Гибкая функция печати, управляющая

выводом данных

""" 

print\('New article found for topic:

\{\}'.format\(self.topic\)\)

print\('URL: \{\}'.format\(self.url\)\)

print\('TITLE: \{\}'.format\(self.title\)\)

print\('BODY:\\n\{\}'.format\(self.body\)\)

Мы также добавили к классу Website несколько новых

свойств. Свойство searchUrl определяет, по какому адресу

следует обратиться, чтобы получить результаты поиска, если

добавить к нему искомую тему. Свойство resultListing определяет блок, в котором заключена информация о каждом

результате поиска, а resultUrl — тег внутри этого блока, содержащий точный URL результата. Свойство absoluteUrl представляет собой значение логического типа, указывающее

на то, какими ссылками являются результаты поиска —

абсолютными или относительными. 

class Website:

"""Содержит информацию о структуре сайта""" 

 

def \_\_init\_\_\(self, name, url, searchUrl, 

resultListing, 

resultUrl, absoluteUrl, titleTag, 

bodyTag\):

self.name = name

self.url = url

self.searchUrl = searchUrl

self.resultListing = resultListing self.resultUrl = resultUrl

self.absoluteUrl=absoluteUrl

self.titleTag = titleTag

self.bodyTag = bodyTag

Мы немного расширили файл crawler.py — теперь в нем

содержатся данные Website, список тем для поиска и два

цикла, в которых перебираются все темы и сайты. В этом файле

также содержится функция search, переходящая на страницу

поиска заданного сайта с заданной темой и извлекающая

оттуда все найденные URL, перечисленные на странице

результатов поиска. 

import requests

from bs4 import BeautifulSoup

 

class Crawler:

 

def getPage\(self, url\):

try:

req = requests.get\(url\)

 

 

 

 

 

 

 

 

except

requests.exceptions.RequestException:

return None

return BeautifulSoup\(req.text, 

'html.parser'\)

 

def safeGet\(self, pageObj, selector\):

childObj = pageObj.select\(selector\)

if childObj is not None and

len\(childObj\) > 0:

return childObj\[0\].get\_text\(\) return '' 

 

def search\(self, topic, site\):

""" 

Поиск на заданном сайте по заданной

теме

и сохранение всех найденных страниц. 

""" 

bs = self.getPage\(site.searchUrl \+

topic\)

 

 

 

 

 

 

 

 

searchResults 

=

bs.select\(site.resultListing\)

for result in searchResults:

url = result.select\(site.resultUrl\)

\[0\].attrs\['href'\]

\# Проверить, является ли URL

относительным или абсолютным. 

if\(site.absoluteUrl\):

bs = self.getPage\(url\)

else:

bs = self.getPage\(site.url \+

url\)

if bs is None:

print\('Something was wrong with

that page or URL. Skipping\!'\)

return

title = self.safeGet\(bs, 

site.titleTag\)

body = self.safeGet\(bs, 

site.bodyTag\)

if title \!= '' and body \!= '':

content = Content\(topic, title, body, url\)

content.print\(\)

 

crawler = Crawler\(\)

 

siteData = \[

\['O\\'Reilly Media', 'http://oreilly.com', 

'https://ssearch.oreilly.com/?q=', 

'article.product-result', 'p.title a', 

True, 'h1', 

'section\#product-description'\], 

 

 

 

 

\['Reuters', 

'http://reuters.com', 

'http://www.reuters.com/search/news?blob=', 

'div.search-result-content', 'h3.search-

result-title a', False, 'h1', 

'div.StandardArticleBody\_body\_1gnLA'\], 

\['Brookings', 'http://www.brookings.edu', 

'https://www.brookings.edu/search/?s=', 

'div.list-content article', 

'h4.title a', True, 'h1', 'div.post-body'\]

\]

sites = \[\]

for row in siteData:

sites.append\(Website\(row\[0\], row\[1\], 

row\[2\], 

row\[3\], row\[4\], 

row\[5\], row\[6\], row\[7\]\)\)

 

topics = \['python', 'data science'\]

for topic in topics:

print\('GETTING INFO ABOUT: ' \+ topic\)

for targetSite in sites:

crawler.search\(topic, targetSite\)

Этот скрипт перебирает все темы из списка topics и, прежде чем приступить к веб-скрапингу по очередной теме, выводит предупреждение:

GETTING INFO ABOUT python

Затем скрипт просматривает все сайты из списка sites и

сканирует каждый из них по каждой теме. Всякий раз, успешно

находя информацию о странице, скрипт выводит ее в консоль: New article found for topic: python

URL: http://example.com/examplepage.html

TITLE: Page Title Here

BODY: Body content is here

Обратите внимание: скрипт перебирает все темы, проходя

по всем сайтам во внутреннем цикле. Почему бы не поступить

наоборот, сначала перебрав все темы на одном сайте, а затем —

на следующем? Цикл с перебором по темам позволяет более

равномерно распределить нагрузку на веб-серверы. Это

особенно важно, если наш список состоит из нескольких сотен

тем и нескольких десятков сайтов. Не стоит направлять на

один сайт сразу несколько десятков тысяч запросов; лучше

сделать десять запросов, подождать несколько минут, затем

сделать еще десять запросов, подождать еще несколько минут

и т.д. 

В итоге общее количество запросов не изменится, однако, как правило, лучше растянуть их на максимально возможное

время. Обратите внимание: структура наших циклов

обеспечивает простой способ сделать это. 

**Сбор данных с сайтов по ссылкам**

В предыдущей главе было показано несколько способов, позволяющих обнаруживать на веб-страницах внутренние и

внешние ссылки и затем использовать их для сбора данных с

сайта. В данном подразделе мы объединим эти базовые

методы и создадим более гибкий краулер сайтов, способный

переходить по любой ссылке, соответствующей заданному

URL-шаблону. 

Веб-краулер такого типа хорошо работает для проектов, в

которых нужно собрать данные со всего сайта, а не только из

определенных результатов поиска или списка страниц. Этот

метод хорошо работает и для страниц, имеющих между собой

мало общего или совсем ничего. 

В отличие от примера сбора данных со страниц с

результатами поиска, рассмотренного выше, данные типы

краулеров не требуют структурированного метода поиска

ссылок, поэтому атрибуты, описывающие страницу поиска, в

объекте Website не нужны. Однако, поскольку краулеру не

даны конкретные инструкции о том, где и как расположены

ссылки, которые он ищет, требуются некие правила, указывающие на то, какие страницы следует выбрать. Для

этого мы предоставляем targetPattern — регулярное

выражение, описывающее нужные URL — и создаем

логическую переменную absoluteUrl:

class Website:

 

 

 

 

def 

\_\_init\_\_\(self, 

name, 

url, 

targetPattern, absoluteUrl, titleTag, bodyTag\):

self.name = name

self.url = url

self.targetPattern = targetPattern

self.absoluteUrl = absoluteUrl

self.titleTag = titleTag self.bodyTag = bodyTag

 

class Content:

 

def \_\_init\_\_\(self, url, title, body\):

self.url = url

self.title = title

self.body = body

 

def print\(self\):

print\('URL: \{\}'.format\(self.url\)\)

print\('TITLE: \{\}'.format\(self.title\)\)

print\('BODY:\\n\{\}'.format\(self.body\)\)

Класс Content — тот же класс, что и в первом примере веб-краулера. 

Класс Crawler стартует с начальной страницы сайта, находит внутренние ссылки и анализирует контент каждой из

этих внутренних ссылок:

import re

 

class Crawler:

def \_\_init\_\_\(self, site\):

self.site = site

self.visited = \[\]

 

def getPage\(self, url\):

try:

req = requests.get\(url\)



 

 

 

 

 

 

 

except

requests.exceptions.RequestException:

return None

return BeautifulSoup\(req.text, 

'html.parser'\)

 

def safeGet\(self, pageObj, selector\):

 

 

 

 

 

 

 

 

selectedElems 

=

pageObj.select\(selector\)

if selectedElems is not None and

len\(selectedElems\) > 0:

return '\\n'.join\(\[elem.get\_text\(\)

for

elem in selectedElems\]\)

return '' 

 

def parse\(self, url\):

bs = self.getPage\(url\)

if bs is not None:

title = self.safeGet\(bs, 

self.site.titleTag\)

body = self.safeGet\(bs, 

self.site.bodyTag\)

if title \!= '' and body \!= '':

content = Content\(url, title, 

body\)

content.print\(\)

 

def crawl\(self\):

""" 

Получить ссылки с начальной страницы

сайта

""" 

bs = self.getPage\(self.site.url\)

targetPages = bs.find\_all\('a', 

href=re.compile\(self.site.targetPat

tern\)\)

for targetPage in targetPages:

targetPage =

targetPage.attrs\['href'\]

if targetPage not in self.visited:

self.visited.append\(targetPage\)

if not self.site.absoluteUrl:

targetPage = '\{\}

\{\}'.format\(self.site.url, targetPage\)

self.parse\(targetPage\)

 

reuters 

= 

Website\('Reuters', 

'https://www.reuters.com', 

'^\(/article/\)', 

False, 

'h1', 'div.StandardArticleBody\_body\_1gnLA'\)

crawler = Crawler\(reuters\)

crawler.crawl\(\)

Еще одно изменение, по сравнению с предыдущими

примерами: объект Website \(в данном случае переменная

reuters\), в свою очередь, является свойством объекта

Crawler. Это позволяет удобно хранить в краулере

посещенные страницы \(visited\), но также означает

необходимость создания для каждого сайта нового краулера, вместо того чтобы многократно использовать один и тот же

для проверки списка сайтов. 

Безотносительно того, хотите ли вы, чтобы веб-краулер не

зависел от конкретных сайтов, или желаете сделать его

атрибутом объекта Crawler, это структурное решение

необходимо принимать в соответствии с конкретными

потребностями. В общем случае годится любой из данных

подходов. 

Следует также отметить: веб-краулер станет получать

ссылки с начальной страницы, но не продолжит сбор данных

после того, как все эти страницы будут пройдены. Возможно, вы захотите написать веб-краулер, работающий по одной из

схем, описанных в главе 3, который будет искать ссылки на

каждой посещенной странице. Вы даже можете пройти по всем

URL на всех страницах \(а не только на имеющих заданную

структуру\), чтобы найти URL, соответствующие заданной

структуре. 

**Сбор данных со страниц нескольких типов**

В отличие от сбора данных с заданного множества страниц

проверка всех внутренних ссылок на сайте может вызвать

проблему, поскольку вы никогда точно не знаете, что получите. 

К счастью, есть несколько основных способов определения

типа страницы. 

• *По URL* — все публикации в блогах могут содержать URL

\(например, **http://example.com/blog/title-ofpost**\). 

*• По наличию или отсутствию определенных полей* — если на

странице есть дата, но нет имени автора, то эту страницу

можно классифицировать как пресс-релиз. При наличии у

страницы заголовка, основного изображения и цены, но при

отсутствии основного контента это может быть страница

товара. 

*• *

*По *

*наличию *

*на *

*странице *

*определенных *

*тегов, *

*идентифицирующих страницу, * — теги можно использовать, даже если мы не собираем данные внутри них. Веб-краулер

может 

искать 

элемент 

наподобие

<divid="relatedproducts">, чтобы идентифицировать

страницу как страницу товара, даже если вас не интересуют

сопутствующие товары. 

Чтобы отслеживать несколько типов страниц, вам

понадобится создать на Python несколько типов объектов

страниц. Это можно сделать следующими двумя способами. 

Если страницы похожи \(имеют в целом одинаковые типы

контента\), то можно добавить к существующему объекту веб-страницы атрибут pageType:

class Website:

def \_\_init\_\_\(self, name, url, titleTag, 

bodyTag, pageType\):

self.name = name

self.url = url

self.titleTag = titleTag

self.bodyTag = bodyTag

self.pageType = pageType

Если страницы хранятся в SQL-подобной базе данных, то

такой тип структуры страниц указывает на вероятное хранение

этих страниц в одной таблице, в которую будет добавлено поле

pageType. 

Если же страницы или контент, который вы ищете, заметно

различаются \(содержат поля разных типов\), то может

понадобиться создать отдельные объекты для каждого типа

страниц. Конечно, все веб-страницы будут иметь нечто общее:

URL и, вероятно, имя или заголовок. Это идеальная ситуация

для использования подклассов:

class Webpage:

def \_\_init\_\_\(self, name, url, titleTag\):

self.name = name

self.url = url

self.titleTag = titleTag

Веб-краулер не будет использовать этот объект напрямую, однако на него будут ссылаться типы страниц:

class Product\(Website\):

"""Содержит информацию для веб-скрапинга

страницы товара""" 

def \_\_init\_\_\(self, name, url, titleTag, 

productNumberTag, priceTag\):

Website.\_\_init\_\_\(self, name, url, 

TitleTag\)

self.productNumberTag =

productNumberTag

self.priceTag = priceTag

 

class Article\(Website\):

"""Содержит информацию для веб-скрапинга

страницы статьи""" 

def \_\_init\_\_\(self, name, url, titleTag, 

bodyTag, dateTag\):

Website.\_\_init\_\_\(self, name, url, 

titleTag\)

self.bodyTag = bodyTag

self.dateTag = dateTag

Страница Product расширяет базовый класс Website, добавляя к нему атрибуты productNumber и price, относящиеся лишь к товарам, а класс Article добавляет

атрибуты body и date, которые к товарам неприменимы. 

Эти два класса можно использовать, например, для веб-скрапинга интернет-магазина, на сайте которого содержатся не

только товары, но и публикации в блоге или пресс-релизы. 

**Размышления о моделях веб-краулеров**

Собирать информацию из Интернета — словно пить из

пожарного шланга. Ее слишком много, и не всегда понятно, что

именно вам нужно и как это получить. Ответ на эти вопросы

должен стать первым шагом в любом крупном \(а иногда и

небольшом\) веб-проекте. 

При сборе одних и тех же данных в разных областях или из

разных источников ваша цель почти всегда должна состоять в

том, чтобы попытаться нормализовать эти данные. Работать с

данными, имеющими одинаковые или сопоставимые поля, гораздо проще, чем с данными, формат которых полностью

определяется их первоначальным источником. 

Во многих случаях приходится создавать веб-скраперы, исходя из вероятного появления новых источников данных в

дальнейшем и с целью минимизировать издержки на

программирование, которые потребуются для добавления этих

новых источников. Даже если на первый взгляд сайт не

соотносится с вашей моделью, возможно, есть более тонкие

способы обеспечить данное соответствие. В долгосрочной

перспективе умение замечать эти базовые закономерности

позволит вам сэкономить время и деньги, а также избавит от

множества неприятностей. 

Кроме того, не следует игнорировать взаимосвязи между

фрагментами данных. Предположим, вы ищете информацию, у

которой в разных источниках есть свойства «тип», «размер»

или «тема». Как вы будете хранить, извлекать и осмысливать

эти атрибуты? 

Архитектура ПО — обширная и важная тема, освоению

которой можно посвятить всю карьеру. К счастью, программная архитектура для веб-скрапинга — гораздо более

ограниченный и управляемый набор навыков, приобретаемых

относительно легко. Занимаясь веб-скрапингом данных, вы, скорее всего, со временем заметите одни и те же постоянно

повторяющиеся базовые закономерности. Чтобы создать

хорошо структурированный веб-скрапер, не нужно много

тайных знаний, но потребуется уделить время обдумыванию

проекта в целом. 

**Глава 5. Scrapy**

В предыдущей главе мы рассмотрели некоторые методы и

схемы построения больших, масштабируемых и \(что самое

важное\!\) удобных в сопровождении веб-краулеров. Несмотря

на то что их несложно разработать вручную, есть множество

библиотек, фреймворков и даже инструментов с графическим

интерфейсом, которые сделают это за вас или по крайней мере

постараются немного упростить вам жизнь. 

В этой главе вы познакомитесь с одной из лучших платформ

для разработки веб-краулеров — Scrapy. Когда я работала над

первым изданием данной книги, Scrapy для Python 3.x еще не

была выпущена, поэтому ее упоминание в книге ограничилось

одним разделом. С тех пор библиотека стала поддерживать

Python 3.3\+, в ней появились дополнительные функции, и я с

удовольствием уделю ей не раздел, а целую главу. 

Одна из проблем создания веб-краулеров состоит в том, что

часто приходится выполнять одни и те же задачи: находить все

ссылки на странице, оценивать разницу между внутренними и

внешними ссылками, переходить на новые страницы. Эти

основные стандартные операции полезно знать и уметь писать

с нуля, но библиотека Scrapy способна многое из упомянутого

сделать автоматически. 

Конечно, Scrapy не читает мысли. Вам по-прежнему

необходимо описать шаблоны страниц, указать точку, с

которой следует начать работу, и построить правила для URL

искомых страниц. Но и в этих случаях библиотека

предоставляет чистую основу для построения четко

структурированного кода. 

**Установка Scrapy**

Scrapy предоставляет инструмент для скачивания библиотеки с

ее сайта \(**http://scrapy.org/download/**\), а также инструкции по

установке с помощью сторонних менеджеров, таких как pip. 

Из-за сравнительно большого размера и сложности Scrapy обычно нельзя установить, как другие фреймворки, с помощью

такой команды:

$ pip install Scrapy

Обратите внимание: я говорю «обычно», поскольку

теоретически можно использовать и эту команду. Однако на

практике я, как правило, сталкиваюсь с одной или

несколькими 

сложными 

проблемами, 

связанными 

с

зависимостями, несовпадением версий и неразрешимыми

ошибками. 

Если вы решили установить Scrapy с помощью pip, то

настоятельно 

рекомендую 

использовать 

виртуальное

окружение \(подробнее о виртуальных окружениях см. во врезке

«Хранение библиотек непосредственно в виртуальных

окружениях» на с. 28\). 

Я предпочитаю другой способ установки — с помощью

менеджера 

пакетов 

Anaconda

\(**https://docs.continuum.io/anaconda/**\). Это программный продукт, производимый компанией Continuum и предназначенный для

того, чтобы сглаживать острые углы при поиске и установке

популярных пакетов Python для обработки данных. В

следующих главах мы будем использовать многие другие

пакеты, которыми управляет Anaconda, такие как NumPy и

NLTK. 

После установки Anaconda можно установить Scrapy с

помощью следующей команды:

conda install -c conda-forge scrapy Если у вас возникнут проблемы или потребуется свежая

информация, то обратитесь к руководству по установке Scrapy \(**https://doc.scrapy.org/en/latest/intro/install.html**\). 


