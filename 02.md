*Регулярные выражения* получили свое название благодаря

тому, что используются для идентификации регулярных строк; регулярное выражение позволяет сделать однозначный вывод

о том, что данная строка соответствует определенным

правилам, и вернуть ее; или же сделать вывод о том, что строка

не соответствует правилам, и отбросить ее. Это невероятно

удобно для быстрой проверки больших документов, в которых

нужно найти номера телефонов или адреса электронной

почты. 

Обратите внимание: я использовала выражение *«регулярная*

*строка»*. Что это такое? Это любая строка, которую можно

построить с учетом последовательности линейных правил3

следующего вида. 

1. Написать хотя бы одну букву *a*. 

2. Добавить ровно пять букв *b*. 

3. Добавить произвольное четное число букв *c*. 

4. В конце поставить букву *d* или *e*. 

Этим правилам соответствуют строки *aaaabbbbbccccd*, *aabbbbbcce* и т.д. \(количество вариантов бесконечно\). 

Регулярные выражения — всего лишь краткий способ

представления этих наборов правил. Например, регулярное

выражение для описанного выше набора правил выглядит так: aa\*bbbbb\(cc\)\*\(d|e\)

На первый взгляд эта строка смотрится жутковато, но она

станет понятнее, если разбить ее на составляющие:

•aa\* — буква a, после которой стоит символ \* \(звездочка\), означает «любое количество букв a, включая 0». Такая

запись гарантирует, что буква a будет написана хотя бы

один раз; 

• bbbbb — ничего особенного, просто пять букв b * * подряд; 

• \(cc\)\* — любое количество чего угодно можно заключить в

скобки. Поэтому для реализации правила о четном

![Image 26](images/000002.png)

количестве букв c мы можем написать две буквы c, заключить их в скобки и поставить после них звездочку. Это

значит, что в строке может присутствовать любое

количество пар, состоящих из букв c \(обратите внимание, что это также может означать 0 пар\); 

• \(d|e\) — вертикальная линия между двумя выражениями

означает «то или это». В данном случае мы говорим

«добавить d или e». Таким образом мы гарантируем, что в

строку добавится ровно один из этих двух символов. 

**Эксперименты с регулярными выражениями**

Осваивая регулярные выражения, очень важно поиграть с ними

и понять, как они работают. Если вам не хочется ради пары

строк открывать редактор кода и запускать программу, то

проверить, работает ли регулярное выражение должным

образом, можно на одном из специальных сайтов, например

Regex Pal \(http://regexpal.com/\). 

В табл. 2.1 приводятся наиболее часто используемые

символы регулярных выражений с краткими пояснениями и

примерами. Этот список ни в коем случае не претендует на

полноту. Кроме того, как уже упоминалось, вы можете

столкнуться с незначительными различиями, в зависимости от

языка. Однако приведенные здесь 12 символов наиболее часто

встречаются в регулярных выражениях Python и могут служить

для поиска и сбора практически любых строк. 

**Таблица 2.1. **Часто используемые символы регулярных выражений

**Символ**

**\(-ы\)**

**Значение**

**Пример**

**Соответствующие**

**строки**

\*

Предыдущий символ, подвыражение или символ

в скобках повторяется ноль или более раз

a\*b\*

aaaaaaaa, 

aaabbbbb, bbbbbb

Предыдущий символ, подвыражение или символ

aaaaaaaab, 

\+

в скобках повторяется один раз или более

a\+b\+

aaabbbbb, 

abbbbbb

\[\]

Любой символ в скобках \(может читаться как

«выберите любое количество этих предметов»\) \[A–Z\]\*

APPLE, CAPITALS, 

QWERTY

Сгруппированное подвыражение \(в «порядке

\(\)

операций» над регулярными выражениями

\(a\*b\)\*

aaabaab, abaaab, 

выполняется в первую очередь\)

ababaaaaab

Предыдущий символ, подвыражение или символ

\{m, n\}

в скобках повторяется от m до n раз

a\{2,3\}b\{2,3\} aabbb, aaabbb, 

\(включительно\)

aabb

\[^\]

Любой одиночный символ, которого нет в

скобках

\[^A–Z\]\*

apple, lowercase, 

qwerty

Любой символ, строка символов или

подвыражение из тех, что разделены знаком |

|

\(обратите внимание: это не заглавная буква i, а b\(a|i|e\)d bad, bid, bed

вертикальная черта, также называемая прямым

слешем или символом конвейеризации\)

. 

Любой одиночный символ \(буква, цифра, пробел

и т.д.\)

b.d

bad, bzd, b$d, b d

^

Указывает на то, что символ или подвыражение

должны находиться в начале строки

^a

apple, asdf, a

Экранирующий символ \(позволяет использовать

\\

специальные символы в их буквальном

\\^ \\| \\\\

^ | \\

значении\)

$

Часто ставится в конце регулярного выражения и \[A–Z\]\*

ABCabc, zzzyx, Bob

означает «до конца строки». Без этого в конце

\[a–z\]\*$

любого регулярного выражения де-факто стоит

«.\*», что позволяет принимать строки, в которых

совпадает только первая часть. Данный знак

можно считать аналогом символа ^

«Не содержит». Эта странная пара символов, непосредственно предшествующая символу \(или

регулярному выражению\), указывает на то, что

данный символ не должен присутствовать в этом ^\(\(?\! 

?\! 

месте строки. Иногда его сложно использовать; в

No-caps-here, 

конце концов, символ может находиться в

\[A–Z\]\).\)\*$ $ymb0ls a4e f\!ne

другой части строки. Если вы хотите совсем

исключить данный символ, то задействуйте это

выражение в сочетании с ^ и $ на обоих концах

строки

Один из классических примеров регулярных выражений —

распознавание адресов электронной почты. Точные правила

составления адресов электронной почты слегка различаются на

разных почтовых серверах, однако можно составить несколько

общих правил. Регулярное выражение для каждого из этих

правил показано во втором столбце табл. 2.2. 

**Таблица 2.2. ** Точные правила составления адресов электронной

почты и регулярные выражения для каждого из них

**Правило 1**

**\[A–Za–z0–9.\_\+\]\+**

Первая часть адреса

электронной почты

Регулярные выражения очень красиво сокращаются. 

обязательно содержит хотя Например, A–Z означает любую заглавную букву от A до Z. 

бы один элемент из

Поместив все возможные последовательности и символы в

следующего списка:

квадратные скобки \(не путать с круглыми\), мы как бы

заглавные буквы, строчные говорим: «Символ может удовлетворять любому из условий, буквы, цифры от 0 до 9, 

перечисленных в скобках». Обратите также внимание: знак

точки \(.\), знаки «плюс» \(\+\)

\+ означает, что символы могут встречаться произвольное

или подчеркивания \(\_\)

количество раз, но не менее одного

**Правило 2**

**@**

После этого в адресе

электронной почты должен Все очень просто: в середине адреса должен стоять символ

стоять символ @

@ и встречаться ровно один раз

**Правило 3**

**\[A–Za–z\]\+**

Адрес электронной почты

должен содержать хотя бы В первой части имени домена, после символа @, допустимы

одну букву, заглавную или только буквы, и их должно быть не менее одной

строчную

![Image 27](images/000031.png)

**Правило 4**

**. **

Потом идет точка \(.\)

Перед доменом верхнего уровня должна стоять точка \(.\) **Правило 5**

**\(com|org|edu|net\)**

Наконец, адрес

электронной почты

заканчивается на com, org, 

edu или net \(в

действительности

Здесь перечислены возможные последовательности букв, существует еще много

которые могут стоять после точки во второй части адреса

доменов верхнего уровня, электронной почты

но для нашего примера этих

четырех должно быть

достаточно\)

Объединив все эти правила, получим следующее регулярное

выражение:

\[A-Za-z0-9.\_\+\]\+@\[A-Za-z\]\+.\(com|org|edu|net\)

При попытке написать регулярное выражение с нуля лучше

сначала составить список шагов, которые бы четко описывали, какой должна быть ваша строка. Обратите внимание на

граничные случаи. Например, если вы описываете номера

телефонов, то учитываете ли коды стран и добавочные номера? 

**Регулярные — не значит неизменные\! **

Стандартная версия регулярных выражений \(описанная в

данной книге и используемая в Python и BeautifulSoup\) основана на синтаксисе Perl. Этот или похожий синтаксис

применяется 

в 

большинстве 

современных 

языков

программирования. Однако следует помнить, что при

использовании регулярных выражений на другом языке вы

можете столкнуться с проблемами. 

Даже у некоторых современных языков, таких как Java, есть

незначительные различия в способе обработки регулярных

выражений. Если сомневаетесь, то читайте документацию\! 

**Регулярные выражения и BeautifulSoup**

Если предыдущий раздел, посвященный регулярным

выражениям, показался вам несколько оторванным от темы

этой книги, то теперь мы восстановим связь. В отношении веб-скрапинга BeautifulSoup и регулярные выражения идут рука об

руку. В сущности, функция, принимающая строку в качестве

аргумента \(например, find\(id="идентификаторТега"\)\), скорее всего, будет принимать и регулярное выражение. 

Рассмотрим несколько примеров, проверив страницу по

адресу **http://www.python scraping.com/pages/page3.html**. 

Обратите внимание: на этом сайте есть много изображений

товаров, представленных в таком виде:

<img src="../img/gifts/img3.jpg"> Если мы хотим собрать URL всех изображений товаров, то

на первый взгляд решение может показаться довольно

простым: достаточно выбрать все теги изображений с

помощью функции .find\_all\("img"\), верно? Не совсем. 

Кроме очевидных «лишних» изображений \(например, логотипов\), на современных сайтах часто встречаются скрытые

и пустые изображения, используемые вместо пробелов и для

выравнивания элементов, а также другие случайные теги

изображений, о которых вы, возможно, не знаете. Определенно

нельзя рассчитывать на то, что все изображения на странице

являются только изображениями товаров. 

Предположим также, что макет страницы может изменяться

или по какой-либо причине поиск правильных тегов не должен

зависеть от *расположения* изображений на странице. Например, вы хотите собрать определенные элементы или фрагменты

данных, разбросанные по всему сайту случайным образом. Так, для 

титульного 

изображения 

товара 

может 

быть

предусмотрено специальное место наверху некоторых — но не

всех — страниц. 

Решение состоит в поиске чего-то идентифицирующего сам

тег. В данном случае можно поискать путь к файлам

изображений товаров:

from urllib.request import urlopen

from bs4 import BeautifulSoup

import re

 

html 

=

urlopen\('http://www.pythonscraping.com/pages/pa

ge3.html'\)

bs = BeautifulSoup\(html, 'html.parser'\)

images = bs.find\_all\('img', 

\{'src':re.compile\('..\\/img\\/gifts/img.\*.jpg

'\)\}\)

for image in images:

print\(image\['src'\]\)

Этот код выводит только те относительные пути к

изображениям, которые начинаются с ../img/gifts/img и

заканчиваются на .jpg:

../img/gifts/img1.jpg

../img/gifts/img2.jpg

../img/gifts/img3.jpg

../img/gifts/img4.jpg

../img/gifts/img6.jpg

Регулярное выражение может использоваться как аргумент

в выражении, написанном на BeautifulSoup, что обеспечивает

большую гибкость при поиске нужных элементов. 

**Доступ к атрибутам**

До сих пор мы исследовали способы доступа к тегам и их

контенту и способы их фильтрации. Однако часто при веб-скрапинге нас интересует не содержимое тега, а его атрибуты. 

Это особенно полезно для таких тегов, как a, в атрибуте href содержащих URL, на которые ссылаются эти теги, или же тегов

img, в атрибуте src содержащих ссылки на целевые

изображения. 

Python позволяет автоматически получить список

атрибутов для объекта тега, вызвав следующую функцию: myTag.attrs

Помните: эта функция возвращает в чистом виде словарь

Python, благодаря чему получение атрибутов и управление ими

становится тривиальной задачей. Например, для того, чтобы

узнать, где находится файл с изображением, можно

воспользоваться следующим кодом:

myImgTag.attrs\['src'\]

**Лямбда-выражения**

Если вы окончили вуз по специальности, связанной с

информатикой или вычислительной техникой, то, скорее всего, проходили лямбда-выражения. Это было давно, один раз, и

потом вы их больше никогда не использовали. Если же нет, то

лямбда-выражения могут быть вам незнакомы \(или же вы о

них где-то слышали и когда-то даже собирались изучить\). В

данном разделе мы не станем слишком углубляться в эти типы

функций, я только покажу, как можно применять их в веб-скрапинге. 

По сути, *лямбда-выражение* — это функция, которая

передается в другую функцию как переменная; вместо того

чтобы определять функцию как *f*\( *x*, *y*\), мы можем определить ее

как *f*\( *g*\( *x*\), *y*\) или даже как *f*\( *g*\( *x*\), *h*\( *x*\)\). 

BeautifulSoup позволяет передавать в функцию find\_all функции определенных типов в качестве параметров. 

Единственное ограничение состоит в том, что эти функции

должны принимать в качестве аргумента объект тега и

возвращать логическое значение. В этой функции

BeautifulSoup оценивает каждый переданный ей объект тега; теги, имеющие значение True, возвращаются, а остальные

отбрасываются. 

Например, следующая функция возвращает все теги, имеющие ровно два атрибута:

bs.find\_all\(lambda tag: len\(tag.attrs\) == 2\)

Здесь в качестве аргумента передается функция

len\(tag.attrs\)==2. Когда она равна True, функция

find\_all станет возвращать соответствующий тег. Другими

словами, будут найдены все теги с двумя атрибутами, например следующие:

<div class="body" id="content"></div> 

<span style="color:red" class="title"></span> Лямбда-функции настолько полезны, что ими даже можно

заменять существующие функции BeautifulSoup:

bs.find\_all\(lambda tag: tag.get\_text\(\) ==

'Or maybe he\\'s only resting?'\)

То же самое можно выполнить и без лямбда-функции: bs.find\_all\('', text='Or maybe he\\'s only

resting?'\)

Однако если вы помните синтаксис лямбда-функции и

знаете, как получить доступ к свойствам тега, то вам, возможно, больше никогда не понадобится вспоминать

остальной синтаксис BeautifulSoup\! 

Поскольку лямбда-функция может быть любой функцией, которая возвращает значение True или False, эти функции

даже можно комбинировать с регулярными выражениями, чтобы 

найти 

теги 

с 

атрибутом, 

соответствующим

определенному строковому шаблону. 

2 Для получения списка всех тегов h<уровень>, имеющихся в документе, существуют более лаконичные варианты кода. Есть и другие способы решения таких

задач, которые мы рассмотрим в разделе, посвященном регулярным выражениям. 

3 Вы спросите: «А существуют ли “нерегулярные” выражения?» Да, существуют, но

выходят за рамки этой книги. Нерегулярные выражения описывают такие строки, как

«написать простое число букв a, а после них — ровно вдвое большее число букв b» или

«написать палиндром». Строки этого типа невозможно описать с помощью

регулярных выражений. К счастью, мне никогда не встречались ситуации, в которых

веб-скрапер должен был бы находить подобные строки. 

**Глава 3. Разработка веб-краулеров**

До сих пор нам встречались лишь отдельные статические

страницы с несколько искусственно законсервированными

примерами. В этой главе мы познакомимся с реальными

задачами, для решения которых веб-скраперы перебирают

несколько страниц и даже сайтов. 

*Веб-краулеры* называются так потому, что они «ползают»

\(crawl\) по Всемирной паутине и собирают данные с веб-страниц. В основе их работы лежит рекурсия. Веб-краулер

получает контент страницы по ее URL, исследует эту страницу, находит URL другой страницы, извлекает содержимое *этой*

*другой* страницы и далее до бесконечности. 

Однако будьте осторожны: возможность собирать данные в

Интернете еще не означает, что вы всегда должны это делать. 

Веб-скраперы, показанные в предыдущих примерах, отлично

работают в ситуациях, когда все необходимые данные

находятся на одной странице. Используя веб-краулеры, следует

быть предельно внимательными к тому, какую часть

пропускной способности сети вы задействуете, и всеми силами

постараться определить, есть ли способ облегчить нагрузку на

интересующий вас сервер. 

**Проход отдельного домена**

Даже если вам еще не приходилось слышать об игре «Шесть

шагов по “Википедии”», вы наверняка знаете о ее

предшественнице — «Шесть шагов до Кевина Бейкона». В обеих

играх цель состоит в том, чтобы установить взаимосвязь между

двумя мало связанными элементами \(в первом случае — между

статьями «Википедии», а во втором — между актерами, 

сыгравшими в одном фильме\) и построить цепь, содержащую

не более шести звеньев \(включая начальный и конечный

элементы\). 

Например, Эрик Айдл \(Eric Idle\) снялся в фильме «Дадли

Справедливый» \(Dudley Do-Right\) с Бренданом Фрейзером

\(Brendan Fraser\), который, в свою очередь, снялся с Кевином

Бейконом \(Kevin Bacon\) в «Воздухе, которым я дышу» \(The Air I Breathe\)4.  В этом случае цепь от Эрика Айдла до Кевина

Бейкона состоит всего из трех элементов. 

В этом разделе мы начнем разработку проекта, который

позволит строить цепи для «Шести шагов по “Википедии”»: например, 

начав 

со 

страницы 

Эрика 

Айдла

\(**https://en.wikipedia.org/wiki/Eric\_Idle**\), вы сможете найти

наименьшее количество переходов по ссылкам, которые

приведут 

вас 

на 

страницу 

Кевина 

Бейкона

\(**https://en.wikipedia.org/wiki/Kevin\_Bacon**\). 

**А как насчет нагрузки на сервер «Википедии»? **

По данным Фонда Викимедиа \(вышестоящей организации, отвечающей в том числе за «Википедию»\), за одну секунду

происходит примерно 2500 обращений к веб-ресурсам сайта, причем более 99 % из них относятся к «Википедии» \(см. 

раздел Traffic volume на странице Wikimedia in figures, **https://meta.wikimedia.org/wiki/Wikimedia\_in\_figures\_-\_Wikipedia\#Traffic\_volume**\). Из-за большого объема трафика

ваши веб-скраперы вряд ли сколько-нибудь заметно

повлияют на нагрузку сервера «Википедии». Тем не менее, если вы намерены активно использовать примеры кода, приведенные в этой книге, или будете разрабатывать

собственные проекты для веб-скрапинга «Википедии», я

призываю 

вас 

совершить 

необлагаемое 

налогом

пожертвование 

в 

Фонд 

Викимедиа

\(**https://wikimediafoundation.org/wiki/Ways\_to\_Give**\) — не

только в качестве компенсации нагрузки на сервер, но и

чтобы помочь сделать образовательные ресурсы более

доступными для других пользователей. 

Кроме того, имейте в виду: если вы планируете создать

большой проект с применением данных из «Википедии», то

стоит убедиться, что эти данные еще неоступны через API

«Википедии»

«\(**https://www.mediawiki.org/wiki/API:Main\_page**\). 

«Википедия» часто используется в качестве сайта для

демонстрации веб-скраперов и веб-краулеров, поскольку

данный сайт имеет простую структуру HTML-кода и

относительно стабилен. Однако эти же данные могут

оказаться более доступными через API. 

Вы уже, вероятно, знаете, как написать скрипт на Python, который бы получал произвольную страницу «Википедии» и

создавал список ссылок, присутствующих на этой странице: from urllib.request import urlopen

from bs4 import BeautifulSoup

 

html 

=

urlopen\('http://en.wikipedia.org/wiki/Kevin\_Bac

on'\)

bs = BeautifulSoup\(html, 'html.parser'\)

for link in bs.find\_all\('a'\):

if 'href' in link.attrs:

print\(link.attrs\['href'\]\)

Если вы посмотрите на созданный список ссылок, то

заметите, что в него вошли все ожидаемые статьи: Apollo 13, Philadelphia, Primetime Emmy Award и т.д. Однако есть и кое-что лишнее:

//wikimediafoundation.org/wiki/Privacy\_policy

//en.wikipedia.org/wiki/Wikipedia:Contact\_us

Дело в том, что на каждой странице «Википедии» есть

боковая панель, нижний и верхний колонтитулы с множеством

ссылок; также есть ссылки на страницы категорий, обсуждений

и другие страницы, которые не содержат полезных статей:

/wiki/Category:Articles\_with\_unsourced\_statemen

ts\_from\_April\_2014

/wiki/Talk:Kevin\_Bacon

Недавно мой друг, работая над аналогичным проектом по

веб-скрапингу «Википедии», заявил, что написал большую

функцию фильтрации, насчитывающую более 100 строк кода, с

целью определить, является ли внутренняя ссылка

«Википедии» ссылкой на страницу статьи. К сожалению, он не

уделил достаточно времени поиску закономерностей между

«ссылками на статьи» и «другими ссылками», иначе бы нашел

более красивое решение. Если вы внимательно посмотрите на

ссылки, которые ведут на страницы статей \(в отличие от других

внутренних страниц\), то заметите, что у всех таких ссылок есть

три общие черты:

• они находятся внутри тега div, у которого атрибут id имеет

значение bodyContent; 

• в их URL нет двоеточий; 

• их URL начинаются с /wiki/. 

Мы можем немного изменить код, включив в него эти

правила, и получить только нужные ссылки на статьи, воспользовавшись регулярным выражением ^\(/wiki/\) \(\(?\!:\).\)\*$"\):

from urllib.request import urlopen

from bs4 import BeautifulSoup

import re

 

html 

=

urlopen\('http://en.wikipedia.org/wiki/Kevin\_Bac

on'\)

bs = BeautifulSoup\(html, 'html.parser'\)

for 

link 

in 

bs.find\('div', 

\{'id':'bodyContent'\}\).find\_all\(

 

 

 

 

'a', 

href=re.compile\('^\(/wiki/\)

\(\(?\!:\).\)\*$'\)\):

if 'href' in link.attrs:

print\(link.attrs\['href'\]\)

Запустив этот код, мы получим список всех URL статей, на

которые ссылается статья «Википедии» о Кевине Бейконе. 

Скрипт, который находит все ссылки на статьи для одной

жестко заданной статьи «Википедии», конечно, интересен, однако с практической точки зрения довольно бесполезен. Вы

должны уметь, взяв этот код за основу, преобразовать его

примерно в такую программу. 

• Отдельная функция getLinks принимает URL статьи

«Википедии» в формате /wiki/<Название\_статьи> и

возвращает список всех URL, на которые ссылается эта

статья, в том же формате. 

• Основная функция, которая вызывает getLinks с

первоначальной статьей, выбирает из возвращенного

списка случайную ссылку и снова вызывает getLinks, пока

пользователь не остановит программу или пока не окажется, что на очередной странице нет ссылок. 

Вот полный код программы, которая это делает:

from urllib.request import urlopen

from bs4 import BeautifulSoup

import datetime

import random

import re

 

random.seed\(datetime.datetime.now\(\)\)

def getLinks\(articleUrl\):

 

 

 

 

html 

=

urlopen\('http://en.wikipedia.org\{\}'.format\(arti

cleUrl\)\)

bs = BeautifulSoup\(html, 'html.parser'\)

 

 

 

 

return 

bs.find\('div', 

\{'id':'bodyContent'\}\).find\_all\('a', 

href=re.compile\('^\(/wiki/\)

\(\(?\!:\).\)\*$'\)\)



links = getLinks\('/wiki/Kevin\_Bacon'\)

while len\(links\) > 0:

newArticle = links\[random.randint\(0, 

len\(links\)-1\)\].attrs\['href'\]

print\(newArticle\)

links = getLinks\(newArticle\)

Сразу после импорта необходимых библиотек программа

присваивает генератору случайных чисел начальное значение, равное текущему системному времени, чем обеспечивает

новый и интересный случайный путь по статьям «Википедии»

практически при каждом запуске программы. 

**Псевдослучайные числа и случайные начальные значения**

В предыдущем примере был задействован генератор

случайных чисел Python для случайного выбора статьи на

странице, по ссылке на которую будет продолжен обход

«Википедии». Однако случайные числа следует использовать

осторожно. 

Компьютеры хорошо справляются с вычислением правильных

решений, но откровенно слабы, когда нужно придумать что-то новое. По этой причине случайные числа могут стать

проблемой. Большинство алгоритмов генерации этих чисел

стремятся 

создать 

равномерно 

распределенную 

и

труднопредсказуемую числовую последовательность, однако

для запуска работы такого алгоритма необходимо

«начальное» число. Если оно каждый раз будет одним и тем

же, то алгоритм станет всякий раз генерировать одну и ту же

последовательность «случайных» чисел. Именно поэтому я

использовала значение системных часов в качестве

начального 

значения 

для 

создания 

новых

последовательностей случайных чисел и, следовательно, генерации последовательностей случайных статей. Благодаря

этому выполнять программу будет немного интереснее. 

Любопытно, что генератор псевдослучайных чисел Python работает по *алгоритму Мерсенна Твистера* \(Mersenne Twister\). 

Он генерирует труднопредсказуемые и равномерно

распределенные случайные числа, однако несколько

загружает процессор. Случайные числа — это хорошо, но за

все приходится платить\! 

Затем программа определяет функцию getLinks, которая

принимает URL статьи в формате /wiki/..., добавляет в

начало имя домена «Википедии» http://en.wikipedia.org и получает объект BeautifulSoup с HTML-кодом, который

находится по этому адресу. Затем функция извлекает список

тегов со ссылками на статьи, исходя из описанных ранее

параметров, и возвращает их. 

В основной части программы сначала создается список

тегов со ссылками на статьи \(переменная links\), в котором

содержатся ссылки, найденные на начальной странице

**https://en.wikipedia.org/wiki/Kevin\_Bacon**. 

Затем 

программа

выполняет цикл и находит тег со случайной ссылкой на статью, извлекает оттуда атрибут href, выводит страницу и получает

по извлеченному URL новый список ссылок. 

Разумеется, решение задачи «Шесть шагов по “Википедии”»

не ограничивается созданием веб-скрапера, переходящего с

![Image 28](images/000063.png)

одной страницы на другую. Нам также необходимо хранить и

анализировать полученные данные. Продолжение решения

этой задачи вы найдете в главе 6. 

**Обрабатывайте исключения\! **

По большей части обработка исключений в этих примерах

пропущена для краткости, однако следует помнить: здесь может

возникнуть множество потенциальных ловушек. Что, если, например, «Википедия» изменит имя тега bodyContent? Тогда

при попытке извлечь текст из тега программа выдаст

исключение AttributeError. 

Таким образом, несмотря на то, что эти сценарии достаточно

хороши в качестве тщательно контролируемых примеров, в

автономном коде готового приложения требуется обрабатывать

исключения гораздо лучше, чем позволяет описать объем

данной книги. Для получения дополнительной информации об

этом вернитесь к главе 1. 

**Сбор информации со всего сайта**

В предыдущем разделе мы прошли по сайту, переходя от одной

случайно выбранной ссылки к другой. Но как быть, если нужно

систематизировать сайт и составить каталог или просмотреть

все страницы? Сбор информации со всего сайта, особенно

большого, — процесс, требующий интенсивного использования

памяти. Лучше всего с этим справляются приложения, имеющие быстрый доступ к базе данных для хранения

результатов краулинга. Однако мы можем исследовать

поведение таких приложений, не выполняя их в полном

объеме. Подробнее об их выполнении с применением базы

данных см. в главе 6. 

**Темный и глубокий Интернет**

Вероятно, вам часто приходилось слышать о *глубоком*, *темном* или *скрытом Интер нете*, особенно в последних

новостях. Что имеется в виду? 

*Глубокий Интернет* — это любая часть Сети, выходящая за

пределы *видимого Интерне та*. Видимой называют ту часть

Интернета, которая индексируется поисковыми системами. 

Несмотря на большой разброс оценок, глубокий Интернет

почти наверняка составляет около 90 % всего Интернета. 

Поскольку Google не способен отправлять формы или

находить страницы, на которые не ссылается домен верхнего

уровня, а также не выполняет поиск сайтов, для которых это

запрещено в файле robots.txt, видимый Интернет

продолжает составлять относительно небольшую часть Сети. 

*Темный Интернет*, также известный как *Даркнет*, — нечто

совершенно иное. Он работает на той же сетевой аппаратной

инфраструктуре, но использует браузер Tor или другой

клиент, у которого протокол приложения работает поверх

HTTP, обеспечивая безопасный канал для обмена

информацией. В темном Интернете, как и на обычном сайте, 

тоже можно выполнять веб-скрапинг, однако эта тема

выходит за пределы данной книги. 

В отличие от темного, в глубоком Интернете веб-скрапинг

выполняется относительно легко. Многие инструменты, описанные в этой книге, научат вас, как выполнять веб-скрапинг и сбор информации во многих местах, недоступных

для ботов Google. 

В каких случаях сбор данных со всего сайта полезен, а в

каких — вреден? Веб-скраперы, которые перебирают весь сайт, хороши во многих случаях, включая следующие. 

• *Формирование карты сайта. * Несколько лет назад мне

встретилась задача: важный клиент хотел оценить затраты

на редизайн сайта, однако не хотел предоставлять моей

компании 

доступ 

ко 

внутренним 

компонентам

существующей системы управления контентом и у него не

было общедоступной карты сайта. Я воспользовалась веб-краулером, чтобы пройти по всему сайту, собрать все

внутренние ссылки и разместить страницы в структуре

папок, соответствующей той, что применялась на сайте. Это

позволило мне быстро обнаружить разделы сайта, о которых

я даже не подозревала, и точно подсчитать, сколько эскизов

страниц потребуется создать и какой объем контента

необходимо перенести. 

• *Сбор данных. * Другой клиент хотел собрать статьи \(истории, посты в блогах, новости и т.п.\), чтобы построить рабочий

прототип специализированной поисковой платформы. Это

исследование сайтов должно было быть не всеобъемлющим, однако достаточно обширным \(нам хотелось получать

данные лишь с нескольких сайтов\). Мне удалось создать веб-краулеры, которые рекурсивно обходили каждый сайт и

собирали данные только со страниц статей. 

Общий подход к полному сбору данных с сайта заключается

в том, чтобы начать со страницы верхнего уровня \(например, с

начальной страницы\) и построить список всех ее внутренних

ссылок. Затем обойти все страницы, на которые указывают эти

ссылки, и на каждой из них собрать дополнительные списки

ссылок, запускающие очередной этап сбора данных. 

Конечно же, в такой ситуации количество ссылок

стремительно растет. Если на каждой странице есть десять

внутренних ссылок, а глубина сайта составляет пять страниц

\(что довольно типично для сайта среднего размера\), то

необходимо проверить 105, то есть 100 000 страниц, чтобы с

уверенностью утверждать: сайт пройден полностью. Как ни

странно, хоть и «пять страниц в глубину и десять внутренних

ссылок на каждой странице» — довольно типичный размер

сайта, очень немногие сайты действительно насчитывают 100

000 и более страниц. Причина, конечно, состоит в том, что

подавляющее большинство внутренних ссылок являются

дубликатами. 

Во избежание повторного сбора данных с одной и той же

страницы крайне важно, чтобы все обнаруженные внутренние

ссылки имели согласованный формат и сохранялись в общем

множестве, что облегчило бы поиск во время работы

программы. *Множество* похоже на список, но его элементы не

располагаются в определенной последовательности. Кроме

того, в множестве содержатся исключительно уникальные

элементы, что идеально подходит для наших целей. Следует

проверять лишь те ссылки, которые являются «новыми», и

искать дополнительные ссылки только по ним:

from urllib.request import urlopen from bs4 import BeautifulSoup

import re

 

pages = set\(\)

def getLinks\(pageUrl\):

global pages

 

 

 

 

html 

=

urlopen\('http://en.wikipedia.org\{\}'.format\(page

Url\)\)

bs = BeautifulSoup\(html, 'html.parser'\)

 

 

 

 

for 

link 

in 

bs.find\_all\('a', 

href=re.compile\('^\(/wiki/\)'\)\):

if 'href' in link.attrs:

if link.attrs\['href'\] not in pages:

\# мы нашли новую страницу

newPage = link.attrs\['href'\]

print\(newPage\)

pages.add\(newPage\)

getLinks\(newPage\)

getLinks\(''\)

Для демонстрации полной работы этого веб-краулера я

ослабила требования к тому, как должны выглядеть

внутренние ссылки \(из предыдущих примеров\). Веб-скрапер не

ограничивается только страницами статей, а ищет все ссылки, начинающиеся с /wiki/, независимо от того, где они

располагаются на странице и содержат ли двоеточия. 

Напомню: в URL страниц статей отсутствуют двоеточия, в

отличие от адресов страниц загрузки файлов, обсу ждений и т.п. 

Изначально функция getLinks вызывается с пустым URL, то есть для начальной страницы «Википедии»: к пустому URL

![Image 29](images/000018.png)

внутри функции добавляется http://en.wikipedia.org. 

Затем функция просматривает каждую ссылку на первой

странице и проверяет, есть ли она в *глобальном множестве*

страниц \(множестве страниц, с которыми скрипт уже

встречался\). Если нет, то ссылка добавляется в список, выводится на экран и функция getLinks вызывается для нее

рекурсивно. 

**Осторожно с рекурсией**

Это предупреждение редко встречается в книгах по

программированию, но я считаю, что вы должны знать: если

оставить программу работать достаточно долго, то предыдущая

программа почти наверняка завершится аварийно. 

В Python установлен по умолчанию предел рекурсии

\(количество рекурсивных вызовов программы\), и он равен

1000. Поскольку сеть ссылок «Википедии» чрезвычайно

обширна, данная программа в итоге достигнет предела

рекурсии и остановится, если только не добавить счетчик

рекурсии или что-то еще, призванное помешать этому. 

Для «плоских» сайтов глубиной менее 1000 ссылок данный

метод обычно — за редким исключением — работает хорошо. 

Например, однажды мне встретилась ошибка в динамически

генерируемом URL, поскольку ссылка на следующую страницу

зависела от адреса текущей страницы. Это привело к

![Image 30](images/000046.png)

![Image 31](images/000077.png)

бесконечно 

повторяющимся 

путям, 

таким 

как

/blogs/blogs.../blogs/blog-post.php. 

Однако, как правило, этот рекурсивный метод должен

подходить для любого обычного сайта, с которым вы, скорее

всего, столкнетесь. 

**Сбор данных со всего сайта. ** Веб-краулеры были бы

довольно скучными программами, если бы только переходили

с одной страницы на другую. Полезными они могут быть, что-то делая на странице, на которую перешли. Рассмотрим

пример веб-скрапера, который бы выбирал заголовок, первый

абзац контента страницы и ссылку на режим редактирования

страницы \(если таковая существует\). 

Как всегда, чтобы выбрать лучший способ сделать это, сначала нужно просмотреть несколько страниц сайта и

определить его структуру. При просмотре нескольких страниц

«Википедии» \(как статей, так и страниц, не являющихся

статьями, 

например 

страницы 

с 

политикой

конфиденциальности\) становится ясно следующее. 

• У любой страницы \(независимо от статуса: статья, история

редактирования или любая другая страница\) есть заголовок, заключенный в теги h1 span, и это единственный тег h1 на

странице. 

• Как уже говорилось, весь основной текст находится внутри

тега div\#bodyCon tent. Но если вы хотите выделить

конкретную часть текста — например, получить доступ

только к первому абзацу, — то лучше использовать div\#mw-content-text p \(выбрать только тег первого абзаца\). Это

подходит для всех контентных страниц, кроме страниц

файлов 

\(например, 

![Image 32](images/000025.png)

![Image 33](images/000057.png)

**https://en.wikipedia.org/wiki/File:Orbit\_of\_274301\_Wikipedia.svg**\)

, на которых нет разделов основного текста. 

• Ссылки для редактирования существуют только на страницах

статей. Если такая ссылка есть, то она находится в теге

li\#ca-edit, в li\#ca-edit span a. 

Изменив исходный код нашего веб-краулера, мы можем

создать комбинированную программу для краулинга и сбора

данных \(или по крайней мере для вывода данных на экран\): from urllib.request import urlopen

from bs4 import BeautifulSoup

import re

 

pages = set\(\)

def getLinks\(pageUrl\):

global pages

 

 

 

 

html 

=

urlopen\('http://en.wikipedia.org\{\}'.format\(page

Url\)\)

bs = BeautifulSoup\(html, 'html.parser'\)

try:

print\(bs.h1.get\_text\(\)\)

print\(bs.find\(id ='mw-content-

text'\).find\_all\('p'\)\[0\]\)

 

 

 

 

 

 

 

 

print\(bs.find\(id='ca-

edit'\).find\('span'\)

.find\('a'\).attrs\['href'\]\)

except AttributeError:

print\('This page is missing something\! 

Continuing.'\)

![Image 34](images/000005.png)

 

 

 

 

 

for 

link 

in 

bs.find\_all\('a', 

href=re.compile\('^\(/wiki/\)'\)\):

if 'href' in link.attrs:

if link.attrs\['href'\] not in pages:

\# мы нашли новую страницу

newPage = link.attrs\['href'\]

print\('-'\*20\)

print\(newPage\)

pages.add\(newPage\)

getLinks\(newPage\)

getLinks\(''\)

Цикл for в этой программе, по сути, такой же, как и в

первоначальной программе сбора данных \(для ясности

добавлен вывод дефисов в качестве разделителей контента\). 

Поскольку никогда нельзя быть полностью уверенными в

том, что на каждой странице будут присутствовать все нужные

данные, операторы print расположены в соответствии с

вероятностью наличия этих данных на странице. Так, тег

заголовка h1 есть на каждой странице \(во всяком случае, насколько я могу судить\), вследствие чего мы сначала

пытаемся получить эти данные. Текстовый контент

присутствует на большинстве страниц \(за исключением

страниц файлов\), так что этот фрагмент данных извлекается

вторым. Кнопка редактирования есть только на страницах с

заголовками и текстовым контентом, и то не на всех. 

**Разные схемы для различных потребностей**

Очевидно, что, когда в блоке обработчика исключений

заключено нескольких строк, возникает некая опасность. 

Прежде всего, вы не можете сказать, какая именно строка

вызвала исключение. Кроме того, если по какой-либо причине

на странице есть кнопка редактирования, но нет заголовка, то

кнопка никогда не будет зарегистрирована. Однако во многих

случаях 

при 

наличии 

определенной 

вероятности

возникновения на сайте каждого из элементов этого достаточно

и непредвиденное отсутствие нескольких точек данных или

ведение подробных журналов не является проблемой. 

Вы могли заметить: в этом и во всех предыдущих примерах

мы не столько «собирали» данные, сколько «выводили» их. 

Очевидно, что данными, выводимыми в окно терминала, сложно манипулировать. Подробнее о хранении информации и

создании баз данных вы узнаете в главе 5. 

**Обработка перенаправлений**

Перенаправления позволяют веб-серверу использовать имя

текущего домена или URL для контента, находящегося в

другом месте. Существует два типа перенаправлений: перенаправления на стороне сервера, когда URL

изменяется до загрузки страницы; 

перенаправления на стороне клиента, иногда с

сообщением наподобие «через десять секунд вы

перейдете на другой сайт», когда сначала загружается

страница сайта, а потом происходит переход на новую

страницу. 

С перенаправлениями на стороне сервера вам обычно ничего

не нужно делать. Если вы используете библиотеку urllib для Python 3.x, то она обрабатывает перенаправления

автоматически\! В случае применения библиотеки запросов

проследите, чтобы флаг allow\_redirects имел значение

True:

r = requests.get\( 'http://github.com' , 

allow\_redirects=True\)

Просто помните, что иногда URL сканируемой страницы

может не совпадать с URL, по которому расположена эта

страница. 

Подробнее о перенаправлениях на стороне клиента, которые

выполняются с помощью JavaScript или HTML, см. в главе 12. 

**Сбор информации с нескольких сайтов**

Каждый раз, когда я делаю доклад о взломе веб-страниц, кто-нибудь обязательно спрашивает: «Как построить Google?» Мой

ответ всегда состоит из двух частей: «Во-первых, вам нужно

где-то взять много миллиардов долларов, чтобы купить

крупнейшие в мире хранилища данных и разместить их в

скрытых местах по всему миру. Во-вторых, вам нужно

разработать веб-краулер». 

В 1996 году, когда Google только начиналась, она состояла

всего из двух аспирантов из Стэнфорда, у которых был старый

сервер и веб-краулер, написанный на Python. Теперь, зная, как

![Image 35](images/000035.png)

работает веб-скрапинг, вы официально располагаете

инструментами, необходимыми для того, чтобы стать

следующим технологическим мультимиллиардером\! 

Заявляю совершенно серьезно: именно веб-краулеры —

основа того, что движет многими современными веб-технологиями, и для их использования не обязательно иметь

большое хранилище данных. Чтобы выполнить любой кросс-доменный анализ данных, необходимо разработать веб-краулеры, способные интерпретировать и хранить данные, собранные со множества интернет-страниц. 

Как и в предыдущем примере, веб-краулеры, которые мы

намерены создать, будут переходить по ссылкам от страницы к

странице, формируя карту сети. Однако на этот раз они не

станут игнорировать внешние ссылки, а, наоборот, будут

переходить по ним. 

**Впереди — неведомые воды**

Имейте в виду: код, представленный ниже, может переходить в

любую точку Интернета. Если мы что-то и узнали из «Шести

шагов по “Википедии”», так это то, что всего за несколько

переходов можно полностью уйти с сайта, например, http://www.sesamestreet.org, и попасть в гораздо менее

приятное место. 

Дети, получите разрешение у родителей, прежде чем запускать

этот код. Если у вас есть твердые убеждения или религиозные

ограничения, которые не позволяют вам посещать порносайты, 

то ознакомьтесь с данным кодом, однако будьте осторожны, запуская программу. 

Прежде чем приступать к написанию веб-краулера, который, хотите вы этого или нет, будет переходить по всем

внешним ссылкам, следует задать себе несколько вопросов. 

• Какие данные я намереваюсь собрать? Достаточно ли для

этого выполнить веб-скрапинг нескольких заранее

определенных сайтов \(что почти всегда проще сделать\), или

же мой краулер должен иметь возможность обнаруживать

новые сайты, о которых я, вероятно, не подозреваю? 

• Попав на определенный сайт, должен ли мой краулер сразу

переходить по следующей внешней ссылке на другой сайт, или же ему следует задержаться на какое-то время на этом и

изучить его более углубленно? 

• Существуют ли какие-либо условия, при которых я бы не

хотел выполнять веб-скрапинг текущего сайта? Интересует

ли меня контент на иностранных языках? 

• Как я буду защищаться от судебных исков, если мой веб-краулер привлечет внимание веб-мастера одного из сайтов, на которые попадет? \(Подробнее об этом см. в главе 18.\) Менее чем в 60 строках кода можно без труда уместить

гибкий набор функций Python, комбинация которых дает

различные типы веб-скраперов:

from urllib.request import urlopen

from urllib.parse import urlparse

from bs4 import BeautifulSoup

import re

import datetime

import random

 

pages = set\(\)

random.seed\(datetime.datetime.now\(\)\)

 

\# Получить список всех внутренних ссылок, 

найденных на странице. 

def getInternalLinks\(bs, includeUrl\):

 

 

 

 

includeUrl 

=

'\{\}://\{\}'.format\(urlparse\(includeUrl\).scheme, 

urlparse\(includeUrl\).netloc\)

internalLinks = \[\]

\# найти все ссылки, которые начинаются с

"/" 

for link in bs.find\_all\('a', 

href=re.compile\('^\(/|.\*'\+includeUrl\+'\)' 

\)\):

if link.attrs\['href'\] is not None:

if link.attrs\['href'\] not in

internalLinks:

if\(link.attrs\['href'\].startswit

h\('/'\)\):

internalLinks.append\(

includeUrl\+link.attrs\[' 

href'\]\)

else:

internalLinks.append\(link.a

ttrs\['href'\]\)

return internalLinks

 

\# Получить список всех внешних ссылок, найденных на странице. 

def getExternalLinks\(bs, excludeUrl\):

externalLinks = \[\]

\# Найти все ссылки, которые начинаются с

"http" или "www", 

\# не содержащие текущий URL. 

for link in bs.find\_all\('a', 

href=re.compile\('^\(http|www\)

\(\(?\!'\+excludeUrl\+'\).\)\*$'\)\):

if link.attrs\['href'\] is not None:

if link.attrs\['href'\] not in

externalLinks:

externalLinks.append\(link.attrs

\['href'\]\)

return externalLinks

 

def getRandomExternalLink\(startingPage\):

html = urlopen\(startingPage\)

bs = BeautifulSoup\(html, 'html.parser'\)

externalLinks = getExternalLinks\(bs, 

urlparse\(startingPage\).netloc\)

if len\(externalLinks\) == 0:

print\('No external links, looking

around the site for one'\)

 

 

 

 

 

 

 

 

 

 

 

 

domain 

=

'\{\}://\{\}'.format\(urlparse\(startingPage\).scheme, 

urlparse\(startingPage\).netloc\)

internalLinks =

getInternalLinks\(bs, domain\)

 

 

 

 

 

 

 

 

 

 

 

 

return

getRandomExternalLink\(internalLinks\[random.rand

int\(0, 

len\(int

ernalLinks\)-1\)\]\)

else:

 

 

 

 

 

 

 

 

 

 

 

 

return

externalLinks\[random.randint\(0, 

len\(externalLinks\)-1\)\]

 

def followExternalOnly\(startingSite\):

 

 

 

 

externalLink 

=

getRandomExternalLink\(startingSite\)

print\('Random external link is:

\{\}'.format\(externalLink\)\)

followExternalOnly\(externalLink\)

followExternalOnly\('http://oreilly.com'\)

Эта программа начинает с сайта **http://oreilly.com** и

случайным образом переходит от одной внешней ссылки к

другой. Вот пример результатов, которые она выводит: http://igniteshow.com/

http://feeds.feedburner.com/oreilly/news

http://hire.jobvite.com/CompanyJobs/Careers.asp

x?c=q319

http://makerfaire.com/

Не обязательно внешние ссылки обнаружатся на первой же

странице сайта. В этом случае для поиска внешних ссылок

используется метод, аналогичный тому, который применялся в

предыдущем примере веб-краулинга для рекурсивного

перехода в глубину сайта до тех пор, пока не будет найдена

внешняя ссылка. 

![Image 36](images/000041.png)

![Image 37](images/000049.png)

Принцип работы этой программы показан на рис. 3.1 в виде

блок-схемы. 

 
