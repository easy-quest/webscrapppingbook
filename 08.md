**Дополнительные ресурсы**

Обработка, анализ и понимание машиной естественного языка

— одна из самых сложных задач информатики. На эту тему

написано бесчисленное множество книг и исследовательских

работ. Я надеюсь, что материалы этой главы вдохновят вас на

размышления, выходящие за рамки обычного веб-скрапинга, или по крайней мере дадут некое первичное направление, с

которого следует начинать проект, требующий анализа

естественного языка. 

Есть множество отличных ресурсов, посвященных

начальной обработке естественного языка и Python Natural Language Toolkit. В частности, в книге *Natural Language* *Processing with Python* Стивена Берда \(Steven Bird\), Эвана Кляйна

\(Ewan Klein\) и Эдварда Лопера \(Edward Loper\) \(издательство

O’Reilly\) \(**http://oreil.ly/1HYt3vV**\) эта тема изложена комплексно, начиная с основ. 

Кроме того, есть книга *Natural Language Annotations for* *Machine Learning* Джеймса Пустейовского \(James Pustejovsky\) и

Эмбер Стаббс \(Amber Stubbs\) \(издательство O’Reilly\) \(**http://oreil.ly/S3BudT**\) 

— 

несколько 

более 

сложное

теоретическое руководство. Чтобы выполнить представленные

там уроки, вам понадобятся знания Python; для тем, рассмотренных в этой книге, прекрасно подходит Python Natural Language Toolkit. 

12 Многие методики, описанные в этой главе, применимы ко всем или к

большинству языков, однако сейчас мы сосредоточимся на обработке только одного

естественного языка — английского. Например, такие инструменты, как Python Natural Language Toolkit, ориентированы именно на английский язык; 56 %

содержимого Интернета все еще написано по-английски \(по данным W3Techs \(http://w3techs.com/technologies/overview/content\_language/all\), немецкий язык

отстает от английского всего на 6 %\). Но кто знает? В будущем английский язык почти

наверняка потеряет главенствующую роль в большей части Интернета, и в

ближайшие несколько лет эти методы могут претерпеть значительные изменения. 

13  *Vinyals O. et al. * A Picture Is Worth a Thousand \(Coherent\) Words: Building a Natural Description of Images \(http://bit.ly/1HEJ8kX\), Google Research Blog, November 17, 2014. 

14 Исключением является последнее слово в тексте, поскольку за ним не стоит

следу ющее. В нашем примере текста последнее слово представляет собой точку \(.\), что удобно, так как это слово встречается в тексте еще 215 раз и поэтому не является

тупиком. Однако в реальных реализациях генератора Маркова может потребоваться

специальная обработка случая, когда достигнуто последнее слово в тексте. 

15 Стоит еще раз отметить, что библиотека NLTK предназначена для работы с

текстами на английском языке. Если вы хотите использовать ее для токенизации

предложений на русском языке, необходимо добавить параметр "language=russian" 

при вызове токенизатора tokens = word\_tokenize\("Текст на русском", lanugage="russian"\). Более подробно о способах использования NLTK можно почитать

по адресу https://python-school.ru/nlp-text-preprocessing/. Конкретно для русского

языка в большинстве случаев используют специализированные библиотеки, такие

как Natasha, Yargi-parser и т.д. Больше информации можно найти по адресу

https://natasha.github.io/. — *Примеч. науч. ред. *

16 На русский язык эту фразу можно перевести как «Он был объективен в

достижении своей цели — написать объективную философию, в первую очередь

используя глаголы со словами в косвенном падеже», что не представляет проблемы

для понимания. В качестве русского аналога можно привести такой пример: «Косил

косой косой косой». — *Примеч. пер. *

17 Аналогичная русская фраза, в которой алгоритму пришлось бы отличать глагол

от существительного, могла бы звучать так: «Слой пыли был таким толстым, что, пока

он ее вытирал, его попросили: “Не пыли\!”».. — *Примеч. пер. *

**Глава 10. Сбор данных из форм и проверка**

**авторизации**

Один из первых вопросов, который возникает, стоит лишь

немного выйти за рамки простейшего анализа веб-страниц, —

«Как получить доступ к информации, требующей входа в

систему?». Сеть все больше стремится к взаимодействию, движется в сторону социальных сетей и пользовательского

контента. Формы и проверка авторизации являются

неотъемлемой частью таких сайтов, избежать их практически

невозможно. К счастью, с ними легко справиться. 

До сих пор взаимодействие наших веб-скраперов с веб-серверами в большинстве примеров сводилось к получению

информации с помощью GET-запроса HTTP. Эта глава

посвящена методу POST, который передает информацию на

веб-сервер для хранения и анализа. 

Формы — это, в сущности, способ отправить веб-серверу от

пользователя POST-запрос, который веб-сервер может понять и

задействовать впоследствии. Если размещенные на сайте теги

ссылок позволяют пользователям форматировать GET-запросы, то HTML-формы дают возможность форматировать

POST-запросы. Всего пара строк кода — и нам будет под силу

самостоятельно создавать такие запросы и отправлять их на

сервер с помощью веб-скрапера. 

**Библиотека Requests**

По веб-формам вполне можно перемещаться, используя только

основные библиотеки Python, но если добавить немного

синтаксического сахара, то жизнь станет заметно слаще. Когда

нужно сделать нечто большее, чем просто отправить GET-

запрос с помощью urllib, имеет смысл выйти за пределы

основных библиотек Python. 

Библиотека Requests \(**http://www.python-requests.org**\) отлично

справляется со сложными HTTP-запросами, cookie-файлами, заголовками и др. Кеннет Рейтц \(Kenneth Reitz\), автор

библиотеки Requests, говорит об основных инструментах

Python следующее: «Стандартный модуль urllib2 обеспечивает

большую часть функциональных возможностей HTML, но его

API никуда не годится. Он был создан для другого времени и

другого Интернета. Он требует слишком много усилий \(даже

переопределения метода\) для выполнения самых простых

задач. 

Так не должно быть. По крайней мере, не в Python». 

Как и любую библиотеку Python, библиотеку *Requests* можно

установить с помощью любого стороннего менеджера

библиотек Python, такого как pip, или же скачав и установив

исходный 

файл

\(**https://github.com/kennethreitz/requests/tarball/master**\). 

**Отправка простейшей формы**

Большинство веб-форм состоят из нескольких HTML-полей, кнопки отправки и страницы обработки, на которой, собственно, и выполняется обработка формы. Многие HTML-поля обычно текстовые, однако встречаются поля для загрузки

файлов и передачи других нетекстовых данных. 

Наиболее популярные сайты блокируют доступ к формам

авторизации в файлах robots.txt \(законность веб-скрапинга

таких форм обсуждается в главе 18\), поэтому на

**pythonscraping.com** я из соображений безопасности разработала

несколько типов форм аутентификации, которые позволят вам

испытать возможности ваших веб-скраперов. Самая простая из

этих 

форм 

размещена 

на 

странице

**http://pythonscraping.com/pages/files/form.html**. 

Полный HTML-код формы выглядит следующим образом:

<form method="post" action="processing.php"> First 

name: 

<input 

type="text" 

name="firstname"><br> 

Last name: <input type="text" name="lastname"> 

<br> 

<input type="submit" value="Submit"> 

</form> 

Здесь следует отметить пару моментов: во-первых, имена

двух полей ввода — firstname и lastname. Это важно. Имена

полей соответствуют именам параметров, которые будут

переданы на сервер при отправке формы. Если вы хотите

имитировать действие, которое выполняет форма, и с

помощью POST-запроса передать ваши собственные данные, то должны проследить, чтобы имена этих параметров

совпадали. 

Второе, на что следует обратить внимание, — это то, что

обработка формы выполняется на странице **processing.php** \(ее

полный 

путь 

—

**http://pythonscraping.com/pages/files/processing.php**\). Все POST-запросы к форме должны выполняться *именно на этой*

странице, а не на той, где размещена сама форма. Запомните: назначение HTML-форм состоит лишь в том, чтобы помочь

посетителям сайта представить свои запросы в правильном

формате для отправки на страницу, на которой выполняется

сама обработка. Если вас не интересует формат самого запроса, то не имеет значения, на какой странице находится форма. 

Для отправки формы с помощью библиотеки *Requests* достаточно четырех строк кода, включая импорт и инструкцию

вывода результатов на экран \(да, это действительно очень

просто\):

import requests

 

params = \{'firstname': 'Ryan', 'lastname':

'Mitchell'\}

r 

=

requests.post\("http://pythonscraping.com/pages/

files/processing.php", data=params\)

print\(r.text\)

После отправки формы скрипт должен вернуть следующий

контент страницы:

Hello there, Ryan Mitchell\! 

Этот скрипт применим ко многим простым формам, встречающимся в Интернете. Например, форма подписки на

новостную рассылку O’Reilly Media выглядит так:

<form

action="http://post.oreilly.com/client/o/oreill y/forms/

quicksignup.cgi" 

id="example\_form2" method="POST"> 

<input name="client\_token" type="hidden" 

value="oreilly" /> 

<input name="subscribe" type="hidden" 

value="optin" /> 

<input name="success\_url" type="hidden" 

value="http://oreilly.com/store/

newsletter-thankyou.html" /> 

<input name="error\_url" type="hidden" 

value="http://oreilly.com/store/

newsletter-signup-error.html" 

/> 

<input name="topic\_or\_dod" type="hidden" 

value="1" /> 

<input name="source" type="hidden" 

value="orm-home-t1-dotd" /> 

<fieldset> 

<input class="email\_address long" 

maxlength="200" name=

"email\_addr" size="25" 

type="text" value=

"Enter your email here" /> 

<button alt="Join" class="skinny" 

name="submit" onclick=

"return

addClickTracking\('orm','ebook','rightrail','dod

' 

 

\);"value="submit">Join</button> 

</fieldset> 

</form> 

На первый взгляд это, возможно, смотрится пугающе, но

учтите, что в большинстве случаев \(исключения из данного

правила мы рассмотрим позже\) мы ищем только два элемента:

• имя поля \(или полей\), данные которого \(которых\) мы хотим

передать \(в данном случае это имя email\_addr\); 

• атрибут action самой формы, то есть страница, на которую

отправляется 

форма 

\(в 

данном 

случае 

это

**http://post.oreilly.com/client/o/oreilly/forms/quicksignup.cgi**\). 

Просто добавьте в код необходимую информацию и

выполните его:

import requests

params 

= 

\{'email\_addr':

'ryan.e.mitchell@gmail.com'\}

r 

=

requests.post\('http://post.oreilly.com/client/o

/oreilly/forms/

quicksignup.cgi', 

data=params\)

print\(r.text\)

В данном случае возвращается ссылка на страницу с еще

одной формой, которую нужно заполнить, прежде чем вы

попадете в список рассылки O’Reilly, но и к данной форме

можно применить ту же концепцию. Однако если вы захотите

попробовать сделать это сами, то я прошу вас использовать

свои новые возможности во благо, а не заваливать издателя

спамом с недействительными регистрационными данными. 

**Переключатели, флажки и другие поля ввода**

Очевидно, что не любая веб-форма представляет собой набор

текстовых полей, под которыми стоит кнопка **Submit** \(Отправить\). 

В 

стандарт 

HTML 

входит 

множество

разнообразных полей ввода данных для форм, таких как

переключатели, флажки и поля выбора. В HTML5 добавились

ползунки \(поля ввода диапазона\), поля для электронной почты, даты и др. Если добавить сюда произвольные поля, создаваемые с помощью JavaScript: палитры выбора цвета, 

календари и все остальное, что еще придумают разработчики, то возможности форм становятся просто безграничными. 

Но каким бы сложным ни казалось поле формы, вас должны

интересовать только две вещи: имя элемента и его значение. 

Имя элемента легко определить, посмотрев в исходный код и

найдя атрибут name. Значение иногда бывает найти сложнее, поскольку оно может заполняться JavaScript непосредственно

перед отправкой формы. В качестве примера приведу столь

экзотическое поле формы, как палитра выбора цвета, — ее

значением, скорее всего, будет что-то наподобие \#F03030. 

Если вы не уверены в формате значения поля ввода, то

можно воспользоваться различными инструментами для

отслеживания запросов GET и POST в браузере, которые

отправляются на сайты и с сайтов. Как уже упоминалось, лучший и, пожалуй, самый очевидный способ отслеживания

GET-запросов — это посмотреть на URL сайта. Если он имеет

вид:

http://domainname.com?thing1=foo&thing2=bar

то это значит, что он соответствует форме следующего вида:

<form method="GET" action="someProcessor.php"> 

<input type="someCrazyInputType" name="thing1" 

value="foo" /> 

<input 

type="anotherCrazyInputType" 

name="thing2" value="bar" /> 

<input type="submit" value="Submit" /> 

</form> 

и объекту параметров Python:

\{'thing1':'foo', 'thing2':'bar'\}

![Image 59](images/000070.png)

Если вы столкнулись со сложной формой POST и хотите

узнать точно, какие параметры браузер отправляет на сервер, то самый простой способ — просмотреть их с помощью

инспектора браузера или встроенных в браузер инструментов

разработчика \(рис. 10.1\). 

 

**Рис. 10.1. ** В разделе Form Data, отмеченном рамкой, показаны POST-параметры thing1 и

thing2 со значениями foo и bar

Чтобы открыть инструменты разработчика в Chrome, можно

выбрать в меню пункты **View**–> **Developer**–> **Developer Tools** \(Просмотр–>Разработчик–>Инструменты разработчика\). Вы

увидите список всех запросов, которые браузер создает при

взаимодействии с текущим сайтом. Это может оказаться

хорошим способом увидеть подробную структуру этих

запросов. 

**Передача файлов и изображений**

Несмотря на то что загрузка файлов широко распространена в

Интернете, она не особенно часто применяется при веб-скрапинге. Однако вы, возможно, захотите написать тест для

своего сайта, который включает в себя загрузку файла. В любом

случае, полезно знать, как это делается. 

Рассмотрим реальную форму загрузки файла, размещенную

по адресу **http://www.pythonscraping.com/pages/files/form2.html**. 

Форма на этой странице имеет следующую разметку:

<form action="processing2.php" method="post" 

enctype="multipart/form-data"> 

Submit a jpg, png, or gif: <input

type="file" name="uploadFile"><br> 

<input type="submit" value="Upload File"> 

</form> 

За исключением тега <input> с атрибутом type, имеющим

значение file, эта форма мало чем отличается от форм с

текстовыми полями, рассмотренных в предыдущих примерах. 

К счастью, способ обработки таких форм в Python-библиотеке

Requests тоже похож:

import requests

 

files = \{'uploadFile': open\('files/python.png', 

'rb'\)\}

r 

=

requests.post\('http://pythonscraping.com/pages/

files/processing2.php', 

files=files\)

print\(r.text\)

Обратите внимание: значением, переданным в поле формы

\(с именем uplo adFile\), теперь является не обычная строка, а

Python-объект File, возвраща емый функцией open. В данном

примере мы отправляем файл с изображением, хранящийся на

локальном компьютере, путь к которому относительно

каталога, откуда запускается скрипт Python, выглядит так:

../files/Python-logo.png. 

Это действительно очень просто\! 

**Обработка данных авторизации и параметров cookie** До сих пор мы по большей части обсуждали формы, позволяющие передавать информацию на сайт или

просматривать необходимые сведения на странице, которая

открывается сразу после отправки формы. Чем это отличается

от формы авторизации, позволяющей сохранять статус

авторизованного 

пользователя 

на 

протяжении 

всего

посещения сайта? 

Кто вошел в систему, а кто — нет, большинство современных

сайтов отслеживают с помощью cookie-файлов. После

аутентификации учетных данных пользователя сайт сохраняет

их в cookie-файле браузера. В этом файле обычно содержатся

сгенерированный сервером маркер, время ожидания и данные

сопровождения. Затем сайт использует этот cookie-файл как

своеобразное доказательство аутентификации, отображаемое

на каждой странице, которую вы посетили во время своего

пребывания на сайте. Cookie-файлы стали широко

использоваться в середине 1990-х годов, а до тех пор

безопасная аутентификация и отслеживание посетителей

сайтов представляли огромную проблему. 

Cookie-файлы — отличное решение для веб-разработчиков, однако для веб-скраперов могут быть проблемой. Вы можете

хоть весь день отправлять аутентификационные данные, но

если не отслеживать cookie-файлы, возвращаемые формой

аутентификации, то следующая же страница, на которую вы

перейдете, будет работать так, словно вы вообще никогда не

входили в систему. 

Я создала простую форму аутентификации по адресу

**http://pythonscraping.com/pages/cookies/login.html** \(имя

пользователя может быть любым, но паролем является слово

password\). Эта форма обрабатывается на странице

**http://pythonscraping.com/pages/cookies/welcome.php**, где

содержится 

ссылка 

на 

главную 

страницу

**http://pythonscraping.com/pages/cookies/profile.php**. 

Если вы попытаетесь открыть страницу **welcome.php** или

страницу **profile.php** без аутентификации, то получите

сообщение об ошибке и совет сначала ввести имя и пароль. На

странице **profile.php** выполняется проверка cookie-файлов

браузера, по результатам которой становится ясно, получил ли

браузер cookie-файл на странице аутентификации. 

Библиотека *Requests* позволяет легко отслеживать cookie-файлы:

import requests

 

params = \{'username': 'Ryan', 'password':

'password'\}

r 

=

requests.post\('http://pythonscraping.com/pages/

cookies/welcome.php', params\)

print\('Cookie is set to:'\)

print\(r.cookies.get\_dict\(\)\)

print\('Going to profile page...'\)

r 

=

requests.get\('http://pythonscraping.com/pages/c

ookies/profile.php', 

cookies=r.cookies\)

print\(r.text\)

Здесь мы отправляем параметры аутентификации на

страницу **welcome.php**, которая обрабатывает данные формы

входа в систему. Затем из результатов последнего запроса мы

извлекаем данные cookie, выводим результат для проверки, а

после передаем их на страницу **profile.php** в виде значения

аргумен та cookies. 

Это хорошо работает в простых ситуациях, но как быть в

случае более сложного сайта, который часто без

предупреждения изменяет cookie-файлы, или если мы с самого

начала не подумали о cookie-файлах? В таких ситуациях

отлично работает функция session из библиотеки Requests: import requests

 

session = requests.Session\(\)

 

params = \{'username': 'username', 'password':

'password'\}

s 

=

session.post\('http://pythonscraping.com/pages/c

ookies/welcome.php', params\)

print\('Cookie is set to:'\)

print\(s.cookies.get\_dict\(\)\)

print\('Going to profile page...'\)

s 

=

session.get\('http://pythonscraping.com/pages/co

okies/profile.php'\)

print\(s.text\)

В этом случае объект session \(полученный путем вызова

функции request.Ses sion\(\)\) отслеживает информацию о

сессии, такую как cookie-файлы, заголовки и даже информация

о протоколах, которые могут использоваться поверх HTTP, например, HTTPAdapters. 

Requests — просто фантастическая библиотека, по полноте

поддерживаемых операций уступающая, пожалуй, только

Selenium \(см. главу 11\), так что программисту нет нужды

обдумывать их или писать код самостоятельно. Идея

откинуться на спинку кресла и позволить библиотеке

выполнить за вас всю работу может показаться заманчивой, однако при написании веб-скраперов крайне важно всегда

знать, как выглядят cookie-файлы и чем именно они

управляют. Это знание позволит вам сэкономить много часов

мучительной отладки в попытках выяснить, почему сайт себя

ведет так странно\! 

**Базовая аутентификация доступа через HTTP. ** До

появления cookie-файлов одним из популярных способов

обработки аутентификационных данных была *базовая*

*аутентификация доступа* через HTTP. Она до сих пор иногда

еще встречается, особенно на сайтах с высоким уровнем

безопасности или на корпоративных сайтах, а также в

некоторых API. Я создала страницу с таким типом

аутентификации 

по 

адресу

**http://pythonscraping.com/pages/auth/login.php** \(рис. 10.2\). 

![Image 60](images/000020.png)

 

**Рис. 10.2. ** Чтобы открыть страницу, защищенную базовой аутентификацией доступа, пользователь должен ввести свое имя и пароль

Как обычно в наших примерах, вы можете ввести любое имя

пользователя и пароль **password**. 

В пакет *Requests* входит модуль auth, специально

созданный для обработки HTTP-аутентификации:

import requests

from requests.auth import AuthBase

from requests.auth import HTTPBasicAuth

 

auth = HTTPBasicAuth\('ryan', 'password'\)

r 

=

requests.post\(url='http://pythonscraping.com/pa

ges/auth/login.php', 

auth=auth\)

print\(r.text\)

Внешне это выглядит как обычный POST-запрос, однако

объект HTTPBasicAuth передается в запросе в виде аргумента

auth. Возвращаемый текст представляет собой страницу, защищенную именем пользователя и паролем \(или страницу

![Image 61](images/000052.png)

**Access Denied** \(Отказано в доступе\), если запрос не будет

выполнен\). 

**Другие проблемы с формами**

Веб-формы — излюбленная точка входа на сайт вредоносных

ботов. Вы же не хотите, чтобы боты создавали

пользовательские учетные записи, занимали драгоценное

время сервера или отправляли спам-комментарии в блог\! 

Именно поэтому на современных сайтах в HTML-формы часто

встраиваются средства обеспечения безопасности, которые не

всегда легко сразу заметить. 

О том, как справляться с капчей, вы узнаете в главе 13, в

которой 

рассматриваются 

обработка 

изображений 

и

распознавание текста в Python. 

Если вы столкнетесь с загадочной ошибкой или сервер

отклонит отправку вашей формы по неизвестной причине, то

ознакомьтесь с главой 14, в которой рассматриваются ловушки

для хакеров \(honeypots\), скрытые поля и другие меры

безопасности, которые сайты предпринимают для защиты

форм. 

**Глава 11. Веб-скрапинг данных JavaScript** Скриптовые языки на стороне клиента — это языки, которые

работают не на веб-сервере, а в самом браузере. Успешность

клиентского языка зависит от способности браузера правильно

интерпретировать и выполнять программы на этом языке. \(Вот

почему так легко отключить JavaScript в браузере.\) Клиентских языков гораздо меньше, чем серверных, — отчасти

из-за того, что очень трудно заставить производителей всех

браузеров выполнять требования стандарта. Для веб-скрапинга это

хорошо: чем с меньшим количеством языков приходится иметь дело, тем лучше. 

По большей части вы будете сталкиваться в Интернете только с

двумя языками: ActionScript \(который используется в приложениях

Flash\) и JavaScript. ActionScript сейчас встречается гораздо реже, чем

десять лет назад, в основном для потоковой передачи

мультимедийных файлов, в качестве платформы для онлайн-игр или

отображения вводных страниц сайтов, владельцам которых еще не

намекнули, что вводные страницы никто не любит. Как бы то ни

было, поскольку большого спроса на веб-скрапинг Flash-страниц нет, в этой главе мы уделим основное внимание другому клиентскому

языку, который на современных веб-страницах встречается

повсеместно: JavaScript. 

В настоящее время JavaScript — самый распространенный в

Интернете и лучше всего поддерживаемый клиентский скриптовый

язык. С его помощью можно собирать информацию для

отслеживания пользователей, отправки форм без перезагрузки

страницы, встраивания мультимедиа и запуска целых онлайн-игр. 

Даже обманчиво простые на вид страницы часто содержат несколько

фрагментов JavaScript. Встроенные скрипты JavaScript можно найти в

исходном коде страницы между тегами script:

<script> 

alert\("This creates a pop-up using

JavaScript"\); 

</script> 

**Краткое введение в JavaScript**

Очень полезно как минимум иметь представление о том, что

происходит в коде, веб-скрапинг которого вы выполняете. Уже по

одной лишь этой причине имеет смысл ближе познакомиться с

JavaScript. 

*JavaScript* — слабо типизированный язык, синтаксис которого

часто сравнивают с C\+\+ и Java. Некоторые элементы синтаксиса, такие как операторы, циклы и массивы, в этих языках, пожалуй, действительно похожи, однако слабая типизация и скриптовая

природа JavaScript способны поставить в тупик некоторых

программистов. 

Например, следующий код рекурсивно вычисляет значения

последовательности Фибоначчи и выводит их в консоль браузера, входящую в комплект инструментария разработчика:

<script> 

function fibonacci\(a, b\)\{

var nextNum = a \+ b; 

console.log\(nextNum\+" is in the Fibonacci sequence"\); 

if\(nextNum < 100\)\{

fibonacci\(b, nextNum\); 

\}

\}

fibonacci\(1, 1\); 

</script> 

Обратите внимание: все переменные идентифицируются путем

добавления к ним ключевого слова var. Это как знак $ в PHP или

объявление типа \(int, String, List и т.п.\) в Java или C\+\+. Python в

этом смысле выделяется тем, что в нем нет столь явного объявления

переменных. 

В JavaScript также очень удобно то, что функции здесь тоже

являются переменными:

<script> 

var fibonacci = function\(\) \{

var a = 1; 

var b = 1; 

return function \(\) \{

var temp = b; 

b = a \+ b; 

a = temp; 

return b; 

\}

\}

var fibInstance = fibonacci\(\); 

console.log\(fibInstance\(\)\+" is in the Fibonacci sequence"\); 

console.log\(fibInstance\(\)\+" is in the Fibonacci sequence"\); 

console.log\(fibInstance\(\)\+" is in the Fibonacci sequence"\); 

</script> 

Поначалу это, возможно, выглядит жутковато, но все станет

проще, если представить себе функции как лямбда-выражения \(см. 

главу 2\). Переменная fibonacci определена как функция. Ее

значением является функция, которая выводит все числа

последовательности Фибоначчи в порядке возрастания. При каждом

вызове эта функция возвращает функцию вычисления очередного

значения Фибоначчи, в результате выполнения которой

увеличиваются значения переменных, используемых внутри

функции fibonacci. 

На первый взгляд это может показаться запутанным, однако

некоторые задачи, такие как вычисление значений Фибоначчи, обычно решаются с помощью подобных схем. Передача функций в

качестве переменных также чрезвычайно полезна в качестве

обратных вызовов при обработке действий пользователя; стоит

освоить этот стиль программирования, если вы хотите научиться

читать код JavaScript. 

**Популярные библиотеки JavaScript**

Знать «чистый» JavaScript, безусловно, важно. Однако не стоит

рассчитывать на многое в современном Интернете, если не

использовать хотя бы одну из множества сторонних библиотек этого

языка. При просмотре исходного кода страницы вам постоянно будет

встречаться одна или несколько широко распространенных

библиотек, которые мы рассмотрим далее. 

Выполнение кода JavaScript с применением Python может

потребовать очень больших затрат труда и вычислительных

ресурсов, особенно в крупных масштабах. Знать, что к чему в

JavaScript, и уметь анализировать код непосредственно \(не прибегая

к необходимости выполнять его для получения информации\) может

быть очень полезным навыком, который избавит вас от множества

проблем. 

**jQuery**

*jQuery* — чрезвычайно распространенная библиотека, которой

пользуется 70 % самых популярных интернет-сайтов и примерно 30

% остальной части Интернета18.  Понять, что на сайте используется

jQuery, очень легко: где-то в его коде будет присутствовать импорт

этой библиотеки:

<script

src="http://ajax.googleapis.com/ajax/libs/jquery/1. 

9.1/jquery.min.js"> 

</script> 

Если вы нашли на сайте jQuery, то выполнять его веб-скрапинг

следует очень осторожно: библиотека отлично умеет создавать

динамический HTML-контент, который образуется лишь после

выполнения скрипта JavaScript. При традиционном веб-скрапинге

вы получите только предварительно загруженную страницу в том

виде, в каком она появляется прежде, чем JavaScript наполнит ее

контентом \(мы рассмотрим эту проблему веб-скрапинга более

подробно в разделе «Ajax и динамический HTML» данной главы на с. 

200\). 

Кроме того, на страницах с jQuery выше вероятность обнаружить

анимацию, интерактивный контент и встроенные медиафайлы, что

обычно усложняет веб-скрапинг. 

**Google Analytics**

Библиотека *Google Analytics* используется примерно на 50 % сайтов19, 

что делает ее, пожалуй, самой распространенной библиотекой

JavaScript и самым популярным инструментом отслеживания

пользователей в Интернете. Она используется также на сайтах

**http://pythonscraping.com** и **http://www.oreilly.com/**. 

Определить, применяется ли на веб-странице Google Analytics, легко. Внизу такой страницы будет размещаться примерно

следующий код JavaScript \(в данном случае код взят с сайта O’Reilly Media\):

<\!-- Google Analytics --> 

<script type="text/javascript"> 



var \_gaq = \_gaq || \[\]; 

\_gaq.push\(\['\_setAccount', 'UA-4591498-1'\]\); 

\_gaq.push\(\['\_setDomainName', 'oreilly.com'\]\); 

\_gaq.push\(\['\_addIgnoredRef', 'oreilly.com'\]\); 

\_gaq.push\(\['\_setSiteSpeedSampleRate', 50\]\); 

\_gaq.push\(\['\_trackPageview'\]\); 

 

\(function\(\) 

\{ 

var 

ga 

=

document.createElement\('script'\); ga.type =

'text/javascript'; ga.async = true; ga.src =

\('https:' ==

document.location.protocol 

? 

'https://ssl' 

:

'http://www'\) \+

'.google-analytics.com/ga.js'; var s =

document.getElementsByTagName\('script'\)\[0\]; 

s.parentNode.insertBefore\(ga, s\); \}\)\(\); 

 

</script> 

Данный скрипт обрабатывает специальные cookie-файлы Google Analytics, с помощью которых отслеживаются посещения страницы

пользователем. Иногда это представляет проблему для веб-скраперов, предназначенных для выполнения JavaScript и обработки

cookie-файлов \(например, для тех, которые используют описанную

далее в этой главе библиотеку Selenium\). 

Если на сайте применяется Google Analytics или аналогичная

система веб-аналитики и вы не хотите, чтобы сайт узнал, что на нем

выполняется веб-краулинг или веб-скрапинг, то обязательно удалите

все cookie-файлы, используемые для аналитики, или вообще все

cookie-файлы. 

**Google Maps**

Если вы провели хоть сколько-нибудь времени в Интернете, то почти

наверняка встречали там встроенные в сайты карты *Google Maps*. API этой библиотеки позволяет очень легко встраивать в любой сайт

карты с пользовательской информацией. 

При веб-скрапинге любых данных о местоположении понимание

принципов работы Google Maps позволяет легко получить

координаты широты и долготы и даже адреса *' * в удобном формате. 

Один из самых распространенных способов обозначить

местоположение в Google Maps — использовать *маркер, * также

известный как *«булавка»* \(pin\). 

Маркеры можно вставить в любую карту Google с помощью

следующего кода:

var marker = new google.maps.Marker\(\{

 

 

 

 

position: 

new

google.maps.LatLng\(-25.363882,131.044922\), 

map: map, 

title: 'Some marker text' 

\}\); 

Python позволяет легко извлечь все координаты, которые

находятся между google.maps.LatLng\( и \), чтобы сформировать

список широт и долгот. 

Используя 

Google 

Reverse 

Geocoding 

API

\(**https://developers.google.com/maps/documentation/javascript/examples/**

**geocoding-reverse**\), можно преобразовать эти пары координат в

адреса, представленные в формате, удобном для хранения и анализа. 

**Ajax и динамический HTML**

До сих пор мы рассматривали только один способ коммуникации с

веб-сервером — отправку ему того или иного HTTP-запроса при

отображении новой страницы. Если вам когда-либо приходилось

отправлять форму или извлекать информацию с веб-сервера без

перезагрузки страницы, то вы, вероятно, имели дело с сайтом, на

котором применялся Ajax. 

Вопреки бытующему мнению, Ajax — это не язык

программирования, а группа технологий, используемых для

выполнения определенной задачи \(очень похоже на веб-скрапинг, в

сущности\). Слово *Ajax* расшифровывается как *Asynchronous JavaScript* *and XML* \(Асинхронный JavaScript и XML\); эта технология служит для

отправки и получения информации с веб-сервера без выполнения

запроса на перезагрузку всей страницы. 

![Image 62](images/000008.png)

Никогда не говорите: «Этот сайт будет написан на Ajax». Правильнее

было бы сказать: «Данная форма будет использовать Ajax для связи с

веб-сервером». 

Подобно Ajax, *динамический HTML* \(Dynamic HTML, DHTML\) представляет собой набор технологий, применяемых для достижения

общей цели. DHTML — это способность изменить HTML-код или язык

CSS либо то и другое вместе так же, как клиентские сценарии

изменяют HTML-элементы на странице. Например, кнопка

появляется только после того, как пользователь переместит

указатель мыши, при щелчке может измениться цвет фона, а по

запросу Ajax — загрузиться новый блок контента. 

Обратите 

внимание: 

слово 

«динамический» 

обычно

ассоциируется с такими словами, как «перемещение» или

«изменение», однако наличие интерактивных компонентов HTML, движущихся изображений или встроенных медиаэлементов не

обязательно означает, что данная страница относится к DHTML, даже

если выглядит динамично. Кроме того, за рядом скучнейших, совершенно неподвижных веб-страниц могут стоять DHTML-процессы, в ходе которых с помощью JavaScript изменяются HTML и

CSS. 

Выполнив веб-скрапинг многих сайтов, вы вскоре столкнетесь с

такой ситуацией: контент, наблюдаемый в браузере, не

соответствует контенту, который вы видите в исходном коде, полученном с данного сайта. Иногда, просматривая результаты

работы веб-скрапера, можно мозги сломать, пытаясь выяснить, куда

девалось все то, что вы видели на этой же самой странице в браузере. 

Веб-страница также может представлять собой страницу загрузки, которая, по идее, перенаправляет пользователя на другую страницу, содержащую результаты. Однако вы заметите, что при таком

перенаправлении URL страницы никогда не меняется. 

Оба явления вызваны тем, что ваш веб-скрапер не способен

выполнить скрипт JavaScript, от которого как раз и зависит все

волшебство на этой странице. Без JavaScript HTML-код просто ничего

не делает, и сайт может сильно отличаться от того, как выглядит в

браузере, легко выполняемом JavaScript. 

Есть несколько признаков того, что данная страница может

использовать Ajax или DHTML для изменения или загрузки контента, но из подобных ситуаций есть только два выхода: выполнить веб-скрапинг содержимого непосредственно из JavaScript или

задействовать пакеты Python, способные выполнять JavaScript, и

провести веб-скрапинг сайта прямо в браузере. 

**Выполнение JavaScript в Python с помощью Selenium** Selenium \(**http://www.seleniumhq.org/**\) — это мощный инструмент веб-скрапинга, изначально разработанный для тестирования сайтов. В

настоящее время он применяется и в тех случаях, когда требуется

представить сайт в точности так, как он отображается в браузере. 

Selenium автоматизирует работу браузера, загружая сайт, получая

необходимые данные и даже создавая снимки экрана или другие

подтверждения того, что на сайте выполняются определенные

действия. 

У Selenium нет собственного браузера; его запуск требует

интеграции со сторонними браузерами. Например, если запустить

Selenium с Firefox, то, когда на экране откроется окно Firefox, следует

перейти на нужный сайт и выполнить код. Кому-то это может

показаться изящным, однако я предпочитаю, чтобы мои скрипты

работали в фоновом режиме, для чего часто использую в Chrome *режим консоли*. 

В *режиме консоли* браузер загружает сайты в память и выполняет

JavaScript на странице без какой-либо графической визуализации

сайта для пользователя. Сочетая Selenium с режимом консоли в

Chrome, можно создавать чрезвычайно мощные веб-скраперы, способные легко обрабатывать cookie-файлы, JavaScript, заголовки и

все необходимое так, как если бы вы задействовали браузер с

выводом страницы на экран. 

Библиотеку 

Selenium 

можно 

скачать 

с 

ее 

сайта

\(**https://pypi.python.org/pypi/selenium**\) или же установить ее из

командной строки с помощью стороннего инсталлятора, такого как

pip. 

Веб-драйвер Chrome можно скачать с сайта ChromeDriver \(**http://chromedri ver.chromium.org/downloads**\). 

ChromeDriver 

не

является специализированной библио текой Python — это

независимое приложение, применяемое для управления Chrome, вследствие чего его нельзя установить с помощью pip. Чтобы

использовать ChromeDriver, его необходимо скачать. 

Ajax применяется для загрузки данных на многих страницах \(в

том числе, что характерно, в Google\), однако я создала собственную

страницу для запуска наших веб-скраперов по адресу

**http://pythonscraping.com/pages/javascript/ajaxDemo.html**. 

На 

ней

содержится некий текстовый фрагмент, жестко закодированный в

HTML-коде страницы, который после двухсекундной задержки

заменяется контентом, сгенерированным с помощью Ajax. Если бы

мы решили выполнить веб-скрапинг этой страницы, используя

традиционные методы, то получили бы только загрузочную страницу

без нужных нам данных. 

Библиотека Selenium — API, вызываемый для объекта WebDriver. 

Обратите внимание: это объект Python, представляющий или

действующий как интерфейс предварительно загруженного

приложения WebDriver. То и другое обозначается одним и тем же

словом \(и объект Python, и само приложение\), однако концептуально

важно различать их. 

Объект WebDriver немного похож на браузер в том смысле, что

способен загружать сайты, однако его также можно использовать как

объект BeautifulSoup для поиска элементов страницы, взаимодействия с ними \(отправки текста, нажатия кнопок и т.п.\), а

также для выполнения других действий, управляющих веб-скрапером. 

Следующий код извлекает текст, стоящий «за стеной» Ajax на

тестовой странице:

from selenium import webdriver

from 

selenium.webdriver.chrome.options 

import

Options

import time

 

chrome\_options = Options\(\)

chrome\_options.add\_argument\('--headless'\)

driver = webdriver.Chrome\(

 

 

 

 

executable\_path='drivers/chromedriver', 

options=chrome\_options\)

driver.get\('http://pythonscraping.com/pages/javascr ipt/ajaxDemo.html'\)

time.sleep\(3\)

print\(driver.find\_element\_by\_id\('content'\).text\) driver.close\(\)

**Селекторы Selenium**

В предыдущих главах мы выбирали элементы страницы с помощью

селекторов BeautifulSoup, таких как find и find\_all. В

Selenium для поиска элементов DOM в WebDriver используется

совершенно другой набор селекторов, хотя их имена довольно

простые. 

В этом примере мы использовали селектор find\_element\_by\_id, хотя следующие селекторы также сработали бы:

driver.find\_element\_by\_css\_selector\('\#content'\)

driver.find\_element\_by\_tag\_name\('div'\)

Разумеется, на тот случай, если нужно выбрать на странице

несколько элементов, большинство этих селекторов позволяют

возвращать список элементов Python. Для этого в имени функций

нужно заменить element на elements \(то есть на множественное

число\):

driver.find\_elements\_by\_css\_selector\('\#content'\) driver.find\_elements\_by\_css\_selector\('div'\)

Если же вы все равно хотите использовать BeautifulSoup для анализа

контента сайта, то можете применить функцию WebDriver page\_source. Она возвращает исходный код страницы

\(просматриваемый в момент вызова с помощью DOM\) в виде строки: pageSource = driver.page\_source

bs = BeautifulSoup\(pageSource, 'html.parser'\)

print\(bs.find\(id='content'\).get\_text\(\)\)

В результате получим новый Selenium WebDriver, использующий

библиотеку Chrome, который дает WebDriver команду загрузить

страницу, а затем приостанавливает выполнение на три секунды, после чего просматривает страницу и получает \(как мы надеемся, загруженный к тому моменту\) контент. 

Создавая экземпляр нового Chrome WebDriver в Python, можно

передать ему различные параметры через объект Options. В данном

случае мы используем параметр --headless, чтобы WebDriver работал в фоновом режиме:

chrome\_options = Options\(\)

chrome\_options.add\_argument\('--headless'\)

Кроме того, параметр executable\_path должен указывать на

местоположение скачанного приложения ChromeDriver: driver = webdriver.Chrome\(

 

 

 

 

executable\_path='drivers/chromedriver', 

options=chrome\_options\)

Если все настроено правильно, то выполнение скрипта займет

несколько секунд, а затем появится следующий текст: Here is some important text you want to retrieve\! 

A button to click\! 

Обратите внимание: хотя сама страница содержит HTML-кнопку, функция Selenium .text извлекает текстовое значение этой кнопки

— так же, как и всего остального контента страницы. 

Если вместо трехсекундной паузы заменить значение

time.sleep на одну секунду, то возвращаемый текст изменится на

исходный:

This is some content that will appear on the page while it's loading. 

You don't care about scraping this. 

Это решение работает, однако оно несколько неэффективно, и его

реализация в больших масштабах может вызвать проблемы. Время

загрузки страниц непостоянно, оно зависит от нагрузки на сервер в

любую конкретную миллисекунду. Скорость соединения также

подвержена естественным изменениям. Загрузка страницы занимает

чуть более двух секунд, но мы даем ей целых три, чтобы она

наверняка успела загрузиться целиком. Более эффективное решение

будет многократно проверять наличие определенного элемента, присущего полностью загруженной странице, и возвращать

результат, только когда он есть. 

В следующем коде показателем того, что страница полностью

загружена, является наличие кнопки с идентификатором

loadedButton:

page has been fully loaded:

from selenium import webdriver

from selenium.webdriver.common.by import By

from 

selenium.webdriver.support.ui 

import

WebDriverWait

from 

selenium.webdriver.support 

import

expected\_conditions as EC

 

chrome\_options = Options\(\)

chrome\_options.add\_argument\('--headless'\)

driver = webdriver.Chrome\(

 

 

 

 

executable\_path='drivers/chromedriver', 

options=chrome\_options\)

 

driver.get\('http://pythonscraping.com/pages/javascr ipt/ajaxDemo.html'\)

try:

element = WebDriverWait\(driver, 10\).until\(

EC.presence\_of\_element\_locat

ed\(\(By.ID, 'loadedButton'\)\)\)

finally:

print\(driver.find\_element\_by\_id\('content'\).text

\)

driver.close\(\)

В этом скрипте есть несколько новых импортируемых модулей —

в частности, WebDriverWait и expected\_conditions. Их

сочетание здесь нужно для формирования того, что в Selenium называется *неявным ожиданием*. 

Неявное ожидание отличается от явного тем, что для

продолжения скрипта ожидается определенное состояние DOM, в то

время как при явном ожидании есть жестко заданное время — как в

предыдущем примере, где длительность ожидания составляла три

секунды. При неявном ожидании инициирующее состояние DOM

определяется с помощью ожидаемого условия expected\_condition \(обратите внимание: здесь импортируемый модуль обозначен как EC

— для краткости, в соответствии с общим соглашением\). Ожидаемые

условия в библиотеке Selenium могут быть самыми разными, включая следующие:

• появление всплывающего окна с предупреждением; 

• переход элемента \(например, текстового поля\) в состояние

*«выбран»*; 

• изменение заголовка страницы, отображение текста на странице

или в определенном элементе; 

• появление или исчезновение элемента DOM. 

Большинство из этих ожидаемых условий требуют сначала

указать элемент, за которым нужно проследить. Элементы

указываются с помощью локаторов. Обратите внимание: локаторы —

не то же самое, что селекторы \(подробнее о селекторах см. выше, во

врезке «Селекторы Selenium» на с. 203\). *Локатор* — это абстрактный

язык запросов, использующий объект By. Локаторы можно

применять различными способами, в том числе для создания

селекторов. 

В следующем коде локатор используется для поиска элементов с

идентификатором loadedButton:

EC.presence\_of\_element\_located\(\(By.ID, 

'loadedButton'\)\)

Локаторы также можно использовать для создания селекторов с

помощью функции WebDriver find\_element:

print\(driver.find\_element\(By.ID, 'content'\).text\) Функционально это, конечно же, эквивалентно следующей строке

из нашего примера:

print\(driver.find\_element\_by\_id\('content'\).text\) Если можно не использовать локатор, то не делайте этого; так вы

сэкономите один оператор импорта. Тем не менее этот удобный

инструмент применим во множестве областей и обладает большой

гибкостью. 

Существуют следующие стратегии выбора локатора с объектом

By:

•ID — используется в нашем примере; находит элементы по их

HTML-атрибуту id; 

• CLASS\_NAME — служит для поиска элементов по их HTML-атрибуту class. Почему данная функция называется

CLASS\_NAME, а не просто CLASS? Дело в том, что использование

записи object.CLASS создало бы проблемы для Java-библиотеки

Selenium, где .class является зарезервированным методом. 

Чтобы сохранить синтаксис Selenium в разных языках, вместо

этого применяется CLASS\_NAME; 

• CSS\_SELECTOR — находит элементы по классу, идентификатору

или имени тега, используя соглашения \#idName, .className и

tagName; 

• LINK\_TEXT — находит HTML-теги <a> по тексту, который в них

содержится. Например, чтобы выбрать ссылку с надписью Next, нужно использовать запись \(By.LINK\_TEXT,'Next'\); 

• PARTIAL\_LINK\_TEXT — работает аналогично LINK\_TEXT, но для

части строки; 

• NAME — находит HTML-теги по их атрибуту name, что удобно для

HTML-форм; 

• TAG\_NAME — находит HTML-теги по имени тега; 

• XPATH — использует для выбора элементов выражение XPath \(синтаксис которого описан в следующей врезке\). 

**Синтаксис XPath**

*XPath* \(сокращение от XML Path\) — язык запросов, используемый

для навигации по XML-документам и выбора их частей. Основанный

W3C в 1999 году, этот язык иногда применяется для работы с XML-документами в таких языках программирования, как Python, Java и

C\#. 

BeautifulSoup не поддерживает XPath, в отличие от многих других

библиотек, описанных в этой книге, например Scrapy и Selenium. 

XPath часто можно использовать аналогично селекторам CSS

\(наподобие mytag\#idname\), хотя он предназначен для работы с

любыми XML-документами, а не только с их более частным случаем

— документами HTML. 

У синтаксиса XPath есть четыре основные концепции. 

Узлы делятся на корневые и некорневые:

селектор /div выбирает узел div, только если тот находится в

корне документа; 

селектор //div выбирает все узлы div, независимо от того, в

каком месте документа они находятся. 

Выбор атрибута:

селектор //@href выбирает все узлы с атрибутом href; селектор //a\[@href='http://google.com'\] выбирает в

документе все ссылки, которые указывают на Google. 

Выбор узлов по положению:

селектор //a\[3\] выбирает третью ссылку в документе; селектор //table\[last\(\)\] выбирает последнюю таблицу в

документе; 

селектор //a\[position\(\)<3\] выбирает первые две ссылки в

документе. 

Звездочка \(\*\) соответствует любому набору символов или узлов и

может использоваться в различных ситуациях:

селектор //table/tr/\* выбирает всех потомков тега tr во всех

таблицах \(это удобно при выборе ячеек с помощью тегов th и

td\); 

селектор //div\[@\*\] выбирает все теги div с любыми

атрибутами. 

В синтаксисе XPath также есть много дополнительных свойств. За

последние годы он превратился в довольно сложный язык запросов

с булевой логикой, функциями \(такими как position\(\)\) и

множеством операторов, которые не рассматриваются в этой книге. 

Если у вас возникнет проблема с выбором элементов в HTML или

XML, которую не удастся решить с помощью представленных здесь

функций, то обратитесь к странице Microsoft с описанием

синтаксиса 

XPath 

\(**https://msdn.microsoft.com/en-**

**us/enus/library/ms256471**\). 

**Дополнительные веб-драйверы Selenium**

В предыдущем разделе мы задействовали Selenium в сочетании с

Chrome WebDriver \(ChromeDriver\). В большинстве случаев нет особой

причины открывать браузер и выполнять в нем веб-скрапинг, удобнее запустить его в режиме консоли. Однако работа без режима

консоли и/или применение разных драйверов браузера могут

оказаться полезными по следующим причинам. 

• Исправление неполадок. Если код, работая в режиме консоли, дает

сбой, то выполнить диагностику иногда бывает трудно, поскольку

вы не видите исходную страницу. 

• Для диагностики проблем вы также можете приостановить

выполнение кода и что-то сделать на веб-странице либо

использовать инспектор кода во время работы веб-скрапера. 

• Результаты тестов могут зависеть от конкретного браузера. Сбой в

одном браузере при нормальной работе в другом может указывать

на проблему, свойственную конкретному браузеру. 

Сегодня в создании и обслуживании веб-драйверов Selenium для

всех крупнейших браузеров принимает участие множество групп

разработчиков, как официальных, так и неофициальных. Группа

Selenium 

курирует 

коллекцию 

этих 

веб-драйверов

\(**http://www.seleniumhq.org/download/**\), чтобы обеспечить удобство их

использования. 

firefox\_driver 

= 

webdriver.Firefox\('<path 

to

Firefox webdriver>'\)

safari\_driver = webdriver.Safari\('<path to Safari webdriver>'\)

ie\_driver 

= 

webdriver.Ie\('<path 

to 

Internet

Explorer webdriver>'\)

**Обработка перенаправлений**

Перенаправления на стороне клиента — это переходы на другие

страницы, которые выполняются в вашем браузере с помощью

JavaScript, а не те, что выполняются на сервере перед отправкой

контента страницы. Иногда бывает сложно определить разницу

между тем и другим, просто просматривая страницу в браузере. 

Перенаправление может произойти настолько быстро, что вы не

заметите задержки при загрузке и предположите, что

перенаправление на стороне клиента на самом деле является

перенаправлением на стороне сервера. 

Однако при веб-скрапинге разница очевидна. Перенаправление

на стороне сервера, в зависимости от способа обработки, легко

отлеживается Python-библиотекой urllib без помощи Selenium \(подробнее это описано в главе 3\). Перенаправления на стороне

клиента вообще не будут обрабатываться, если в веб-скрапере не

использовать нечто способное выполнять код JavaScript. 

Selenium обрабатывает перенаправления JavaScript идентично

другим скриптам JavaScript. Однако основная проблема с такими

перенаправлениями состоит в том, когда следует остановить

выполнение кода на странице — другими словами, как определить, что перенаправление завершено. На демонстрационной странице по

адресу 

**http://pythonscraping.com/pages/javascript/redirectDemo1.html** приведен пример перенаправления этого типа с двухсекундной

паузой. 

Существует разумный способ обнаружить это перенаправление, 

«отследив» некий DOM-элемент в начале загрузки страницы, а затем

многократно проверяя данный элемент, пока Selenium не выдаст

исключение Sta leElementReferenceException, говорящее о

том, что данный элемент больше не привязан к DOM страницы и, следовательно, перенаправление завер шилось:

from selenium import webdriver

from 

selenium.webdriver.chrome.options 

import

Options

from 

selenium.webdriver.remote.webelement 

import

WebElement

from 

selenium.common.exceptions 

import

StaleElementReferenceException

import time

 

def waitForLoad\(driver\):

elem = driver.find\_element\_by\_tag\_name\('html'\)

count = 0

while True:

count \+= 1

if count > 20:

print\('Timing out after 10 seconds and

returning'\)

return

time.sleep\(.5\)

try:

 

 

 

 

 

 

 

 

 

 

 

 

elem 

==

driver.find\_element\_by\_tag\_name\('html'\)

except StaleElementReferenceException:

return

chrome\_options = Options\(\)

chrome\_options.add\_argument\('--headless'\)

driver = webdriver.Chrome\(

 

 

 

 

executable\_path='drivers/chromedriver', 

options=chrome\_options\)

driver.get\('http://pythonscraping.com/pages/javascr ipt/redirectDemo1.html'\)

waitForLoad\(driver\)

print\(driver.page\_source\)

driver.close\(\)

Данный скрипт проверяет страницу каждые полсекунды с

предварительной задержкой десять секунд, хотя при необходимости

время проверки и задержки легко изменить. 

Вместо этого можно написать аналогичный цикл, проверяющий

текущий URL страницы, пока он не изменится или пока не совпадет с

искомым URL. 

Ожидание появления и исчезновения элементов — обычная

задача для Selenium. Для этого можно задействовать ту же функцию

WebDriverWait, которую мы использовали в предыдущем примере

с загрузкой кнопки. В следующем коде мы для выполнения той же

задачи задаем время ожидания 15 секунд и определяем селектор

XPath, который ищет контент тега body страницы:

from selenium.webdriver.common.by import By

from 

selenium.webdriver.support.ui 

import

WebDriverWait

from 

selenium.webdriver.chrome.options 

import

Options

from 

selenium.webdriver.support 

import

expected\_conditions as EC

from 

selenium.common.exceptions 

import

TimeoutException

 

chrome\_options = Options\(\)

chrome\_options.add\_argument\('--headless'\)

driver = webdriver.Chrome\(

 

 

 

 

executable\_path='drivers/chromedriver', 

options=chrome\_options\)

 

driver.get\('http://pythonscraping.com/pages/javascr ipt/redirectDemo1.html'\)

try:

 

 

 

 

bodyElement 

= 

WebDriverWait\(driver, 

15\).until\(EC.presence\_of\_element\_

located\(

\(By.XPATH, '//body\[contains\(text\(\), 

'This is the page you are looking

for\!\)\]'\)\)\)

print\(bodyElement.text\)

except TimeoutException:

print\('Did not find the element'\)


