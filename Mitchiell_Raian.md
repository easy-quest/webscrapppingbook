**pytesseract**

После установки Tesseract мы готовы установить библиотеку-оболочку 

Python 

pytesseract, 

использующую 

уже

установленную нами программу Tesseract для чтения файлов

изображений, формируя на выходе строки и объекты, которые

затем можно применять в скриптах Python. 

**Для выполнения примеров нужен pytesseract 0.1.9**

Имейте в виду: между версиями pytesseract 0.1.8 и 0.1.9 есть

ряд существенных изменений \(в том числе появившихся при

участии автора этой книги\). В данном пункте описываются

функции, которые есть только в версии 0.1.9 данной

библиотеки. 

Пожалуйста, 

выполняя 

примеры 

кода, 

представленные в этой главе, убедитесь, что установили

правильную версию. 

Как обычно, вы можете установить pytesseract через pip или

скачать библиотеку со страницы проекта pytesseract \(**https://pypi.python.org/pypi/pytesseract**\) и выполнить следующую

команду:

$ python setup.py install

Библиотеку pytesseract можно использовать в сочетании с

PIL для чтения текста из изображений:

from PIL import Image

import pytesseract

 

print\(pytesseract.image\_to\_string\(Image.open\('f

iles/test.png'\)\)\)

Если вы установили библиотеку Tesseract в папке Python, то можете указать pytesseract это местоположение, добавив

следующую строку:

pytesseract.pytesseract.tesseract\_cmd 

=

'/path/to/tesseract' 

Кроме выдачи результатов OCR для изображений, как было

показано в предыдущем примере, у pytesseract есть еще

несколько полезных функций. Эта библиотека позволяет

определять box-файлы \(группы пикселов, заключенные в

границах символа\):

print\(pytesseract.image\_to\_boxes\(Image.open\('fi

les/test.png'\)\)\)

Она также дает возможность выводить полные данные, в

том числе степень уверенности, номера страниц и строк, информацию о границах и т.п.:

print\(pytesseract.image\_to\_data\(Image.open\('fil

es/test.png'\)\)\)

По умолчанию для этих двух последних файлов выводятся

строковые файлы с разделителями в виде пробелов или

табуляций, но также можно получить результаты в виде

словарей или \(если декодирования в UTF-8 недостаточно\) байтовых строк:

from PIL import Image

import pytesseract

from pytesseract import Output

 

print\(pytesseract.image\_to\_data\(Image.open\('fil

es/test.png'\), 

output\_type=Output.DICT\)\)

print\(pytesseract.image\_to\_string\(Image.open\('f

iles/test.png'\), 

output\_type=Output.BYTES\)\)

В этой главе библиотека pytesseract применяется в

сочетании с утилитой командной строки Tesseract, которая

запускается из Python через библиотеку subprocess. 

Библиотека pytesseract полезна и удобна, однако есть функции

Tesseract, которые она не может выполнять, поэтому стоит

ознакомиться и с другими методами. 

**NumPy**

Для простого распознавания текста библиотека NumPy не

нужна, однако понадобится далее в этой главе, когда мы будем

обучать Tesseract распознавать дополнительные наборы

символов и другие шрифты. Кроме того, применим NumPy для

решения простых математических задач \(таких как

вычисление средневзвешенных значений\) в ряде примеров, которые рассмотрим позже. 

NumPy — это мощная библиотека, используемая для

линейной алгебры и других серьезных математических

приложений. Она хорошо сочетается с Tesseract благодаря

способности математически представлять изображения и

обрабатывать их в виде больших массивов пикселов. 

NumPy можно установить с помощью любого стороннего

инсталлятора Python, такого как pip, или скачав пакет

\(**https://pypi.python.org/pypi/numpy**\) и установив его через

команду $pythonsetup.pyinstall. 

Даже если вы не планируете запускать какие-либо примеры

кода с помощью этой библиотеки, я настоятельно рекомендую

установить ее и добавить в свой арсенал Python. Она дополняет

встроенную математическую библиотеку Python и имеет много

полезных функций, особенно для операций со списками чисел. 

По соглашению NumPy импортируется как np и может

использоваться следующим образом:

import numpy as np

 

numbers = \[100, 102, 98, 97, 103\]

print\(np.std\(numbers\)\)

print\(np.mean\(numbers\)\)

В этом примере вычисляется стандартное отклонение и

среднее 

арифметическое 

набора 

чисел, 

переданного

программе. 

**Обработка хорошо отформатированного текста**

Если повезет, то большая часть текста, который вам придется

обрабатывать, будет относительно чистой и хорошо

отформатированной. Такой текст обычно соответствует

![Image 67](images/000059.png)

нескольким требованиям, хотя грань между «грязным» и

«хорошо отформатированным» текстом может быть весьма

субъективной. 

В целом, хорошо отформатированный текст удовлетворяет

следующим требованиям:

• набран одним стандартным шрифтом \(без использования

курсива, рукописных и чрезмерно декоративных шрифтов\); 

• при копировании или фотографировании имеет очень четкие

края без артефактов копирования и темных пятен; 

• хорошо выровнен, без перекошенных букв; 

• не выходит за пределы изображения, не имеет обрезанных

краев или полей по краям изображения. 

Какие-то из этих моментов можно исправить в процессе

предварительной обработки. Например, преобразовать цвета в

оттенки серого, отрегулировать яркость и контрастность, а

само изображение при необходимости обрезать и повернуть. 

Однако некоторые фундаментальные ограничения могут

потребовать более глубокого обучения. Подробнее об этом см. 

в разделе «Чтение капчи и обучение Tesseract» данной главы на

с. 247. 

На рис. 13.1 показан идеальный пример хорошо

отформатированного текста. 

 

![Image 68](images/000009.png)

**Рис. 13.1. ** Образец текста, сохраненный в .tiff-файле и предназначенный для чтения с

помощью Tesseract

Чтобы прочитать этот файл и записать результаты в

текстовый файл, можно запустить Tesseract из командной

строки:

$ 

tesseract 

text.tif 

textoutput 

| 

cat

textoutput.txt

В результате получим строку с информацией о библиотеке

Tesseract, где говорится, что она работает, после чего следует

контент созданного файла textoutput.txt:

Tesseract Open Source OCR Engine v3.02.02 with Leptonica

This is some text, written in Arial, that will

be read by

Tesseract. Here are some symbols: \!@\#$%"&'\(\) Результат в основном точен, за исключением символов ^ и \*, которые были интерпретированы как двойные и одинарные

кавычки соответственно. Однако в целом это делает текст

более удобным для чтения. 

Если размыть текст изображения, добавить несколько

артефактов JPG-сжатия и небольшой градиент фона, то

результаты станут намного хуже \(рис. 13.2\). 

 

**Рис. 13.2. ** К сожалению, многие документы, которые вам встретятся в Интернете, будут

выглядеть приблизительно как этот пример

Tesseract справился с этим изображением далеко не так

хорошо, прежде всего из-за градиента фона, и выдал

следующий результат:

This is some text, written In Arlal, that" 

Tesseract. Here are some symbols: \_

Обратите внимание: текст обрезается сразу, как только

фоновый градиент затрудняет распознавание, и последний

символ в каждой строке неверен, поскольку Tesseract безуспешно пытается его интерпретировать. Кроме того, JPG-артефакты и размытость мешают Tesseract отличать строчные

i и прописные I от цифры 1. 

Именно здесь нам пригодится скрипт Python, с помощью

которого мы сначала очистим изображение. Используя

библиотеку Pillow, мы можем создать пороговый фильтр, который позволит избавиться от серого фона, выделить текст и

сделать изображение более четким для последующего чтения в

Tesseract. 

Далее вместо того, чтобы запускать Tesseract из командной

строки, мы можем использовать библиотеку pytesseract для

выполнения команд Tesseract и чтения полученного файла: from PIL import Image

import pytesseract

 

def cleanFile\(filePath, newFilePath\):

image = Image.open\(filePath\)

 

\# Устанавливаем пороговое значение для

изображения и сохраняем его. 

image = image.point\(lambda x: 0 if x < 143

else 255\)

![Image 69](images/000038.png)

image.save\(newFilePath\)

return image

 

image 

= 

cleanFile\('files/textBad.png', 

'files/textCleaned.png'\)

 

\# Вызываем tesseract, чтобы выполнить OCR

созданного изображения. 

print\(pytesseract.image\_to\_string\(image\)\)

Полученное изображение, автоматически сохраненное в

файле textCleaned.png, показано на рис. 13.3. 

 

**Рис. 13.3. ** Мы получили это изображение, пропустив исходное «грязное» изображение

через пороговый фильтр

За 

исключением 

некоторых 

трудночитаемых 

или

отсутствующих знаков пунк туации, текст читается — по

крайней мере для нас. Теперь Tesseract сделает все, что от него

зависит:

This us some text' written In Anal, that will be read by

Tesseract Here are some symbols: \!@\#$%"&'\(\) Точки и запятые, будучи чрезвычайно мелкими, стали

первыми жертвами этого изображения, походя друг на друга и

почти исчезая как для нас, так и для Tesseract. Вдобавок здесь

неудачное ошибочное распознавание слова Arial как Anal, 

поскольку Tesseract интерпретировал буквы r и i как одну

букву n. 

Тем не менее это гораздо лучше, по сравнению с

предыдущей версией, где была обрезана почти половина

текста. 

Главное слабое место Tesseract — разная яркость фона. 

Алгоритмы Tesseract пытаются автоматически отрегулировать

контрастность изображения перед чтением текста, но вам, скорее всего, удастся добиться лучших результатов, выполнив

это самостоятельно с помощью такого инструмента, как

библиотека Pillow. 

К 

изображениям, 

которые 

следует 

обязательно

откорректировать перед передачей в Tesseract, относятся

изображения с наклонным текстом, большими нетекстовыми

областями или другими проблемами. 

**Автоматическая коррекция изображений**

В предыдущем примере значение 143 было подобрано

экспериментально в качестве «идеального» порога для

преобразования всех пикселов изображения в черные или

белые для того, чтобы Tesseract смог прочесть текст, содержащийся в изображении. Но как быть, если изображений

много, у каждого свои проблемы с градациями серого и вы не

можете откорректировать их все вручную в разумных

пределах? 

Один из способов найти наилучшее \(или хотя бы достаточно

хорошее\) решение — запустить Tesseract для набора

изображений, где каждое из них откорректировано со своим

пороговым значением, и алгоритмически выбрать тот вариант, который демонстрирует наилучший результат, измеряемый

неким сочетанием количества символов и/или строк, которые

Tesseract может прочесть, и «достоверностью», с которой были

прочитаны эти символы. 

Выбор алгоритма может немного различаться в разных

приложениях, но вот один из вариантов перебора разных

пороговых параметров при обработке изображения с целью

найти «лучший» параметр:

import pytesseract

from pytesseract import Output

from PIL import Image

import numpy as np

 

def cleanFile\(filePath, threshold\):

image = Image.open\(filePath\)

\# Выбираем пороговое значение для

изображения и сохраняем его. 

image = image.point\(lambda x: 0 if x < threshold else 255\)

return image

 

def getConfidence\(image\):

data = pytesseract.image\_to\_data\(image, 

output\_type=Output.DICT\)

text = data\['text'\]

confidences = \[\]

numChars = \[\]

 

for i in range\(len\(text\)\):

if data\['conf'\]\[i\] > -1:

confidences.append\(data\['conf'\]\[i\]\)

numChars.append\(len\(text\[i\]\)\)

 



 

 

 

return 

np.average\(confidences, 

weights=numChars\), sum\(numChars\)

 

filePath = 'files/textBad.png' 

 

start = 80

step = 5

end = 200

 

for threshold in range\(start, end, step\):

image = cleanFile\(filePath, threshold\)

scores = getConfidence\(image\)

print\("threshold: " \+ str\(threshold\) \+ ", confidence: " 

\+ str\(scores\[0\]\) \+ " numChars " \+

str\(scores\[1\]\)\)

У этого скрипта есть две функции:

•cleanFile — принимает исходный «плохой» файл и

значение порога, с которым запускает пороговый

инструмент PIL. Обрабатывает файл и возвращает объект

изображения PIL; 

• getConfidence — принимает PIL-объект с очищенным

изображением и пропускает его через Tesseract. Вычисляет

среднюю достоверность каждой распознанной строки

\(взвешенной по количеству символов в этой строке\), а также

количество распознанных символов. 

Изменяя пороговое значение и вычисляя каждый раз

достоверность и количество распознанных символов, получим

следующий результат:

threshold: 80, confidence: 61.8333333333 nu

mChars 18

threshold: 85, confidence: 64.9130434783 nu

mChars 23

threshold: 90, confidence: 62.2564102564 nu

mChars 39

threshold: 95, confidence: 64.5135135135 nu

mChars 37

threshold: 100, confidence: 60.7878787879 n

umChars 66

threshold: 105, confidence: 61.9078947368 n

umChars 76

threshold: 110, confidence: 64.6329113924 n

umChars 79

threshold: 115, confidence: 69.7397260274 n

umChars 73

threshold: 120, confidence: 72.9078947368 n

umChars 76

threshold: 125, confidence: 73.582278481 nu

mChars 79

threshold: 130, confidence: 75.6708860759 n

umChars 79

threshold: 135, confidence: 76.8292682927 n

umChars 82

threshold: 140, confidence: 72.1686746988 n

umChars 83

threshold: 145, confidence: 75.5662650602 n

umChars 83

threshold: 150, confidence: 77.5443037975 n

umChars 79

threshold: 155, confidence: 79.1066666667 n umChars 75

threshold: 160, confidence: 78.4666666667 n

umChars 75

threshold: 165, confidence: 80.1428571429 n

umChars 70

threshold: 170, confidence: 78.4285714286 n

umChars 70

threshold: 175, confidence: 76.3731343284 n

umChars 67

threshold: 180, confidence: 76.7575757576 n

umChars 66

threshold: 185, confidence: 79.4920634921 n

umChars 63

threshold: 190, confidence: 76.0793650794 n

umChars 63

threshold: 195, confidence: 70.6153846154 n

umChars 65

Существует четкая тенденция как по средней достоверности

результата, так и по количеству распознанных символов. Оба

показателя стремятся к максимуму в районе порогового

значения 145, что близко к найденному вручную «идеальному»

результату 143. 

Пороговые значения от 140 до 145 дают максимальное

количество распознанных символов \(83\), но пороговое

значение 145 показывает наибольшую достоверность для этих

символов, поэтому мы можем выбрать данный результат и

вернуть текст, который был распознан с данным порогом как

«наилучшее предположение» того, какой именно текст

содержится в изображении. 

Конечно, простое распознавание «большинства» символов

еще не говорит о правильном распознавании всех этих

символов. При некоторых пороговых значениях Tesseract может разбивать отдельные символы на несколько или

интерпретировать случайный шум в изображении как

текстовый символ, которого на самом деле не существует. В

такой ситуации имеет смысл больше полагаться на среднюю

достоверность каждого результата. 

Например, предположим, что среди результатов оказались

\(в том числе\) следующие:

threshold: 

145, 

confidence: 

75.5662650602

numChars 83

threshold: 

150, 

confidence: 

97.1234567890

numChars 82

Тогда из этих двух вариантов, бесспорно, следует выбрать

тот, который дает более чем 20-процентное повышение

достоверности с потерей всего лишь одного символа, и

предположить, что результат с порогом 145 оказался просто

неверным, или, возможно, один символ разделился на два, или

же алгоритм нашел нечто, чего не было в тексте. 

Здесь 

имеет 

смысл 

предварительно 

немного

поэкспериментировать, чтобы подобрать наилучший порог для

алгоритма. Например, можно выбрать вариант, при котором

достигается максимальное *произведение* достоверности и

количества распознанных символов \(в таком случае для порога

145 это значение равно 6272, и в нашем воображаемом

примере побеждает пороговое значение 150, для которого это

произведение равно 7964\) или же какой-либо другой

показатель. 

Обратите внимание: такие алгоритмы выбора работают не

только с обычными threshold, но и с произвольными

значениями инструмента PIL. Кроме того, PIL позволяет делать

выбор на основе двух и более значений, варьируя их и выбирая

наилучший результат аналогичным образом. 

Очевидно, что такие алгоритмы выбора требуют больших

вычислительных затрат. Мы многократно запускаем PIL и

Tesseract для каждого изображения, в то время как, знай мы

«идеальные» пороговые значения заранее, пришлось бы

запускать эти программы только один раз. 

Учтите: углубившись в обработку изображений, вы можете

начать замечать закономерности в найденных «идеальных»

значениях. Вместо проверки всех пороговых значений от 80 до

200 на практике может потребоваться проверить только пороги

от 130 до 180. 

Вы можете даже применить другой подход и при первом

проходе устанавливать пороговые значения с шагом, например, 20, а затем использовать «жадный» алгоритм, чтобы

усовершенствовать наилучший результат, уменьшив шаг для

порогов между «лучшими» решениями, найденными на

предыдущей итерации. Такой вариант может оказаться

оптимальным и в случае выбора по нескольким переменным. 

**Веб-скрапинг текста, представленного в виде изображений на**

**сайтах**

Применение Tesseract для чтения текста из изображения, сохраненного на жестком диске, кому-то покажется не

особенно интересным, но при использовании веб-скрапера

может стать мощным инструментом. Иногда изображения, содержащие текст на сайтах, непреднамеренно вводят в

заблуждение \(как в случае меню на сайте ресторана в формате

JPG\), но часто их задействуют целенаправленно, чтобы

скрывать текст, как я покажу на следующем примере. 

Хоть в файле robots.txt сайта Amazon и стоит

разрешение на просмотр страниц товаров, предварительный

просмотр книг обычно недоступен для ботов. Это стало

возможным потому, что предварительный просмотр книг

загружается с помощью специальных скриптов Ajax, а

изображения тщательно скрываются под слоями тегов div. Для

обычного посетителя сайта эти изображения, пожалуй, больше

похожи на Flash-презентации, чем на файлы изображений. Но

даже если бы вы могли добраться до изображений, остается не

такой уж простой вопрос: как прочитать содержащийся в них

текст? 

Следующий скрипт совершает именно этот подвиг: переходит к крупноформатному изданию25 повести Л. Толстого

«Смерть Ивана Ильича», открывает раздел чтения книги, собирает URL изображений, а затем последовательно

скачивает, читает и выводит текст каждого из них. 

Обратите внимание: корректная работа этого кода зависит

от текущего списка товаров Amazon, а также от некоторых

архитектурных особенностей данного сайта. Если список

товаров исчезнет или будет изменен, то достаточно лишь

поставить в коде страницы предварительного просмотра URL

другой 

книги 

\(я 

считаю, 

что 

лучше 

выбирать

крупноформатные издания, набранные шрифтом без засечек\). 

Поскольку это относительно сложный код, который

опирается на несколько концепций, изложенных в

предыдущих главах, я добавила комментарии, чтобы вам было

проще понять происходящее ниже:

import time

from urllib.request import urlretrieve

from PIL import Image

import tesseract

from selenium import webdriver

 

def getImageText\(imageUrl\):

urlretrieve\(image, 'page.jpg'\)

p = subprocess.Popen\(\['tesseract', 

'page.jpg', 'page'\], 

stdout=subprocess.PIPE,stderr=subproces

s.PIPE\)

p.wait\(\)

f = open\('page.txt', 'r'\)

print\(f.read\(\)\)

 

\# создаем новый драйвер Selenium

driver 

=

webdriver.Chrome\(executable\_path='<Path 

to

chromedriver>'\)

 

driver.get\('https://www.amazon.com/Death-Ivan-

Ilyich'\\

'-Nikolayevich-Tolstoy/dp/1427027277'\)

time.sleep\(2\)

 

\# Нажимаем кнопку предварительного просмотра

книги. 

driver.find\_element\_by\_id\('imgBlkFront'\).click\(

\)

imageList = \[\]

 

\# Ждем, пока загрузится страница. 

time.sleep\(5\)

 

while 'pointer' in driver.find\_element\_by\_id\(

'sitbReaderRightPageTurner'\).get\_attribute\(

'style'\):

\# Листаем страницы, пока доступна стрелка

вправо. 

driver.find\_element\_by\_id\('sitbReaderRightP

ageTurner'\).click\(\)

time.sleep\(2\)

\# Получаем все загружаемые страницы \(может

загружаться

\# несколько страниц одновременно, 

\# но дубликаты не добавляются в множество\). 

 

 

 

 

pages 

=

driver.find\_elements\_by\_xpath\('//div\[@class=\\'p

ageImage\\'\]/div/img'\)

if not len\(pages\):

print\('No pages found'\)

for page in pages:

image = page.get\_attribute\('src'\)

print\('Found image: \{\}'.format\(image\)\)

if image not in imageList:

imageList.append\(image\)

getImageText\(image\)

driver.quit\(\)

Теоретически этот скрипт можно выполнять с любым веб-драйвером Sele nium, но я обнаружила, что в настоящее время

он наиболее надежно работает с Chrome. 

Как мы уже знаем по предыдущему опыту работы с

функцией чтения Tesseract, она выводит длинные отрывки из

книги в целом разборчиво, что видно из предварительного

просмотра первой главы:

Chapter I

 

During an Interval In the Melvmskl trial In the

large

building of the Law Courts the members and public

prosecutor met in \[van Egorowch Shebek's

private

room, where the conversation turned on the celebrated

Krasovski case. Fedor Vasillevich warmly

maintained

that it was not subject to their jurisdiction, 

Ivan

Egorovich maintained the contrary, while Peter

ivanowch, not havmg entered into the discussmn at

the start, took no part in it but looked through the

Gazette which had Just been handed in. 

 

"Gentlemen," he said, "Ivan Ilych has died\!" 

Однако во многих словах здесь есть очевидные ошибки, такие как Melvmsl вместо фамилии Melvinski и discusmn вместо

discussion. Многие подобные ошибки можно исправить, делая

предположения на основе списка словарных слов \(вероятно, с

дополнениями, основанными на соответствующих именах

собственных, таких как Melvinski\). 

Иногда ошибочным может оказаться целое слово, например:

it is he who is dead and not 126. 

В данном случае слово I \(«я»\) заменено цифрой 1. Здесь, в

дополнение к словарю слов, пригодится анализ цепей Маркова. 

Если какая-либо часть текста содержит крайне необычную

фразу \(and not 1\), то можно предположить, что на самом деле

это более распространенная фраза \(and not I\). 

Конечно, здесь помогает тот факт, что такие замены

символов выполняются по предсказуемым схемам: vi заменяется на w, а I — на 1. Если эти замены встречаются часто, то можно создать список и использовать его, чтобы

«пробовать» новые слова и фразы, выбирая наиболее разумное

решение. Подход может состоять в том, чтобы заменять

символы, часто распознаваемые некорректно, и задействовать

тот вариант, который соответствует слову в словаре или

является общепризнанной \(или наиболее распространенной\) *n*-

граммой. 

Если вы решите воспользоваться этим методом, то

обязательно 

прочитайте 

главу 

9, 

чтобы 

получить

дополнительную информацию о работе с текстом и обработке

естественного языка. 

Несмотря на то что текст в этом примере набран обычным

шрифтом без засечек и Tesseract, по идее, должен относительно

легко его распознавать, иногда небольшое дообучение

помогает повысить точность. В следующем разделе

обсуждается другой подход к решению задачи распознавания

искаженного 

текста, 

который 

потребует 

небольших

предварительных затрат времени. 

Если предоставить Tesseract большой набор изображений с

известными текстами, то можно «научить» программу гораздо

точнее и достовернее распознавать тот же шрифт в будущем, даже несмотря на периодически возникающие проблемы с

фоном и размещением текста на странице. 

**Чтение капчи и обучение Tesseract** Слово CAPTCHA знают все, но мало кому известно его

значение: *Completely Automated Public Turing Test to Tell* *Computers and Humans Apart* \(«полностью автоматизированный

публичный тест Тьюринга, позволяющий отличить компьютер

от человека»\). Эта неуклюжая аббревиатура намекает на

примерно такую же неуклюжую роль тестов капчи, которые

создают препятствия во вполне удобных веб-интерфейсах: и

люди, и роботы часто испытывают трудности, пытаясь

выполнить эти тесты. 

Тест Тьюринга впервые был описан Аланом Тьюрингом в

его работе 1950 года «Вычислительные машины и разум». В

этой статье автор описал систему, в которой человек общался

бы как с людьми, так и с программами искусственного

интеллекта через компьютерный терминал. Если в процессе

произвольной беседы человек не мог отличить другого

человека от программы с ИИ, то считалось, что эта программа

прошла тест Тьюринга, а искусственный интеллект, как

рассуждал автор, можно было бы считать по-настоящему

«мыслящим». 

Ирония состоит в том, что за последние 60 лет мы перешли

от использования этих тестов для проверки машин к их

применению для тестирования самих себя и получили

странные результаты. Недавно Google отказалась от известной

своей сложностью системы reCAPTCHA, во многом из-за ее

тенденции блокировать легальных пользователей сайта27. 

Другие капчи несколько проще. Например, в Drupal, распространенной системе управления контентом на основе

PHP, 

есть 

популярный 

модуль 

капчи

\(**https://www.drupal.org/project/captcha**\), способный

генерировать тестовые изображения разной степени

сложности. По умолчанию изображение выглядит так, как

показано на рис. 13.4. 

Почему людям и машинам гораздо легче прочитать этот

текст капчи, по сравнению с другими? 

• Символы не перекрываются и не пересекают границ

пространства друг друга по горизонтали. Другими словами, вокруг каждого символа можно нарисовать правильный

прямоугольник, который не будет пересекать другие

символы. 

• Нет фоновых изображений, линий и другого отвлекающего

мусора, который бы вводил в заблуждение OCR-программы. 

• Для данного изображения это не очевидно, но в капче

используется всего несколько шрифтов. В тексте чередуется

чистый шрифт без засечек \(на рисунке это символы 4 и M\) и

шрифт, похожий на рукописный \(символы m, C и 3\). 

• Высокий контраст между белым фоном и темными

символами. 

![Image 70](images/000069.png)

 

**Рис. 13.4. ** Пример текстового изображения, используемого по умолчанию в проекте Drupal CAPTCHA

Тем не менее в этом изображении капчи есть несколько

помех, которые затрудняют чтение текста OCR-программами:

• используются не только буквы, но и цифры, что увеличивает

количество потенциально возможных символов; 

• буквы со случайным наклоном легко читаются людьми, но

могут сбить с толку OCR-программы; 

• странноватый рукописный шрифт вызывает особые

трудности, а пара дополнительных линий в C и 3 и необычно

маленькая строчная буква m требуют дополнительного

обучения компьютера. 

Запустив Tesseract для этого изображения с помощью

команды:

![Image 71](images/000019.png)

$ tesseract captchaExample.png output

получим следующий файл output.txt:

4N\\,,,C<3

Программа правильно распознала символы 4, C и 3, но

определенно не сможет в обозримом будущем заполнить поле

формы, защищенное этим изображением капчи. 

**Обучение Tesseract. ** Чтобы обучить Tesseract распознавать

написанное, будь то неясный и трудночитаемый шрифт или

капча, нужно предоставить программе несколько примеров

каждого символа. 

Здесь самое время включить хороший подкаст или фильм, поскольку это пара часов довольно скучной работы. Первым

шагом будет скачивание нескольких примеров ваших

изображений капчи в один каталог. Количество примеров, которые вы подберете, будет зависеть от сложности капчи; я

использовала 100 образцов файлов \(всего 500 символов, или в

среднем около восьми примеров на каждый символ\); в моем

случае для обучения на капче это, похоже, сработало неплохо. 

Рекомендую присваивать изображениям имена по тому тексту

капчи, который в них содержится \(например, 4MmC3.jpg\). Я

обнаружила, что это помогает быстро оценить ошибки сразу

для большого количества файлов; достаточно просмотреть все

миниатюры файлов и сравнить изображения с именами. Это

также очень помогает при проверке ошибок на последующих

этапах. 

Второй шаг — объяснить Tesseract, что именно означает

каждый символ и где он находится в изображении. Сюда

входит создание box-файлов, по одному на каждое

изображение капчи. Box-файл выглядит так:

4 15 26 33 55 0

M 38 13 67 45 0

m 79 15 101 26 0

C 111 33 136 60 0

3 147 17 176 45 0

Первый символ — этот символ в изображении, следующие

четыре числа — координаты прямоугольника, в который

заключено изображение этого символа, а последнее число —

номер 

страницы, 

используемой 

при 

обучении

многостраничных документов \(в нашем случае 0\). 

Очевидно, эти box-файлы не очень-то приятно создавать

вручную, но здесь помогут различные инструменты. Мне

нравится онлайн-инструмент Tesseract OCR Chopper, поскольку

он не требует установки или дополнительных библиотек, работает на любом компьютере, где есть браузер, и

относительно прост в использовании. Загрузите изображение, нажмите внизу кнопку **Add** \(Добавить\), если вам нужны

дополнительные box-файлы, отрегулируйте их размер при

необходимости и скопируйте и вставьте текст в новый box-файл. 

Box-файлы нужно сохранять как обычный текст, в файлах с

расширением .box. Аналогично файлам изображений их

удобно именовать в соответствии с решениями капчи, которые

они представляют \(например, 4MmC3.box\). Это также

облегчает перепроверку и сравнение текстового контента в

box-файлах 

с 

содержимым 

одноименного 

файла 

с

изображением, если упорядочить файлы в каталоге по именам. 

Аналогично случаям с изображениями вам придется

создать примерно 100 таких файлов, чтобы получить

достаточно данных. Кроме того, Tesseract может отбросить

некоторые файлы как нечитаемые, поэтому может

понадобиться папка с резервными копиями. Если вы

обнаружите, что результаты распознавания не так хороши, как

хотелось бы, или у Tesseract возникнут трудности с

некоторыми символами — это хороший момент для отладки

процесса, когда можно создать дополнительные обучающие

данные и повторить попытку. 

После того как вы создадите папку данных с box-файлами и

файлами изображений, скопируйте эти данные в резервную

папку, прежде чем делать с ней какие-либо дальнейшие

манипуляции. Выполняя обучающие скрипты с этими

данными, программа вряд ли что-нибудь удалит, однако лучше

подстраховаться, чем потом сожалеть о многих часах, потраченных на создание потерянных box-файлов. Кроме того, полезно иметь возможность удалить «грязный» каталог со

скомпилированными данными, чтобы повторить попытку. 

Для полного анализа данных и создания обучающих

файлов, необходимых для Tesseract, нужно выполнить еще

полдюжины шагов. Есть инструменты, способные сделать это

за вас, если предоставить им соответствующие исходные

изображения и box-файлы, однако, к сожалению, на данный

момент для Tesseract 3.02 таких инструментов не существует. 

Я 

написала 

решение 

на 

Python

\(**https://github.com/REMitchell/tesseract-trainer**\), которое

обрабатывает файл, содержащий файлы изображений и box-файлы, и автоматически создает все файлы, необходимые для

обучения. 

Исходные параметры и операции, которые выполняет эта

программа, содержатся в методах \_\_init\_\_ и runAll класса:

def \_\_init\_\_\(self\):

languageName = 'eng' 

fontName = 'captchaFont' 

directory = '<path to images>' 

 

def runAll\(self\):

self.createFontFile\(\)

self.cleanImages\(\)

self.renameFiles\(\)

self.extractUnicode\(\)

self.runShapeClustering\(\)

self.runMfTraining\(\)

self.runCnTraining\(\)

self.createTessData\(\)

Здесь вам нужно определить значения всего трех простых

переменных:

•languageName — трехбуквенный код языка, который

используется в Tesseract, чтобы понять, к какому языку

относится текст. В большинстве случаев вы, скорее всего, будете применять код eng, что означает «английский язык»; 

• fontName — название выбранного шрифта. Это может быть

что угодно, но оно должно быть одним словом без пробелов; 

**• directory** — каталог, содержащий все изображения и box-файлы. Я рекомендую указать абсолютный путь; если же вы

используете относительный, то он должен касаться того

каталога, откуда вы запускаете код Python. В случае

абсолютного пути можно запускать код из любого каталога

на компьютере. 

Рассмотрим используемые здесь функции. 

Функция createFontFile создает необходимый файл

font\_properties, куда Tesseract заносит знания о новом

шрифте, который мы создаем:

captchaFont 0 0 0 0 0

В этом файле содержится имя шрифта, после которого стоят

единицы и нули, указывающие на то, следует ли рассматривать

для данного шрифта курсив, полужирный или другие варианты

начертания. \(Обучение шрифтов с этими свойствами —

интересное упражнение, но, к сожалению, выходит за рамки

данной книги.\)

Функция cleanImages создает высококонтрастные версии

всех найденных файлов с изображениями, преобразует их в

оттенки серого и выполняет другие операции, облегчающие

чтение OCR-программами файлов с изображениями. Если мы

имеем дело с изображениями капчи, содержащими визуальный

«мусор», который легко отфильтровывается при постобработке, то ее можно добавить именно здесь. 

Функция renameFiles присваивает всем box-файлам и

соответствующим файлам изображений имена, необходимые

для обработки в Tesseract \(здесь числа в именах файлов — это

последовательные номера, позволяющие отличать файлы друг

от друга\):

•<languageName>.<fontName>.exp<fileNumber>.box; 

• <languageName>.<fontName>.exp<fileNumber>.tiff. 

Функция extractUnicode просматривает все созданные

box-файлы и формирует общее множество символов, 

доступных для обучения. Получившийся файл в формате

Unicode сообщит нам о том, сколько разных символов было

найдено. Это может быть хорошим способом быстро

определить, не упустили ли мы что-нибудь. 

Следующие 

три 

функции, 

runShapeClustering, 

runMfTraining 

и 

runCtTraining, 

создают 

файлы

shapetable, pfftable и normproto соответственно. Все они

предоставляют информацию о геометрии и форме каждого

символа, а также статистическую информацию, которую

Tesseract использует для вычисления вероятности того, что

данный символ относится к тому или иному типу. 

Наконец, Tesseract переименовывает каждый каталог со

скомпилированными данными так, чтобы его имя начиналось

с 

обозначения 

соответствующего 

языка 

\(например, 

shapetable превратится в eng.shapetable\), и компилирует

все содержащиеся в них файлы в итоговый файл обучающих

данных под названием eng.traineddata. 

Единственное, что вам придется сделать вручную, — это

переместить созданный файл eng.traineddata в корневой

каталог tessdata с помощью следующих команд в Linux и

Mac:

$cp 

/path/to/data/eng.traineddata

$TESSDATA\_PREFIX/tessdata

Если вы выполните эти операции, то у вас не должно

возникнуть проблем с распознаванием капчи того типа, для

которого вы обучили Tesseract. Теперь, когда вы дадите

библиотеке задание прочитать изображение из нашего

примера, то получите правильный ответ:

$ 

tesseract 

captchaExample.png 

output|cat

output.txt

4MmC3

Это успех\! Значительно лучше, по сравнению с предыдущей

интерпретацией изображения как 4N\\,,,C<3. 

Это был лишь краткий обзор всех возможностей обучения и

распознавания шрифтов с помощью Tesseract. Если вы

заинтересованы в более тщательном обучении Tesseract или, возможно, в создании собственной библиотеки обуча ющих

файлов для распознавания капчи либо хотите подарить миру

новые возможности распознавания шрифтов, то рекомендую

изучить 

документацию 

по 

Tesseract

**https://code.google.com/p/tesseract-ocr/wiki/TrainingTesseract3**. 

**Получение капчи и отправка решений**

Многие популярные системы управления контентом часто

страдают от спама, поскольку боты, запрограммированные на

известное 

расположение 

их 

страниц 

регистрации

пользователей, заваливают эти системы информацией о

регистрации. Например, на сайте **http://pythonscraping.com** даже

капча \(по общему мнению, слабая\) мало помогает сдержать

наплыв регистрационных данных. 

Как же ботам это удается? Мы успешно справились с

изображениями капчи, расположенными на локальном

жестком диске, но как создать полностью функциональный

бот? В этом разделе будут объединены многие приемы, описанные в предыдущих главах. Если вы еще не прочитали

главу 10, то хотя бы бегло просмотрите ее. 

Большинство изображений капчи обладают следующими

свойствами. 

• Это динамически генерируемые изображения, создаваемые

программой на сервере. Их источники могут быть

непохожими на обычные изображения и иметь вид

наподобие 

<imgsrc="WebForm.aspx? 

id=8AP85CQKE9TJ">, 

но их можно скачивать и

обрабатывать как любые другие изображения. 

• Правильный ответ на тестовое изображение хранится в базе

данных на сервере. 

• Многие изображения капчи имеют ограничение по времени

и не позволяют думать слишком долго. Обычно для ботов

это не проблема, но постановка решений капчи в очередь

или другие методики, способные вызвать задержку между

запросом капчи и отправкой решения, могут оказаться

неудачными. 

Обычно подход заключается в том, чтобы скачать файл с

изображением капчи на локальный диск, очистить это

изображение, использовать Tesseract для его анализа и вернуть

решение в виде соответствующего параметра формы. 

Я создала по адресу **http://pythonscraping.com/humans-only** страницу с формой комментариев, защищенной капчей, чтобы

написанному мною боту было с чем сражаться. Он использует

библиотеку Tesseract, запускаемую из командной строки, а не

из оболочки pytesseract \(хотя вполне можно использовать

любой вариант\), и выглядит так:

from urllib.request import urlretrieve

from urllib.request import urlopen

from bs4 import BeautifulSoup

import subprocess

import requests

from PIL import Image

from PIL import ImageOps

 

def cleanImage\(imagePath\):

image = Image.open\(imagePath\)

image = image.point\(lambda x: 0 if x<143

else 255\)

 

 

 

 

borderImage 

=

ImageOps.expand\(image,border=20,fill='white'\)

borderImage.save\(imagePath\)

 

html 

=

urlopen\('http://www.pythonscraping.com/humans-

only'\)

bs = BeautifulSoup\(html, 'html.parser'\)

\# Собираем заполненные перед этим значения

формы. 

imageLocation = bs.find\('img', \{'title': 'Image

CAPTCHA'\}\)\['src'\]

formBuildId 

= 

bs.find\('input', 

\{'name':'form\_build\_id'\}\)\['value'\]

captchaSid 

= 

bs.find\('input', 

\{'name':'captcha\_sid'\}\)\['value'\]

captchaToken 

= 

bs.find\('input', 

\{'name':'captcha\_token'\}\)\['value'\]

 

captchaUrl 

=

'http://pythonscraping.com'\+imageLocation

urlretrieve\(captchaUrl, 'captcha.jpg'\)

cleanImage\('captcha.jpg'\)

p 

= 

subprocess.Popen\(\['tesseract', 

'captcha.jpg', 'captcha'\], stdout=

subprocess.PIPE,stderr=subprocess.PIPE\)

p.wait\(\)

f = open\('captcha.txt', 'r'\)

 

\# Удаляем все пробельные символы. 

captchaResponse 

= 

f.read\(\).replace\(' 

', 

''\).replace\('\\n', ''\)

print\('Captcha 

solution 

attempt:

'\+captchaResponse\)

 

if len\(captchaResponse\) == 5:

params = \{'captcha\_token':captchaToken, 

'captcha\_sid':captchaSid, 

'form\_id':'comment\_node\_page\_form

', 'form\_build\_id': formBuildId, 

'captcha\_response':captchaRespons

e, 'name':'Ryan Mitchell', 

'subject': 'I come to seek the

Grail', 

'comment\_body\[und\]\[0\]\[value\]':

'...and I am definitely not a

bot'\}

 

 

 

 

r 

=

requests.post\('http://www.pythonscraping.com/co

mment/reply/10', 

data=params\)

responseObj = BeautifulSoup\(r.text, 

'html.parser'\)

 

 

 

 

if 

responseObj.find\('div', 

\{'class':'messages'\}\) is not None:

print\(responseObj.find\('div', 

\{'class':'messages'\}\).get\_text\(\)\)

else:

print\('There was a problem reading the CAPTCHA correctly\!'\)

Обратите внимание: этот скрипт завершается ошибкой в

двух случаях — если Tesseract не извлек из изображения ровно

пять символов \(поскольку мы знаем, что все допустимые

решения для данной капчи должны состоять именно из пяти

символов\) или же если форма была отправлена, но ответ на

капчу был неверным. Первое происходит примерно в 50 %

случаев, и здесь не приходится заниматься отправкой формы

— программа завершается с сообщением об ошибке. Второе

происходит приблизительно в 20 % случаев при общей

точности около 30 % \(или около 80 % точности для каждого из

пяти символов\). 

Такая точность может показаться низкой, однако имейте в

виду: обычно пользователь не ограничен в количестве попыток

распознать капчу и от большинства этих неправильных

попыток можно отказаться, не отправляя саму форму. При

отправке формы капча в большинстве случаев распознается

точно. Если и это вас не убедило, то примите во внимание, что

показатель точности для простого угадывания составляет

0,0000001 %. Выполнив вместо угадывания данную программу

три или четыре раза, вы сэкономите время в 900 миллионов

раз\! 

24 См.: *Хобсон Л., Ханнес Х., Коул Х. * Обработка естественного языка в действии. —

СПб.: Питер, 2020. 

25 При обработке текста, на котором он не обучался, Tesseract намного лучше

справляется с крупноформатными изданиями книг, особенно если изображения

небольшие. В следующем разделе будет показано, как обучить Tesseract распознавать

различные шрифты, чтобы программа могла читать шрифты гораздо меньшего

размера, в том числе при предварительном просмотре книг небольшого формата\! 

26 Перевод на английский фразы: «умер он, а не я», где вместо «я» \(I\) по ошибке

стоит цифра 1. — *Примеч. пер. *

27 См. https://gizmodo.com/google-has-finally-killed-the-captcha-1793190374. 

**Глава 14. Как избежать ловушек веб-скрапинга**

Мало что огорчает больше, чем выполнить веб-скрапинг сайта, просмотреть результаты и обнаружить отсутствие в них

данных, которые вы определенно видели в браузере. Или

отправить абсолютно правильно заполненную форму, которую

потом отклонит веб-сервер. Или получить сообщение о том, что ваш IP-адрес заблокирован сайтом по неизвестным

причинам. 

Это лишь часть самых трудных ошибок, которые

приходится устранять, — не только потому, что они порой

совершенно неожиданные \(скрипт, отлично работавший на

одном сайте, может вообще не действовать на другом, на вид

практически таком же\), но и потому, что эти ошибки

намеренно 

не 

сопровождаются 

сколько-нибудь

содержательными сообщениями об ошибках или данными

трассировки стека, которыми можно было бы воспользоваться. 

Вас идентифицировали как бот, отклонили, и вы не знаете

почему. 

В этой книге я описала много способов, позволяющих

выполнять различные сложные задачи на сайтах \(отправлять

формы, извлекать и очищать сложные данные, выполнять

скрипты JavaScript и т.д.\). Эта глава является чем-то вроде

сборника в том смысле, что описанные здесь методы касаются

самых разных аспектов \(заголовки HTTP, стили CSS и HTML-формы — лишь некоторые из них\). Однако у всех них есть кое-что общее: они призваны преодолеть препятствие, установленное с единственной целью — предотвратить

автоматический веб-скрапинг сайта. 

Независимо от того, насколько эта информация будет вам

полезна в данный момент, я настоятельно рекомендую хотя бы

прочитать главу. Вы никогда не знаете, когда это поможет вам

устранить сложную ошибку или вообще предотвратить

проблему. 

**Этический момент**

В первых нескольких главах мы затронули правовую «серую

зону», где обитают веб-скраперы, а также ряд этических

принципов, которыми следует руководствоваться. Честно

говоря, эта глава с этической точки зрения является для меня, пожалуй, самой сложной. Мои сайты тоже страдают от ботов, спамеров, 

веб-скраперов 

и 

других 

всевозможных

нежелательных виртуальных гостей — как, возможно, и ваши. 

Так зачем же учить людей, как сделать боты еще лучше? 

Я считаю, что эту главу важно было включить в книгу по

следующим причинам. 

• Существуют в высшей степени этичные, юридически

обоснованные причины для веб-скрапинга некоторых

сайтов, не желающих, чтобы их обработал веб-скрапер. 

Моей 

предыдущей 

задачей 

веб-скрапинга 

был

автоматический сбор информации с сайтов, которые

публиковали в Интернете имена, адреса, номера телефонов

и другую личную информацию клиентов без их согласия. Я

использовала собранную информацию, чтобы делать

официальные запросы на эти сайты с требованием удалить

данную информацию. Во избежание конкуренции эти сайты

бдительно охраняют свою информацию от веб-скраперов. 

Тем не менее моя работа по обеспечению анонимности

клиентов моей компании \(часть которых подвергалась

преследованиям, являлась жертвой домашнего насилия или

имела другие очень веские причины для того, чтобы не

привлекать к себе внимания\) убедительно доказала

необходимость веб-скрапинга, и я была благодарна судьбе за

то, что у меня были навыки, требуемые для выполнения

этой работы. 

• Практически невозможно создать сайт, «защищенный от веб-скраперов» \(который при этом был бы доступен для

обычных пользователей\), однако я все же надеюсь, что

информация, изложенная в этой главе, поможет тем, кто

хотел бы защитить свои сайты от вредоносных атак. Я

постоянно буду указывать на слабые стороны каждого

метода веб-скрапинга, которые вы сможете применять для

защиты своего сайта. Имейте в виду: большинство сетевых

ботов сегодня просто широко собирают информацию и

находят уязвимости, так что использование всего

нескольких простых методов, описанных в данной главе, скорее всего, станет помехой для 99 % из них. Тем не менее

эти боты с каждым месяцем становятся все более

изощренными, и лучше быть готовыми. 

• Как и большинство программистов, я не верю, что отказ от

изучения какой-либо информации — это хорошо. 

Читая эту главу, имейте в виду: многие из описанных здесь

скриптов вовсе не обязательно использовать для каждого

сайта, который вам встретится. Поступать так не просто

нехорошо — вы можете получить письмо-предупре ждение о

возможных санкциях или кое-что похуже \(подробнее о том, что

делать, если вы все же получите такое письмо, см. в главе 18\). 

Но я не собираюсь тыкать вас в это носом всякий раз, когда мы

будем обсуждать новую методику. Поэтому отложим данный

вопрос до конца главы. 

**Выдать скрипт за человека**

Фундаментальная проблема для сайтов, которые хотят

избежать веб-скрапинга, состоит в том, чтобы выяснить, как

отличать боты от людей. Несмотря на то что многие методы, используемые сайтами \(например, капчи\), сложно обмануть, все же есть несколько довольно простых вещей, позволяющих

вашему боту стать более похожим на человека. 

**Настройте заголовки**

На протяжении всей этой книги мы использовали Python-библиотеку Requests для создания, отправки и получения

HTTP-запросов, таких как обработка форм на сайте в главе 10. 

Библиотека Requests отлично подходит и для определения

заголовков. HTTP-заголовки — это списки атрибутов или

предпочтений, которые передаются на веб-сервер вместе с

каждым запросом. В протоколе HTTP определено несколько

десятков странных типов заголовков, большинство из которых

обычно не используется. Однако следующие семь полей

постоянно применяются в большинстве основных браузеров

при инициализации любого соединения \(ниже показан пример

данных из моего собственного браузера\):

Host https://www.google.com/

Connection keep-alive

Accept text/html,application/xhtml\+xm

l,application/xml;q=0.9,image/

webp,\*/\*;q=0.8

User-Agent Mozilla/5.0 \(Macintosh; Intel

Mac OS X 10\_9\_5\)

AppleWebKit/537.36 \(KHTML, 

like Gecko\) Chrome/

39.0.2171.95 Safari/537.36

Referrer https://www.google.com/

Accept-Encoding gzip, deflate, sdch

Accept-Language en-US,en;q=0.8

А вот заголовки, которые отправляет по умолчанию

типичный веб-скрапер Python, написанный с помощью

библиотеки urllib:

Accept-Encoding identity

User-Agent Python-urllib/3.4

Если бы вы были администратором сайта и хотели бы

заблокировать веб-скраперы, то какой из этих заголовков

пропустили бы с большей вероят ностью? 

К счастью, заголовки можно полностью изменить с

помощью библиотеки Requests. Для тестирования свойств

браузера, доступных для просмотра веб-сервером, отлично

подходит 

сайт 

**https://www.whatismybrowser.com**. 

Чтобы

проверить настройки cookie, мы выполним веб-скрапинг этого

сайта, применив следующий скрипт:

import requests

from bs4 import BeautifulSoup

 

session = requests.Session\(\)

headers 

= 

\{'User-Agent':'Mozilla/5.0

\(Macintosh; Intel Mac OS X 10\_9\_5\)' 

'AppleWebKit 537.36 \(KHTML, like

Gecko\) Chrome', 

'Accept':'text/html,application/xhtm

l\+xml,application/xml;' 

'q=0.9,image/webp,\*/\*;q=0.8'\}

url = 'https://www.whatismybrowser.com/'\\

'developers/what-http-headers-is-my-

browser-sending' 

req = session.get\(url, headers=headers\)

bs = BeautifulSoup\(req.text, 'html.parser'\)

print\(bs.find\('table', 

\{'class':'table-

striped'\}\).get\_text\)

Результат выполнения этого скрипта должен показать, что

теперь заго ловки соответствуют тем, которые заданы в коде, в

словарном объекте headers. 

Хотя сайты способны проверять, является ли посетитель

человеком, на основе любых свойств в заголовках HTTP, я

обнаружила, что, как правило, единственный параметр, который действительно имеет значение, — это User-Agent. 

Поэтому, над каким бы проектом вы ни работали, имеет смысл

присвоить ему какое-нибудь менее подозрительное значение, чем Python-urllib/3.4. Кроме того, если вы когда-либо

столкнетесь с чрезвычайно бдительным сайтом, то заполнение

одного из часто используемых, но редко проверяемых

заголовков, таких как Accept-Language, может оказаться

решающим условием для того, чтобы вас приняли за человека. 

**То, как вы видите мир, зависит от заголовков**

Предположим, вы хотите написать основанную на машинном

обучении программу-переводчик для исследовательского

проекта и для ее тестирования вам нужны большие объемы

переведенных текстов. Многие крупные сайты предоставляют

различные переводы одного и того же контента, в

зависимости от языковых предпочтений, указанных в

заголовках. Просто изменив в заголовке AcceptLanguage:en-US на AcceptLanguage:fr, вы можете

получить перевод на французском, если сайт поддерживает

такую 

возможность 

\(практически 

беспроигрышным

вариантом являются крупные международные компании\). 

Заголовки также могут подсказать сайту, что следует изменить

формат представляемого ими контента. Например, при

доступе в Интернет с мобильных устройств часто

предоставляются урезанные версии сайтов, без рекламных

баннеров, Flash-анимации и других отвлекающих элементов. 

Если вы попытаетесь заменить значение параметра User-Agent примерно на следующее, то, возможно, обнаружите, что задача веб-скрапера немного упростится\! 

User-Agent:Mozilla/5.0 \(iPhone; CPU iPhone

OS 7\_1\_2 like Mac OS X\)

AppleWebKit/537.51.2 \(KHTML, like Gecko\)

Version/7.0 Mobile/11D257

Safari/9537.53

**Обработка данных cookie с помощью JavaScript**

Правильная обработка данных cookie может смягчить многие

проблемы веб-скрапинга, хотя у этой медали есть и обратная

сторона. Сайты, которые отслеживают ваши перемещения по

сайту с помощью данных cookie, могут попытаться выбросить с

сайта веб-скрапер, если он будет демонстрировать

ненормальное для человека поведение — например, слишком

быстро заполнять формы или посещать слишком много

страниц. Конечно, такое поведение можно замаскировать, 

разорвав и снова установив соединение с сайтом или даже

изменив свой IP-адрес \(подробнее о том, как это сделать, см. в

главе 17\), однако если данные cookie будут выдавать вас с

головой, то все попытки маскировки окажутся тщетными. 

Данные cookie также могут оказаться необходимыми для

веб-скрапинга сайта. Как показано в главе 10, чтобы оставаться

на сайте, следует иметь возможность сохранять данные cookie и представлять их, переходя на новую страницу. Некоторые

сайты даже не требуют явной аутентификации и получения

каждый раз новых данных cookie — достаточно, посещая сайт, сохранить старую копию данных cookie, полученных после

аутентификации. 

При веб-скрапинге только одного или небольшого

количества определенных сайтов я рекомендую изучить

данные cookie, генерируемые этими сайтами, и подумать, какие из них должен поддерживать ваш веб-скрапер. Есть

разные подключаемые модули браузеров, которые показывают, как устанавливаются данные cookie при посещении сайта и

перемещении по нему. Один из моих любимых таких модулей

— 

расширение 

Chrome 

EditThisCookie

\(**http://www.editthiscookie.com/**\). 

Для получения дополнительной информации об обработке

данных cookie с помощью библиотеки Requests еще раз изучите

примеры кода в разделе «Обработка данных авторизации и

параметров cookie» в главе 10 на с. 191. Конечно, из-за

невозможности выполнить JavaScript библиотека Requests не в

состоянии обрабатывать многие данные cookie, создаваемые

современными программами слежения наподобие Google Analytics. Такие данные cookie создаются только после

выполнения скриптов на стороне клиента \(или иногда на

основании событий страницы, таких как нажатия кнопок при

просмотре страницы\). Чтобы справиться с этим, нужно

использовать пакеты Selenium и Chrome WebDriver \(их

установку и основы применения мы рассмотрели в главе 11\). 

Чтобы просмотреть данные cookie, можно посетить любой

сайт \(в данном примере **http://pythonscraping.com**\) и вызвать в

веб-драйвере функцию get\_cookies\(\):

from selenium import webdriver

from selenium.webdriver.chrome.options import

Options

chrome\_options = Options\(\)

chrome\_options.add\_argument\('--headless'\)

driver = webdriver.Chrome\(

executable\_path='drivers/chromedriver', 

options=chrome\_options\)

driver.get\('http://pythonscraping.com'\)

driver.implicitly\_wait\(1\)

print\(driver.get\_cookies\(\)\)

В результате получим вполне типичный для Google Analytics массив cookie:

\[\{'value': '1', 'httponly': False, 'name':

'\_gat', 'path': '/', 'expiry': 1422806785, 

'expires': 'Sun, 01 Feb 2015 16:06:25 GMT', 

'secure': 

False, 

'domain':

'.pythonscraping.com'\}, 

\{'value':

'GA1.2.1619525062.1422806186', 

'httponly':

False, 'name': '\_ga', 'path': '/', 'expiry': 1485878185, 'expires': 'Tue, 31 Jan 2017

15:56:25 GMT', 'secure': False, 'domain':

'.pythonscraping.com'\}, 

\{'value': 

'1', 

'httponly': False, 'name': 'has\_js', 'path':

'/', 'expiry': 1485878185, 'expires': 'Tue, 31

Jan 2017 15:56:25 GMT', 'secure': False, 

'domain': 'pythonscraping.com'\}\]

Для управления данными cookie можно воспользоваться

функциями 

delete\_cookie\(\), 

add\_cookie\(\) 

и

delete\_all\_cookies\(\). Кроме того, можно сохранять

данные cookie для применения в других веб-скраперах. Вот

пример того, как совместно использовать эти функции: from selenium import webdriver

from selenium.webdriver.chrome.options import

Options

 

chrome\_options = Options\(\)

chrome\_options.add\_argument\('--headless'\)

 

driver = webdriver.Chrome\(

executable\_path='drivers/chromedriver', 

options=chrome\_options\)

driver.get\('http://pythonscraping.com'\)

driver.implicitly\_wait\(1\)

 

savedCookies = driver.get\_cookies\(\)

print\(savedCookies\)

 

driver2 = webdriver.Chrome\(

executable\_path='drivers/chromedriver', 

options=chrome\_options\)

 

driver2.get\('http://pythonscraping.com'\)

driver2.delete\_all\_cookies\(\)

for cookie in savedCookies:

driver2.add\_cookie\(cookie\)

 

driver2.get\('http://pythonscraping.com'\)

driver.implicitly\_wait\(1\)

print\(driver2.get\_cookies\(\)\)

В этом примере первый веб-драйвер получает сайт, выводит данные cookie, а затем сохраняет их в переменной

savedCookies. Второй веб-драйвер загружает тот же сайт, удаляет собственные данные cookie и добавляет полученные от

первого веб-драйвера. 

Обратите внимание: второй веб-драйвер, прежде чем

добавить данные cookie, должен сначала загрузить сайт. Таким

образом, Selenium знает, к какому домену принадлежат данные

cookie, даже если загрузка сайта ничего не дала веб-скраперу. 

После этого второй веб-драйвер должен получить те же

данные cookie, что и первый. Согласно Google Analytics теперь

второй веб-драйвер идентичен первому и они будут

отслеживаться 

одинаково. 

Если 

первый 

прошел

аутентификацию на сайте, то второй тоже будет

аутентифицирован. 

**Своевременность — наше все**

Некоторые хорошо защищенные сайты могут препятствовать

отправке форм или другому взаимодействию с сайтом, если

пользователь делает это слишком быстро. Но даже при

отсутствии подобных мер безопасности все равно скачивать с

сайта слишком много информации и делать это значительно

быстрее, чем может обычный человек, — хороший способ

добиться того, чтобы вас заметили и заблокировали. 

Многопоточное программирование — отличный способ

загружать страницы быстрее, так что в одном потоке

обрабатываются данные, а в другом потоке многократно

загружаются страницы. Однако это кошмарный метод, если вы

хотите написать хороший веб-скрапер. Всегда следует

стремиться загружать как можно меньше отдельных страниц и

делать минимум запросов данных. По возможности старайтесь

растянуть этот процесс на несколько секунд, даже если

придется добавить в код такие строки:

import time

 

time.sleep\(3\)

Нужны ли вам эти дополнительные несколько секунд между

загрузками страниц, часто определяется экспериментально. 

Мне неоднократно приходилось с боем выцарапывать данные с

сайта, каждые несколько минут доказывая, что «я не робот»

\(проходя тест капчи вручную и вставляя полученные данные

cookie обратно в веб-скрапер, чтобы тот продолжал работу с

сайтом как «доказавший, что он человек»\). И лишь добавив

time.sleep, я решила свои проблемы, и сайт позволил мне

выполнять веб-скрапинг до бесконечности. 

Иногда приходится замедляться, чтобы двигаться быстрее\! 

**Основные средства защиты форм**

Есть много проверенных тестов, вот уже многие годы более или

менее надежно позволяющих отличить веб-скрапер от

человека, использующего браузер. Не проблема, если бот

скачивает статьи и публикации в блогах, которые все равно

являются общедоступными. Проблема, когда бот создает

тысячи пользовательских учетных записей и начинает

рассылать спам всем, кто зарегистрирован на вашем сайте. 

Веб-формы, особенно связанные с созданием учетных записей

и аутентификацией, представляют значительную угрозу

безопасности и вычислительным ресурсам, если уязвимы для

произвольного использования ботами. Поэтому владельцы

сайтов в высшей степени заинтересованы постараться

ограничить доступ к их сайту \(или по крайней мере им

кажется, что доступ ограничен\). 

Эти меры защиты от ботов, основанные на формах и

аутентификации, могут стать серьезной проблемой для веб-скраперов. 

Примите во внимание, что в этом обзоре кратко

рассмотрена лишь часть мер безопасности, с которыми вы

можете столкнуться при создании автоматических ботов для

этих форм. Подробнее о взаимодействии с хорошо

защищенными формами см. в главе 13, посвященной капче и

обработке изображений, а также в главе 17, где описывается

работа с заголовками и IP-адресами. 

**Значения скрытых полей ввода**

Скрытые поля в HTML-формах содержат значения, видимые

для браузера, но не для пользователя \(если только он не

заглянет в исходный код сайта\). По мере роста популярности

данных cookie для хранения и передачи переменных между

страницами сайта скрытые поля на какое-то время перестали

использоваться, однако затем у них обнаружилось еще одно

замечательное назначение: не позволять веб-скраперам

отправлять формы. 

На рис. 14.1 показан пример применения таких скрытых

полей на странице входа в Facebook. У этой формы всего три

видимых поля \(**Username** \(Имя пользователя\), **Password** \(Пароль\)

![Image 72](images/000051.png)

и кнопка **Submit** \(Отправить\)\), но много скрытых, через

которые на сервер передается много информации. 

Скрытые поля используются для предотвращения веб-скрапинга двумя основными способами: поле может

заполняться 

значением 

переменной, 

случайно

сгенерированным на странице формы, и его же сервер ожидает

получить на странице обработки формы. Если в форме такое

значение отсутствует, то сервер разумно предполагает, что

данные не были отправлены естественным путем со страницы

формы, а переданы ботом непосредственно на страницу

обработки. Лучший способ обойти это препятствие — сначала

выполнить веб-скрапинг страницы формы, получить случайно

сгенерированную переменную и уже оттуда передать данные

формы на страницу обработки. 

 

**Рис. 14.1. ** Форма аутентификации в Facebook содержит несколько скрытых полей

Второй способ — своего рода поля-приманки. Если форма

содержит скрытое поле с безобидным именем вроде «Имя

пользователя» или «Адрес электронной почты», то плохо

написанный бот может заполнить это поле и попытаться его

отправить, независимо от того, является оно скрытым или нет. 

Любые скрытые поля с реальными значениями \(отличными от

предлагаемых по умолчанию на странице отправки формы\)

следует игнорировать, иначе пользователя могут даже

заблокировать на сайте. 

Одним словом, иногда необходимо проверить страницу, на

которой находится форма, чтобы увидеть, не пропустили ли вы

что-нибудь такое, чего может ожидать сервер. Если вы видите

несколько скрытых полей зачастую с длинными, случайно

сгенерированными строковыми переменными, то, скорее

всего, веб-сервер будет проверять их наличие при отправке

формы. Или же может проверять, были ли эти переменные

сгенерированы недавно с целью гарантировать однократное

использование переменных формы \(чтобы не позволить веб-скраперу просто сохранять эти значения в скрипте и время от

времени снова их использовать\). Или и то и другое. 

**Как справляться с полями-приманками**

С одной стороны, CSS часто значительно упрощает жизнь

\(например, путем чтения тегов id и class\), но, с другой

стороны, когда нужно отличить полезную информацию от

бесполезной, веб-скраперу бывает трудно это сделать. Если

поле веб-формы скрыто от пользователя с помощью CSS, то

разумно 

предположить, 

что 

обычный 

пользователь, 

посещающий сайт, не может заполнить данное поле, поскольку

оно не отображается в браузере. Если же форма все-таки

заполнена, то, скорее всего, это признак деятельности бота, и

сообщение будет отклонено. 

Это относится не только к формам, но и к ссылкам, изображениям, файлам и другим элементам сайта, которые

могут быть прочитаны ботом, но скрыты от обычных

пользователей, посещающих сайт через браузер. Посещение

страницы сайта со скрытой ссылкой на нее вполне может

запустить серверный скрипт, который заблокирует IP-адрес

пользователя, аннулирует его аутентификацию на сайте или

предпримет какие-либо другие действия, чтобы предотвратить

дальнейший доступ данного пользователя к сайту. В сущности, именно на этой концепции основаны многие бизнес-модели. 

Возьмем, к примеру, страницу, расположенную по адресу

**http://pythonscraping.com/pages/itsatrap.html**. 

Эта 

страница

содержит две ссылки: одна из них скрыта средствами CSS, а

вторая является видимой. Кроме того, здесь есть форма с

двумя скрытыми полями:

<html> 

<head> 

<title>A bot-proof form</title> 

</head> 

<style> 

body \{

overflow-x:hidden; 

\}

.customHidden \{

position:absolute; 

right:50000px; 

\}

</style> 

<body> 

<h2>A bot-proof form</h2> 

<a href=

"http://pythonscraping.com/dontgohere" 

style="display:none;">Go here\!</a> 

<a href="http://pythonscraping.com">Click me\!</a> 

<form> 

<input type="hidden" name="phone" 

value="valueShouldNotBeModified"/><p/> 

<input type="text" name="email" 

class="customHidden" 

value="intentionallyBlank"/> 

<p/> 

<input type="text" name="firstName"/> 

<p/> 

<input type="text" name="lastName"/> 

<p/> 

<input type="submit" value="Submit"/> 

<p/> 

</form> 

</body> 

</html> 

Эти три элемента скрыты от пользователя следующими

тремя способами:

• первая ссылка скрыта просто с помощью атрибута CSS

display:none; 

• поле phone является скрытым полем ввода; 

• поле email невидимое, так как смещено на 50 000 пикселов

вправо \(предположительно за пределы экрана любого

монитора\); полоса прокрутки, благодаря которой это могло

бы быть заметно, тоже скрыта. 

К счастью, поскольку Selenium отображает помещаемые

страницы, это позволяет отличать элементы, визуально

присутствующие на странице, от тех, которые не видны. 

Наличие элемента на странице можно определить с помощью

функции is\_displayed\(\). 

Например, следующий код извлекает описанную ранее

страницу, находит скрытые ссылки и поля ввода форм: from selenium import webdriver

from 

selenium.webdriver.remote.webelement

import WebElement

from selenium.webdriver.chrome.options import

Options

 

driver = webdriver.Chrome\(

executable\_path='drivers/chromedriver', 

options=chrome\_options\)

driver.get\('http://pythonscraping.com/pages/its

atrap.html'\)

links = driver.find\_elements\_by\_tag\_name\('a'\)

for link in links:

if not link.is\_displayed\(\):

print\('The link \{\} is a

trap'.format\(link.get\_attribute\('href'\)\)\)

 

fields 

=

driver.find\_elements\_by\_tag\_name\('input'\)

for field in fields:

if not field.is\_displayed\(\):

print\('Do not change value of

\{\}'.format\(field.get\_attribute\('name'\)\)\)

Selenium находит все скрытые поля и выводит следующие

данные:

The link http://pythonscraping.com/dontgohere

is a trap

Do not change value of phone

Do not change value of email

Скорее всего, вы не захотите переходить по обнаруженным

скрытым ссылкам, однако, отправляя форму, стоит убедиться в

правильности заполнения скрытых полей \(или в их заполнении

Selenium\). Резюмируем: просто игнорировать скрытые поля

опасно, хотя следует быть осторожными при взаимодействии с

ними. 

**Контрольный список: как выдать программу за человека**

В данной главе, да и вообще во всей книге, много говорится о

способах разработки веб-скрапера, который будет как можно

меньше похож на него и как можно больше — на человека. 

Если, несмотря на это, вас все равно блокируют сайты и вы не

знаете почему, то вот контрольный список, который можно

использовать для устранения проблемы. 

• Прежде всего, если страница, которую вы получаете с веб-сервера, пуста, на ней отсутствует информация или она

каким-либо иным образом не соответствует вашим

ожиданиям \(или увиденному вами в браузере\), — это, вероятно, вызвано тем, что для создания данной страницы

выполняется скрипт JavaScript. Обратитесь к главе 11. 

• Отправляя на сайт форму или POST-запрос, просмотрите

страницу и убедитесь в отправке всего, что сайт ожидает от

вас, причем в правильном формате. Чтобы увидеть, какой

POST-запрос в действительности отправляется на сайт, и

убедиться, что в нем есть все необходимое и «естественный»

запрос выглядит идентично тому, который отправляет ваш

бот, используйте соответствующий инструмент, такой как

панель Chrome Inspector. 

• Если при попытке аутентифицироваться на сайте вам не

удается там «закрепиться» или сайт себя ведет как-то

странно, то проверьте данные cookie. Убедитесь, что они

правильно сохраняются перед каждой загрузкой страницы и

ваши данные cookie передаются на сайт при каждом

запросе. 

• Если вы получаете от клиента ошибки HTTP, особенно

ошибку 403 Forbidden \(доступ запрещен\), то это может

означать, что сайт идентифицировал ваш IP-адрес как адрес

бота и не желает принимать какие-либо дополнительные

запросы. Нужно либо подождать, пока ваш IP-адрес будет

удален из списка, либо получить новый IP-адрес \(например, сходить в ближайшее кафе или заглянуть в главу 17\). Чтобы

убедиться в отсутствии блокировок, попробуйте сделать

следующее. 

• Убедитесь, что веб-скраперы не перемещаются по сайту

слишком быстро. Быстрый веб-скрапинг — порочная

практика. Она ложится тяжелым бременем на серверы

веб-администратора, может привести к юридическим

проблемам и является главной причиной попадания веб-скраперов в черный список. Добавьте в ваши веб-скраперы задержки и оставьте их работать на ночь. 

Помните: писать программы или собирать данные в

спешке — признак плохого управления проектами; стройте планы заранее, в первую очередь чтобы избежать

подобной суматохи. 

• Самое очевидное: смените заголовки\! Некоторые сайты

блокируют все, что объявляет себя веб-скрапером. Если

вы не знаете точно, какие значения заголовков стоит

использовать, то скопируйте заголовки из браузера. 

• Убедитесь, что не нажимаете что-то и не получаете доступ

к чему-либо, обычно недоступному человеку \(подробнее

об этом см. в подразделе «Как избежать ловушек» раздела

«Основные средства защиты форм» на с. 265\). 

• Если обнаружится, что для получения доступа вам

требуется слишком много «танцев с бубном», то

посмотрите, нельзя ли связаться с администратором

сайта, объяснить ему ваши действия и получить

разрешение 

на 

использование 

веб-скраперов. 

Попробуйте 

отправить 

письмо 

по 

адресу

webmaster@<имядомена> или admin@<имядомена>. 

Администраторы тоже люди, и вы будете удивлены тем, насколько они готовы делиться данными. 

**Глава 15. Тестирование сайтов с помощью веб-скраперов**

При работе с веб-проектами, в которых используется большой

стек технологий, как правило, регулярно тестируется только

серверная часть стека. У большинства современных языков

программирования \(включая Python\) есть тот или иной

фреймворк для тестирования, но фронтенд сайтов часто

остается за пределами этих автоматических тестов, хотя

именно он обычно является единственной частью проекта, которую видит клиент. 

Проблема отчасти состоит в том, что сайты, как правило, представляют собой смесь многих языков разметки и

программирования. Можно написать юнит-тесты для разделов

JavaScript, но эти тесты будут бесполезны, если HTML-код, с

которым взаимодействует JavaScript, изменится таким

образом, что JavaScript не будет выполнять предполагаемое

действие на странице даже при правильной работе самого

скрипта. 

Задачу тестирования клиентской части сайтов часто

оставляют напоследок или делегируют программистам более

низкого уровня, вооруженным в лучшем случае контрольным

списком того, что нужно проверить, и средством для

отслеживания ошибок. Но если заблаговременно приложить

немного больше усилий, то можно заменить этот контрольный

список серией юнит-тестов, а человеческий глаз — веб-скрапером. 

Вы только представьте: веб-разработка через тестирование\! 

Ежедневные тесты, позволяющие убедиться, что все части веб-интерфейса работают должным образом. Каждый раз, когда

кто-то добавляет на сайт новую функцию или меняет

положение элемента, запускается набор тестов. В этой главе мы

рассмотрим основы тестирования и узнаем, как тестировать

всевозможные виды сайтов, от простых до сложных, с

помощью веб-скраперов на основе Python. 

**Основы тестирования**

Если вам прежде не приходилось писать тесты для кода, то

самое время начинать. Набор тестов, который можно

запустить, чтобы убедиться в должной работе кода \(по крайней

мере, настолько, насколько вы написали тесты\), экономит

время и нервы, а также упрощает выпуск обновлений. 

**Что такое юнит-тесты**

Термины *«тесты»* и *«юнит-тесты»* \(иногда называют

*модульными*\) нередко считают взаимозаменяемыми. Часто, когда программисты говорят о «написании тестов», они

действительно имеют в виду написание юнит-тестов. Но есть и

такие, которые, говоря о написании юнит-тестов, на самом

деле пишут какие-то другие. 

Определения и методы юнит-тестирования часто меняются

от компании к компании, однако такое тестирование обычно

характеризуется следу ющими общими свойствами. 

• Каждый юнит-тест проверяет один аспект функциональности

компонента. Например, он может проверять, выдается ли

соответствующее сообщение об ошибке при попытке снять с

банковского счета отрицательное количество долларов. 

Юнит-тесты часто группируются в один класс для того

компонента, который тестируют. Например, после теста на

отрицательное значение суммы в долларах, снятых с

банковского счета, может идти юнит-тест поведения

банковского счета при попытке снять сумму, превышающую

остаток. 

• Каждый юнит-тест может проводиться совершенно

независимо от других. Все настройки или отмены

настройки, требуемые для юнит-теста, должен совершать он

сам. Аналогично юнит-тесты не должны влиять на успешное

или неудачное прохождение других тестов и должны иметь

возможность 

успешно 

выполняться 

в 

любой

последовательности. 

• Каждый юнит-тест обычно содержит хотя бы одно

*утверждение*. Например, юнит-тест может утверждать, что 2

\+ 2 равно 4. Иногда такой тест содержит только состояние

неудачи. Например, может завершиться неудачно при

выдаче исключения, но выполниться по умолчанию, если

все идет хорошо. 

• Юнит-тесты отделены от основной части кода. Они

обязательно должны импортировать и использовать

тестируемый код, однако обычно тесты хранятся в

отдельных классах и каталогах. 

Есть много других типов тестов — например, комплексные

и контрольные, — однако в этой главе основное внимание

уделяется юнит-тестированию. Такие тесты не просто стали

чрезвычайно популярными благодаря последним тенденциям

разработки на основе тестирования; длина кода и гибкость

этих тестов облегчают взаимодействие с ними в качестве

примеров, а у Python есть ряд встроенных возможностей для

юнит-тестирования, о которых вы узнаете в следующем

разделе. 

**Python-модуль unittest**

Библиотека юнит-тестирования Python под названием

unittest 

входит 

в 

комплект 

всех 

стандартных

инсталляционных пакетов Python. Достаточно импортировать

и расширить класс unittest.TestCase, и у вас появятся

следующие возможности:

• функции setUp и tearDown, которые выполняются до и

после каждого юнит-теста; 

• несколько типов операторов утверждений, описывающих

условия прохо ждения или непрохождения тестов; 

• возможность выполнять любые функции, имена которых

начинаются с test\_, как у юнит-тестов, и игнорировать

функции, не объявленные тестами. 

Вот пример простого юнит-теста, позволяющего проверить, что в Python 2 \+ 2 = 4:

import unittest

 

class TestAddition\(unittest.TestCase\):

def setUp\(self\):

print\('Setting up the test'\)

 

def tearDown\(self\):

print\('Tearing down the test'\)



def test\_twoPlusTwo\(self\):

total = 2\+2

self.assertEqual\(4, total\); 

 

if \_\_name\_\_ == '\_\_main\_\_':

unittest.main\(\)

Несмотря на то что функции setUp и tearDown здесь не

совершают никаких полезных действий, они все же включены в

код в иллюстративных целях. Обратите внимание: эти

функции выполняются до и после каждого теста, а не до и

после всех тестов класса. 

Результат выполнения тестовой функции при запуске из

командной строки должен выглядеть так:

Setting up the test

Tearing down the test

. 

-----------------------------------------------

-----------------------

Ran 1 test in 0.000s

 

OK

Это указывает на то, что тест пройден успешно и 2 \+ 2

действительно равно 4. 

**Выполнение unittest в Jupyter Notebook**

Все сценарии юнит-тестов в этой главе запускаются

одинаково:

if \_\_name\_\_ == '\_\_main\_\_':

unittest.main\(\)

Условие if \_\_name\_\_ == '\_\_main\_\_' истинно только в том

случае, если данная строка выполняется непосредственно в

Python, а не с помощью оператора import. Это позволяет

запускать юнит-тест, используя расширение класса

unittest.TestCase, непосредственно из командной

строки. 

В Jupyter Notebook все немного иначе. Параметры argv, создаваемые Jupyter, могут вызывать ошибки в юнит-тестах, и, поскольку фреймворк unittest по умолчанию завершает

работу Python после выполнения теста \(что вызывает

проблемы в ядре Notebook\), это необходимо предотвратить. 

В Jupyter Notebook мы будем запускать юнит-тесты так: if \_\_name\_\_ == '\_\_main\_\_':

unittest.main\(argv=\[''\], exit=False\)

%reset

Во второй строке всем переменным argv \(аргументам

командной строки\) присваиваются значения в виде пустых

строк, которые unnittest.main игнорирует. Таким образом

предотвращается еще и завершение работы unittest после

выполнения теста. 

Строка %reset нужна для того, чтобы освободить память и

уничтожить все переменные, созданные пользователем в

Jupyter Notebook. Без нее каждый юнит-тест, написанный в

Notebook, будет содержать все методы всех ранее

выполненных тестов, которые тоже являются наследниками

unittest.TestCase, включая методы setUp и tearDown. 

Это также означает, что каждый следующий юнит-тест будет запускать все методы

из предыдущих аналогичных тестов\! 

Однако использование %reset говорит о том, что

пользователь при выполнении тестов должен будет

совершить еще одну операцию. При запуске теста Notebook выведет подсказку и спросит у пользователя, уверен ли он в

своем желании освободить память. Чтобы это сделать, нужно

просто ввести y и нажать **Enter**. 

**Тестирование «Википедии». ** Чтобы протестировать

клиентский интерфейс вашего сайта \(за исключением

скриптов JavaScript, о которых я расскажу далее\), надо всего

лишь объединить Python-библиотеку unittest с веб-скрапером:

from urllib.request import urlopen

from bs4 import BeautifulSoup

import unittest

 

class TestWikipedia\(unittest.TestCase\):

bs = None

def setUpClass\(\):

 

 

 

 

 

 

 

 

url 

=

'http://en.wikipedia.org/wiki/Monty\_Python' 

 

 

 

 

 

 

 

 

TestWikipedia.bs 

=

BeautifulSoup\(urlopen\(url\), 'html.parser'\)

 

def test\_titleText\(self\):



 

 

 

 

 

 

 

pageTitle 

=

TestWikipedia.bs.find\('h1'\).get\_text\(\)

self.assertEqual\('Monty Python', 

pageTitle\); 

 

def test\_contentExists\(self\):

content = TestWikipedia.bs.find\('div', 

\{'id':'mw-content-text'\}\)

self.assertIsNotNone\(content\)

 

if \_\_name\_\_ == '\_\_main\_\_':

unittest.main\(\)

На этот раз у нас два теста: первый проверяет, соответствует

ли заголовок страницы ожидаемому Monty Python, а второй —

есть ли на странице элемент div с контентом. 

Обратите внимание: контент страницы загружается только

один раз, и глобальный объект bs используется тестами

совместно. Это возможно благодаря тому, что в unittest определена функция setUpClass, которая выполняется только

один раз в начале класса \(в отличие от setUp, запускаемой

перед каждым тестом в отдельности\). Применяя setUpClass вместо setUp, мы исключаем лишние загрузки страницы; можно получить контент один раз и провести для него

несколько тестов. 

Помимо того, когда и как часто выполняются эти функции, между setUpClass и setUp есть еще одно фундаментальное

архитектурное различие: setUpClass — статический метод, который «принадлежит» самому классу и использует

глобальные переменные класса, а setUp — функция

экземпляра класса, и она принадлежит конкретному

экземпляру класса. Именно поэтому setUp может

устанавливать атрибуты для self — конкретного экземпляра

класса, тогда как setUpClass может обращаться только к

статическим атрибутам класса TestWikipedia. 

Тестирование каждой страницы в отдельности может

показаться не таким уж мощным или интересным, однако, как

вы, вероятно, помните из главы 3, сравнительно легко

построить веб-краулер, который бы итеративно перемещался

по всем страницам сайта. Что произойдет, если объединить

веб-краулер с юнит-тестом, проверяющим одну страницу? 

Есть много способов запускать тесты многократно, но это

следует делать осторожно, чтобы загружать каждую страницу

только один раз для каждого набора тестов, которые вы хотите

выполнить на этой странице. Кроме того, следует по

возможности не хранить в памяти слишком много

информации одновременно. Именно так работает следующая

функция:

from urllib.request import urlopen

from bs4 import BeautifulSoup

import unittest

import re

import random

from urllib.parse import unquote

 

class TestWikipedia\(unittest.TestCase\):

 

def test\_PageProperties\(self\):

 

 

 

 

 

 

 

 

self.url 

=

'http://en.wikipedia.org/wiki/Monty\_Python' 

\# протестировать первые 10 попавшихся

страниц

for i in range\(1, 10\):



 

 

 

 

 

 

 

 

 

 

 

self.bs 

=

BeautifulSoup\(urlopen\(self.url\), 'html.parser'\)

titles = self.titleMatchesURL\(\)

self.assertEquals\(titles\[0\], 

titles\[1\]\)

self.assertTrue\(self.contentExists\(

\)\)

self.url = self.getNextLink\(\)

print\('Done\!'\)

 

def titleMatchesURL\(self\):

 

 

 

 

 

 

 

 

pageTitle 

=

self.bs.find\('h1'\).get\_text\(\)

 

 

 

 

 

 

 

 

urlTitle 

=

self.url\[\(self.url.index\('/wiki/'\)\+6\):\]

urlTitle = urlTitle.replace\('\_', ' '\)

urlTitle = unquote\(urlTitle\)

return \[pageTitle.lower\(\), 

urlTitle.lower\(\)\]

 

def contentExists\(self\):

content = self.bs.find\('div',\{'id':'mw-

content-text'\}\)

if content is not None:

return True

return False

 

def getNextLink\(self\):

\# возвращает случайную ссылку, 

найденную на странице, 

\# используя методику из главы 3

links = self.bs.find\('div', 

\{'id':'bodyContent'\}\).find\_all\(

'a', href=re.compile\('^\(/wiki/\)

\(\(?\!:\).\)\*$'\)\)

 

 

 

 

 

 

 

 

randomLink 

=

random.SystemRandom\(\).choice\(links\)

 

 

 

 

 

 

 

 

return

'https://wikipedia.org\{\}'.format\(randomLink.att

rs\['href'\]\)

 

if \_\_name\_\_ == '\_\_main\_\_':

unittest.main\(\)

Есть несколько моментов, на которые стоит обратить

внимание. Во-первых, в этом классе содержится только один

тест. Остальные функции с технической точки зрения являются

лишь вспомогательными, даже если проделывают основную

часть вычислительной работы с целью определить, был ли тест

пройден без ошибок. Поскольку тестовая функция выполняет

операторы утверждений, результаты теста возвращаются в

тестовую функцию, в которой выполняются утверждения. 

Кроме того, хоть contentExists и возвращает логическое

значение, функция titleMatchesURL возвращает сами

значения для оценки. Чтобы понять, зачем возвращать сами

значения, а не только логику, сравним результаты следующего

логической проверки:



===============================================

=======================

 

FAIL: 

test\_PageProperties

\(\_\_main\_\_.TestWikipedia\)

-----------------------------------------------

-----------------------

Traceback \(most recent call last\): File 

"15-3.py", 

line 

22, 

in

test\_PageProperties

self.assertTrue\(self.titleMatchesURL\(\)\)

AssertionError: False is not true

с результатами выполнения оператора assertEquals:



===============================================

=======================

 

FAIL: 

test\_PageProperties

\(\_\_main\_\_.TestWikipedia\)

-----------------------------------------------

-----------------------

Traceback \(most recent call last\):

 

 

 

 

File 

"15-3.py", 

line 

23, 

in

test\_PageProperties

self.assertEquals\(titles\[0\], titles\[1\]\)

AssertionError: 'lockheed u-2' \!= 'u-2 spy

plane' 

Какой из них легче отлаживать? \(В данном случае ошибка

возникает из-за перенаправления, когда публикация

**http://wikipedia.org/wiki/u-2%20spy%20plane** перенаправляет на

статью под названием Lockheed U-2.\)

**Тестирование с помощью Selenium**

JavaScript создает определенные проблемы не только при веб-скрапинге скриптов Ajax, описанном в главе 11, но и при

тестировании сайтов. К счастью, у Selenium есть отличный

фреймворк для работы с особенно сложными сайтами; 

собственно говоря, эта библиотека изначально была создана

именно для тестирования сайтов\! 

Несмотря на то что юнит-тесты на Python и на Selenium, безусловно, написаны на одном языке, в их синтаксисе на

удивление мало общего. Selenium не требует представления

юнит-тестов в виде функций внутри классов; здесь операторы

assert не требуют круглых скобок, а тесты выполняются без

каких-либо сообщений, за исключением сообщений о сбое: driver = webdriver.Chrome\(\)

driver.get\('http://en.wikipedia.org/wiki/Monty\_

Python'\)

assert 'Monty Python' in driver.title

driver.close\(\)

При выполнении этого теста результаты выводиться не

должны. 

Таким образом, тесты на Selenium можно писать более

небрежно, чем юнит-тесты на Python, и операторы assert можно интегрировать в обычный код, если вы хотите, чтобы

действие кода прекращалось, когда не выполняется какое-либо

условие. 

**Взаимодействие с сайтом**

Недавно я захотела связаться с небольшой местной компанией

через контактную форму на их сайте, но обнаружила

повреждение HTML-формы; когда я нажимала кнопку

отправки, ничего не происходило. Проведя небольшое

расследование, я обнаружила, что на сайте применялась

простая форма с отправкой данных по электронной почте —

каждый раз при заполнении формы отправлялось электронное

письмо с контентом формы. К счастью, зная это, я сумела

отправить представителям компании электронное письмо, в

котором объяснила суть проблемы с их формой, и все же

воспользовалась их услугами, несмотря на техническую

проблему. 

Если бы я захотела написать традиционный веб-скрапер, который бы использовал или тестировал эту форму, то он, скорее всего, просто скопировал бы ее разметку и отправил

электронное письмо напрямую, вообще минуя данную форму. 

Как же проверить функциональность формы и убедиться, что

она правильно работает через браузер? 

В предыдущих главах мы уже обсуждали навигацию по

ссылкам, отправку форм и другие виды интерактивных

действий, однако все наши действия в своей основе были

предназначены для того, чтобы *обойти* интерфейс браузера, а

не использовать его. Но ведь Selenium позволяет именно

вводить текст, нажимать кнопки и делать все остальное в

браузере \(в данном случае в Chrome в режиме консоли\) и

обнаруживать такие вещи, как неправильно написанные

формы, плохой код JavaScript, опечатки в HTML-коде и другие

проблемы, которые бы поставили в тупик обычных

посетителей сайта. 

Главное условие в тестах подобного рода — концепция

elements в Selenium. Я уже упоминала этот объект в главе 11, он возвращается при вызовах функций такого типа: usernameField 

=

driver.find\_element\_by\_name\('username'\)

Подобно тому как в браузере можно совершать

всевозможные действия с различными элементами сайта, Selenium точно так же позволяет выполнять множество

действий с любым элементом, в том числе следующие:

myElement.click\(\)

myElement.click\_and\_hold\(\)

myElement.release\(\)

myElement.double\_click\(\)

myElement.send\_keys\_to\_element\('content 

to

enter'\)

Элементы позволяют не только совершать одиночные

действия — строки действий можно объединять в *цепи*

*действий*, сохранять их и выполнять в программе один или

несколько раз. Эти цепи полезны тем, что позволяют удобно

объединять действия в длинные наборы, но при этом

функционально идентичны явному вызову действия для

одного элемента, как в предыдущих примерах. 

Чтобы увидеть эту разницу, рассмотрим страницу формы, размещенную 

по 

адресу

**http://pythonscraping.com/pages/files/form.html** \(которая 

уже

использовалась в качестве примера в главе 10\). Мы можем

заполнить и отправить эту форму следующим образом: from selenium import webdriver

from 

selenium.webdriver.remote.webelement

import WebElement

from selenium.webdriver.common.keys import Keys

from selenium.webdriver import ActionChains

from selenium.webdriver.chrome.options import

Options

 

chrome\_options = Options\(\)

chrome\_options.add\_argument\('--headless'\)

 

driver = webdriver.Chrome\(

executable\_path='drivers/chromedriver', options=chrome\_options\)

driver.get\('http://pythonscraping.com/pages/fil

es/form.html'\)

 

firstnameField 

=

driver.find\_element\_by\_name\('firstname'\)

lastnameField 

=

driver.find\_element\_by\_name\('lastname'\)

submitButton 

=

driver.find\_element\_by\_id\('submit'\)

 

\#\#\# МЕТОД 1 \#\#\#

\#firstnameField.send\_keys\('Ryan'\)

lastnameField.send\_keys\('Mitchell'\)

submitButton.click\(\)



\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#

 

\#\#\# МЕТОД 2 \#\#\#

actions 

=

ActionChains\(driver\).click\(firstnameField\)

.send\_keys\('Ryan'\)

.click\(lastnameField\)

.send\_keys\('Mitchell'\)

.send\_keys\(Keys.RETURN\)

actions.perform\(\)



\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#

 

print\(driver.find\_element\_by\_tag\_name\('body'\).t

ext\)

 

driver.close\(\)

Метод 1 состоит в вызове функции send\_keys для двух

полей, после чего нажимается кнопка отправки формы. В

методе 2 используется общая цепь действий, в которой после

вызова метода perform последовательно производится

нажатие каждого поля формы, после чего туда вводится текст. 

Результат выполнения скрипта в обоих случаях один и тот же. 

Независимо от того, какой из двух методов был применен, выводится следующая строка:

Hello there, Ryan Mitchell\! 

Помимо этих двух методов, есть еще один вариант, в

дополнение к объектам, которые используются для обработки

команд: обратите внимание, что в первом методе для отправки

формы мы нажимаем кнопку отправки, а во втором —

используем клавишу **Enter** \(Ввод\) после заполнения текстового

поля. Поскольку существует множество последовательностей

событий для выполнения одного и того же действия, есть

большое количество способов совершить его с помощью

Selenium. 

**Метод drag-and-drop**

Нажатие кнопок и ввод текста — само по себе уже неплохо, но в

чем Selenium действительно незаменим, так это в работе с

относительно новыми способами взаимодействия в Интернете. 

Selenium легко управляет интерфейсами, построенными на

основе 

метода 

перетаскивания 

\(drag-and-drop\). 

Для

использования функции перетаскивания необходимо указать

*исходный* элемент \(подлежащий перетаскиванию\) и задать либо

величину смещения, на которую он будет сдвинут, либо

*конечный элемент*, на который нужно перетащить исходный

элемент. 

Пример интерфейса такого типа представлен на

демонстрационной странице, расположенной по адресу

**http://pythonscraping.com/pages/javascript/draggableDemo.html**: from selenium import webdriver

from 

selenium.webdriver.remote.webelement

import WebElement

from selenium.webdriver import ActionChains

from selenium.webdriver.chrome.options import

Options

import unittest

 

class TestAddition\(unittest.TestCase\):

driver = None

 

def setUp\(self\):

chrome\_options = Options\(\)

chrome\_options.add\_argument\('--

headless'\)

self.driver = webdriver.Chrome\(

executable\_path='drivers/chromedriv

er', options=chrome\_options\)

 

 

 

 

 

 

 

 

url 

=

'http://pythonscraping.com/pages/javascript/dra

ggableDemo.html' 

self.driver.get\(url\)

 

def tearDown\(self\):

driver.close\(\)

 

def test\_drag\(self\):



 

 

 

 

 

 

 

element 

=

self.driver.find\_element\_by\_id\('draggable'\)

 

 

 

 

 

 

 

 

target 

=

self.driver.find\_element\_by\_id\('div2'\)

actions = ActionChains\(self.driver\)

actions.drag\_and\_drop\(element, 

target\).perform\(\)

self.assertEqual\('You are definitely

not a bot\!', 

self.driver.find\_element\_by\_id\('mes

sage'\).text\)

На демонстрационной странице в теге div с

идентификатором message выводятся два сообщения. Первое

гласит:

Prove you are not a bot, by dragging the square

from the blue area

to the red area\! 

\(Докажите, что вы не бот, перетащив квадрат из синей

области в красную\!\) Затем, сразу же после выполнения этой

задачи, выводится другое сообщение:

You are definitely not a bot\! 

\(Вы точно не бот\!\) Как следует из примера, показанного на

демонстрационной странице, перетаскивание элементов с

целью доказать, что вы не бот, является обычной практикой во

многих тестах капчи. Боты уже давно успешно перетаскивают

объекты \(ведь это всего лишь последовательность действий: нажать, удерживать и перемещать\), однако идея предложить

посетителю сайта перетащить тот или иной элемент с целью

подтвердить, что он человек, еще долго не умрет. 

Кроме того, библиотеки капч с перетаскиваниями редко

используют какие-либо трудные для ботов задания, такие как

«перетащить изображение котенка на изображение коровы»

\(что потребовало бы от программы синтаксического анализа

умения различать изображения котенка и коровы\); вместо

этого они часто задействуют упорядочение чисел или какую-либо другую довольно тривиальную задачу, подобную

приведенной в предыдущем примере. 

Конечно, эффективность этих библиотек заключается в том, что вариантов слишком много, а такие тесты используются

слишком редко; скорее всего, никто не озаботится созданием

бота, способного проходить любые подобные тесты. В любом

случае данного примера должно хватить для демонстрации

того, почему никогда не следует применять описанный метод

для крупных сайтов. 

**Создание снимков экрана**

Помимо обычных возможностей тестирования, у Selenium есть

интересная хитрость, которая позволит вам значительно

упростить тестирование \(или произвести впечатление на

шефа\): снимки экрана. Да, вы действительно можете получать

фотоподтверждения прямо из юнит-тестов и вам не придется

самолично нажимать **PrtScn**:

driver = webdriver.Chrome\(\)

driver.get\('http://www.pythonscraping.com/'\)

driver.get\_screenshot\_as\_file\('tmp/pythonscrapi

ng.png'\)

Этот 

скрипт 

переходит 

на 

страницу

**http://pythonscraping.com**, делает снимок экрана начальной

страницы и сохраняет его в локальной папке tmp \(для

правильного сохранения файла она уже должна существовать\). 

Снимки можно сохранять в различных графических форматах. 

**unittest или Selenium**

Синтаксическая строгость и многословность Python-библиотеки unittest, возможно, желательна для большинства

крупных наборов тестов. Но если нужно протестировать лишь

несколько функций сайта, то единственным вариантом может

оказаться гибкий и мощный Selenium. Что же выбрать? 

Раскрою секрет: вам не нужно выбирать. Selenium вполне

пригоден для получения информации о сайте, а unittest позволит оценить, соответствует ли эта информация

критериям прохождения теста. Нет никаких причин, по

которым вы не могли бы импортировать инструменты

Selenium в Python unittest, объединив лучшие свойства

обоих. 

Например, в следующем скрипте создается юнит-тест для

сайта с интерфейсом drag-and-drop с предположением, что при

правильной работе после перетаскивания одного элемента на

другой выводится сообщение **You are not a bot\! ** \(Вы не бот\!\): from selenium import webdriver

from selenium.webdriver import ActionChains

from selenium.webdriver.chrome.options import

Options

import unittest

 

class TestDragAndDrop\(unittest.TestCase\):

driver = None

def setUp\(self\):

chrome\_options = Options\(\)

chrome\_options.add\_argument\('--

headless'\)

self.driver = webdriver.Chrome\(

executable\_path='drivers/chromedriv

er', options=chrome\_options\)

 

 

 

 

 

 

 

 

url 

=

'http://pythonscraping.com/pages/javascript/dra

ggableDemo.html' 

self.driver.get\(url\)

 

def tearDown\(self\):

self.driver.close\(\)

 

def test\_drag\(self\):

 

 

 

 

 

 

 

 

element 

=

self.driver.find\_element\_by\_id\('draggable'\)

 

 

 

 

 

 

 

 

target 

=

self.driver.find\_element\_by\_id\('div2'\)

actions = ActionChains\(self.driver\)

actions.drag\_and\_drop\(element, 

target\).perform\(\)

self.assertEqual\('You are definitely

not a bot\!', 

self.driver.find\_element\_by\_id\('mes

sage'\).text\)

Комбинируя Python unittest и Selenium, можно

протестировать практически все, что есть на сайте. В сущности, присоединив некоторые библиотеки обработки изображений, 

описанные в главе 13, можно даже сделать снимок экрана веб-страницы и попиксельно проверить, что должно находиться на

ней. 

**Глава 16. Параллельный веб-краулинг**

Веб-краулинг выполняется быстро. По крайней мере это, как

правило, гораздо быстрее, чем если нанять дюжину стажеров, которые станут вручную копировать данные из Интернета\! 

Конечно, развитие технологии и гедонистическое нетерпение в

какой-то момент приведут к тому, что вам заявят, будто даже

это «недостаточно быстро». Обычно в подобные моменты люди

начинают задумываться о распределенных вычислениях. 

В отличие от большинства других технологий веб-краулинг

часто попросту невозможно улучшить, «бросая больше циклов

на амбразуру». Быстрое выполнение одного процесса еще не

означает, что с двумя процессами задача будет решена в два

раза быстрее. А выполнение трех процессов может привести к

блокировке вас на удаленном сервере, который вы уже

заклевали своими запросами\! 

Однако есть случаи, когда параллельный веб-краулинг или

запуск параллельных потоков/процессов могут быть полезны:

• сбор данных не из одного, а из нескольких источников \(с

нескольких удаленных серверов\); 

• выполнение длинных или сложных операций с собранными

данными \(например, анализ изображений или OCR\), которые можно совершать параллельно с извлечением

данных; 

• сбор данных из крупного веб-сервиса, где вы платите за

каждый запрос или где создание нескольких подключений к

сервису не выходит за рамки пользовательского соглашения. 

**Процессы или потоки**

Python поддерживает как многопроцессность, так и

многопоточность. И та и другая обработка в итоге имеют одну

и ту же цель: одновременное решение двух задач

программирования вместо выполнения программы более

традиционным линейным способом. 

В информатике каждый процесс, работающий в

операционной системе, может иметь несколько потоков. У него

есть своя выделенная память — таким образом, несколько

потоков могут обращаться к одной и той же памяти, а

несколько процессов — не могут и должны передавать

информацию явно. 

Часто считается, что использовать многопоточное

программирование и выполнять задачи в отдельных потоках с

общей памятью проще, чем применять многопроцессное

программирование. Но за это удобство приходится платить. 

Глобальная блокировка интерпретатора Python \(Global Interpreter 

Lock, 

GIL\) 

предотвращает 

одновременное

выполнение разными потоками одной и той же строки кода. 

GIL гарантирует отсутствие повреждений общей памяти, совместно используемой всеми процессами \(например, не

случится так, что в одни и те же байты памяти наполовину

запишется одно значение, а наполовину — другое\). Такая

блокировка позволяет писать многопоточные программы, всегда точно зная, что именно вы получите в одной и той же

строке, но может создавать и узкие места. 

**Многопоточный веб-краулинг**

В Python 3.x используется модуль the\_thread; модуль thread устарел. 

В следующем примере показано использование нескольких

потоков для выполнения задачи:

import \_thread

import time

 

def print\_time\(threadName, delay, iterations\):

start = int\(time.time\(\)\)

for i in range\(0,iterations\):

time.sleep\(delay\)

seconds\_elapsed = str\(int\(time.time\(\)\)

- start\)

print \("\{\} \{\}".format\(seconds\_elapsed, threadName\)\)

 

try:

\_thread.start\_new\_thread\(print\_time, 

\('Fizz', 3, 33\)\)

\_thread.start\_new\_thread\(print\_time, 

\('Buzz', 5, 20\)\)

\_thread.start\_new\_thread\(print\_time, 

\('Counter', 1, 100\)\)

except:

print \('Error: unable to start thread'\)

 

while 1:

pass

Это пример классического программного теста FizzBuzz \(**http://wiki.c2.com/?Fizz BuzzTest**\) с несколько измененным

выводом результатов:

1 Counter

2 Counter

3 Fizz

3 Counter

4 Counter

5 Buzz

5 Counter

6 Fizz

6 Counter

Скрипт запускает три потока, один из которых каждые три

секунды выводит слово Fizz, другой каждые пять секунд —

Buzz, а третий — каждую секунду слово Counter. 

После запуска потоков основной поток выполнения

запускает цикл while1, благодаря которому программа \(и ее

дочерние потоки\) продолжает работать до тех пор, пока

пользователь не нажмет **Ctrl**\+**C**, чтобы остановить выполнение

программы. 

Вместо того чтобы выводить слова Fizz и Buzz, можно

выполнять в потоках какую-либо полезную задачу, например, собрать данные с сайта:

from urllib.request import urlopen

from bs4 import BeautifulSoup

import re

import random

 

import \_thread

import time

 

def get\_links\(thread\_name, bs\):

 

 

 

 

print\('Getting 

links 

in

\{\}'.format\(thread\_name\)\)



 

 

 

return 

bs.find\('div', 

\{'id':'bodyContent'\}\).find\_all\('a', 

href=re.compile\('^\(/wiki/\)\(\(?\!:\).\)\*$'\)\)

 

\# Определяем функцию для потока

def scrape\_article\(thread\_name, path\):

 

 

 

 

html 

=

urlopen\('http://en.wikipedia.org\{\}'.format\(path

\)\)

time.sleep\(5\)

bs = BeautifulSoup\(html, 'html.parser'\)

title = bs.find\('h1'\).get\_text\(\)

 

 

 

 

print\('Scraping 

\{\} 

in 

thread

\{\}'.format\(title, thread\_name\)\)

links = get\_links\(thread\_name, bs\)

if len\(links\) > 0:

newArticle = links\[random.randint\(0, 

len\(links\)-1\)\].attrs\['href'\]

print\(newArticle\)

scrape\_article\(thread\_name, newArticle\)

 

\# Создаем следующие два потока:

try:

\_thread.start\_new\_thread\(scrape\_article, 

\('Thread 1', '/wiki/Kevin\_Bacon',\)\)

\_thread.start\_new\_thread\(scrape\_article, 

\('Thread 2', '/wiki/Monty\_Python',\)\)

except:

print \('Error: unable to start threads'\)

 

while 1:

pass

Обратите внимание, что мы включили в код следующую

строку:

time.sleep\(5\)

Поскольку мы сканируем «Википедию» почти вдвое

быстрее, чем если бы поток был один, эта строка

предотвращает чрезмерную нагрузку, которую скрипт мог бы

создать на серверы «Википедии». На практике при работе с

сервером, где количество запросов не является проблемой, данную строку следует удалить. 

А если немного переписать код, чтобы он отслеживал

статьи, которые потоки уже встречали, с целью исключить

повторное посещение статей? Для этого можно использовать

список в многопоточной среде — точно так же, как и в

однопоточной:

visited = \[\]

def get\_links\(thread\_name, bs\):

 

 

 

 

print\('Getting 

links 

in

\{\}'.format\(thread\_name\)\)

 

 

 

 

links 

= 

bs.find\('div', 

\{'id':'bodyContent'\}\).find\_all\('a', 

href=re.compile\('^\(/wiki/\)\(\(?\!:\).\)\*$'\)\)

return \[link for link in links if link not

in visited\]

 

def scrape\_article\(thread\_name, path\):

visited.append\(path\)

Обратите внимание: первым действием, которое выполняет

scrape\_article, является добавление пути в список

посещенных путей. Это уменьшает, но не исключает полностью

вероятность того, что веб-скрапер обработает данную страницу

дважды. 

Если вам особенно не повезет, то оба потока наткнутся на

один и тот же путь в одно и то же время. Оба увидят, что этого

пути нет в списке посещенных, и, соответственно, одновременно добавят его в список и проведут веб-скрапинг. 

Однако на практике такое вряд ли случится вследствие

скорости выполнения и количества страниц, содержащихся в

«Википедии». 

Это пример *состояния гонки*. Такие состояния вызывают

сложности при отладке даже у опытных программистов, так

что важно определить, насколько ваш код способен создавать

подобные ситуации, оценить их вероятность и предвидеть

серьезность возможных последствий. 

В случае конкретно этого состояния гонки, когда веб-скрапер дважды обрабатывает одну и ту же страницу, возможно, и не стоит переписывать код. 

**Состояния гонки и очереди**

Мы могли бы наладить коммуникацию между потоками с

помощью списков, однако те не предназначены специально

для обмена данными между потоками и их неправильное

использование вполне может привести к тому, что программа

будет выполняться медленно или даже возникнут ошибки

вследствие состояния гонки. 

Списки отлично подходят для добавления или чтения

элементов. Но с удалением элементов в произвольных точках, особенно в начале списка, дела обстоят далеко не так хорошо. 

Используя строку, подобную этой:

myList.pop\(0\)

мы фактически требуем, чтобы Python переписал весь

список, а это замедлило бы выполнение программы. 

Еще более опасно то, что при использовании списка легко

сделать в строке случайную запись, которая не является

поточно-ориентированной. Например, такая запись: myList\[len\(myList\)-1\]

в многопоточной среде может в действительности получить

не последний элемент списка или даже выдать исключение, если 

непосредственно 

после 

вычисления 

значения

len\(myList\)-1 другая операция изменит список. 

Вы можете возразить, что было бы более в духе Python записать предыдущее выражение как myList\[-1\]. Ну да, конечно же, никто из нас в минуту слабости никогда не писал

код не в стиле Python \(особенно Java-разработчики не любят

признаваться, что было время, когда они писали нечто вроде

myList\[myList.length-1\]\)\! Но даже если ваш код

безупречен, 

посмотрите 

на 

другие 

варианты

потоконебезопасных строк, где используются списки: my\_list\[i\] = my\_list\[i\] \+ 1

my\_list.append\(my\_list\[-1\]\)

Обе эти записи могут привести к состоянию гонки, которое

будет иметь неожиданные последствия. Так что откажемся от

списков и станем передавать сообщения в потоки другими

способами\! 

\# Считываем входящее сообщение из глобального

списка

my\_message = global\_message

\# Записываем сообщение обратно

global\_message = 'I've retrieved the message' 

\# делаем что-то с my\_message

Это выглядит отлично, пока мы не обнаружим, что могли

случайно стереть сообщение, поступившее из другого потока в

момент между первой и второй строками, записав вместо него

текст I’ve retrieved the message. И теперь нам придется для

каждого потока построить сложную последовательность

объектов личных сообщений с какой-то логикой с целью

выяснять, кто что получает... или же использовать модуль

Queue, созданный как раз для этого. 

Очереди — это объекты, похожие на списки, которые

работают либо по принципу «первым пришел — первым

вышел» \(First In First Out, FIFO\), либо по принципу «последним

пришел — первым вышел» \(Last In First Out, LIFO\). Очередь

получает сообщения из любого потока с помощью функции

queue.put\('Mymessage'\) и может передать сообщение

любому потоку, вызывающему функцию queue.get\(\). 

Очереди предназначены не для хранения статических

данных, а для их передачи потокобезопасным способом. После

извлечения из очереди данные должны существовать только в

том потоке, который их извлек. Поэтому очереди обычно

используются для делегирования задач или отправки

временных уведомлений. 

Это может быть полезно при веб-краулинге. Например, мы

хотим сохранить данные, собранные веб-скрапером, в базе, и

чтобы при этом каждый поток сохранял свои данные быстро. 

Одно общее соединение для всех потоков может вызвать

проблемы \(одно соединение неспособно обрабатывать запросы

параллельно\), однако нет смысла присваивать каждому потоку

веб-скрапера отдельное соединение с базой данных. По мере

увеличения размера веб-скрапера \(возможно, вы в итоге будете

собирать данные с сотен разных сайтов в сотне потоков\) это

может привести к большому количеству неиспользуемых

соединений с базой данных, которые лишь время от времени

выполняют запись после загрузки страницы. 

Вместо этого можно создать меньшее количество потоков

базы данных, у каждого из которых будет собственное

соединение; каждый из них будет время от времени извлекать

элементы из очереди и сохранять их в базе. Так мы получим

гораздо более управляемый набор соединений с базой данных. 

from urllib.request import urlopen

from bs4 import BeautifulSoup

import re

import random

import \_thread

from queue import Queue

import time

import pymysql

 

def storage\(queue\):

conn = pymysql.connect\(host='127.0.0.1', 

unix\_socket='/tmp/mysql.sock', 

user='root', passwd='', db='mysql', 

charset='utf8'\)

cur = conn.cursor\(\)

cur.execute\('USE wiki\_threads'\)

while 1:

if not queue.empty\(\):

article = queue.get\(\)

cur.execute\('SELECT \* FROM pages

WHERE path = %s', 

\(article\["path"\]\)\)

if cur.rowcount == 0: print\("Storing article

\{\}".format\(article\["title"\]\)\)

cur.execute\('INSERT INTO pages

\(title, path\) VALUES \(%s, %s\)', \\

\(article\["title"\], 

article\["path"\]\)\)

conn.commit\(\)

else:

print\("Article already exists:

\{\}".format\(article\['title'\]\)\)

 

visited = \[\]

def getLinks\(thread\_name, bs\):

 

 

 

 

print\('Getting 

links 

in

\{\}'.format\(thread\_name\)\)

 

 

 

 

links 

= 

bs.find\('div', 

\{'id':'bodyContent'\}\).find\_all\('a', 

href=re.compile\('^\(/wiki/\)\(\(?\!:\).\)\*$'\)\)

return \[link for link in links if link not

in visited\]

 

def scrape\_article\(thread\_name, path, queue\):

visited.append\(path\)

 

 

 

 

html 

=

urlopen\('http://en.wikipedia.org\{\}'.format\(path

\)\)

time.sleep\(5\)

bs = BeautifulSoup\(html, 'html.parser'\)

title = bs.find\('h1'\).get\_text\(\)

print\('Added \{\} for storage in thread

\{\}'.format\(title, thread\_name\)\)

queue.put\(\{"title":title, "path":path\}\) links = getLinks\(thread\_name, bs\)

if len\(links\) > 0:

newArticle = links\[random.randint\(0, 

len\(links\)-1\)\].attrs\['href'\]

scrape\_article\(thread\_name, newArticle, 

queue\)

 

queue = Queue\(\)

try:

\_thread.start\_new\_thread\(scrape\_article, 

\('Thread 1', 

'/wiki/Kevin\_Bacon', queue,\)\)

\_thread.start\_new\_thread\(scrape\_article, 

\('Thread 2', 

'/wiki/Monty\_Python', queue,\)\)

\_thread.start\_new\_thread\(storage, \(queue,\)\)

except:

print \('Error: unable to start threads'\)

 

while 1:

pass

В этом скрипте создаются три потока: два из них

предназначены для веб-скрапинга страниц «Википедии», выбираемых случайным образом, а третий — для сохранения

собранных данных в базе данных MySQL. Подробнее о MySQL и

хранении данных см. в главе 6. 

**Модуль threading**

Python-модуль \_thread — довольно низкоуровневый, позволяющий управлять всеми нюансами потоков, однако

высокоуровневых функций, которые могли бы облегчить жизнь

разработчику, у этого модуля немного. Модуль threading является 

высокоуровневым 

интерфейсом, 

позволящим

аккуратно использовать потоки, одновременно реализуя все

функции лежащего в его основе модуля \_thread. 

Например, мы можем использовать статические функции, такие как enumerate, чтобы получить список всех активных

потоков, инициализированных через модуль threading, и для

этого не придется отслеживать потоки самостоятельно. 

Аналогичным образом, функция activeCount возвращает

общее количество потоков. Многие функции из модуля

\_thread получили в threading более удобные или

запоминающиеся имена, например currentThread вместо

get\_ident, которая позволяет узнать имя текущего потока. 

Вот простой пример использования модуля threading: import threading

import time

 

def print\_time\(threadName, delay, iterations\):

start = int\(time.time\(\)\)

for i in range\(0,iterations\):

time.sleep\(delay\)

seconds\_elapsed = str\(int\(time.time\(\)\)

- start\)

print \('\{\} \{\}'.format\(seconds\_elapsed, 

threadName\)\)

 

threading.Thread\(target=print\_time, 

args=

\('Fizz', 3, 33\)\).start\(\)

threading.Thread\(target=print\_time, args=

\('Buzz', 5, 20\)\).start\(\)

threading.Thread\(target=print\_time, 

args=

\('Counter', 1, 100\)\).start\(\)

Этот код выводит те же результаты алгоритма FizzBuzz, что

и предыдущий простой пример с \_thread. 

Одна из приятных особенностей модуля threading —

простота создания локальных данных потоков, недоступных

для других потоков. Это может быть удобным, если у вас есть

несколько потоков, каждый из которых выполняет веб-скрапинг своего сайта и ведет собственный локальный список

посещенных страниц. 

Эти локальные данные можно создавать в любой точке

внутри функции потока, вызвав threading.local\(\): import threading

 

def crawler\(url\):

data = threading.local\(\)

data.visited = \[\]

\# Сканируем сайт

 

threading.Thread\(target=crawler, 

args=

\('http://brookings.edu'\)\).start\(\)

Это решает проблему состояния гонки, которое могло бы

возникнуть между общими объектами потоков. Если объект не

должен быть общедоступным, то его и не следует таким делать; подобный объект следует хранить в локальной памяти потока. 

Для безопасного общего доступа нескольких потоков к объекту

можно использовать тот же модуль Queue, описанный в

предыдущем подразделе. 

Модуль threading играет роль своеобразной «няни» для

потока, и такие его обязанности можно легко настроить. 

Функция isAlive по умолчанию проверяет, активен ли поток. 

Поток будет активным до тех пор, пока не завершит веб-краулинг \(или не случится сбой\). 

Веб-краулеры часто рассчитаны на очень длительное время

работы. Метод isAlive позволяет гарантировать, что в случае

сбоя потока веб-краулер перезапустится:

threading.Thread\(target=crawler\)

t.start\(\)

 

while True:

time.sleep\(1\)

if not t.isAlive\(\):

t = threading.Thread\(target=crawler\)

t.start\(\)

Чтобы добавить другие методы мониторинга, нужно

расширить объект threading.Thread:

import threading

import time

 

class Crawler\(threading.Thread\):

def \_\_init\_\_\(self\):

threading.Thread.\_\_init\_\_\(self\)

self.done = False

 

def isDone\(self\):

return self.done

 

def run\(self\):

time.sleep\(5\)

self.done = True

raise Exception\('Something bad

happened\!'\)

 

t = Crawler\(\)

t.start\(\)

 

while True:

time.sleep\(1\)

if t.isDone\(\):

print\('Done'\)

break

if not t.isAlive\(\):

t = Crawler\(\)

t.start\(\)

Этот новый класс Crawler содержит метод isDone, который годится для проверки того, завершил ли веб-краулер

работу. Это может быть полезно, если есть какие-то

дополнительные 

методы 

журналирования, 

которые

необходимо завершить перед закрытием потока, при этом

основная часть работы по веб-краулингу уже выполнена. Как

правило, isDone можно заменить каким-либо статусом или

показателем прогресса — например, показывающим текущую

страницу или сколько страниц зарегистрировано. 

Любые исключения, вызванные Crawler.run, приведут к

перезапуску класса, пока isDone не станет равным True, после

чего программа завершит работу. 

Создавая 

классы 

веб-краулеров 

как 

расширения

threading.Thread, можно сделать их более надежными и

гибкими, а также одновременно контролировать все свойства

нескольких веб-краулеров. 

**Многопроцессный веб-краулинг**

Python-модуль Processing создает новые объекты процессов, которые можно запускать и присоединять из главного

процесса. В следующем коде для демонстрации используется

пример FizzBuzz из раздела, посвященного потокам и

процессам. 

from multiprocessing import Process

import time

 

def print\_time\(threadName, delay, iterations\):

start = int\(time.time\(\)\)

for i in range\(0,iterations\):

time.sleep\(delay\)

seconds\_elapsed = str\(int\(time.time\(\)\)

- start\)

print \(threadName if threadName else

seconds\_elapsed\)

 

processes = \[\]

processes.append\(Process\(target=print\_time, 

args=\('Counter', 1, 100\)\)\)

processes.append\(Process\(target=print\_time, 

args=\('Fizz', 3, 33\)\)\)

processes.append\(Process\(target=print\_time, 

args=\('Buzz', 5, 20\)\)\)

![Image 73](images/000000.png)

 

for p in processes:

p.start\(\)

for p in processes:

p.join\(\)

Помните, что операционная система рассматривает каждый

процесс как отдельную независимую программу. Если вы

посмотрите на свои процессы через монитор активности или

диспетчер задач ОС, то увидите картину, похожую на ту, что

показана на рис. 16.1. 

 

**Рис. 16.1. ** Пять процессов Python, выполняемых во время работы FizzBuzz Четвертый процесс с PID 76154 — это действующий

экземпляр Jupyter Notebook, который должен присутствовать, если вы работаете из редактора iPython. Пятый процесс, 83560, является основным потоком выполнения, запускающимся при

первом запуске программы. PID присваиваются операционной

системой последовательно. Если у вас нет другой программы, которая одновременно работает с FizzBuzz и быстро выделяет

PID, то вы должны увидеть еще три последовательных PID — в

данном случае это 83561, 83562 и 83563. 

Эти PID также можно получить из кода с помощью модуля

os:

import os

... 

\# Выводит дочерний PID

os.getpid\(\)

\# Выводит родительский PID

os.getppid\(\)

Каждый процесс программы при выполнении os.getpid\(\) должен выводить свой, отдельный PID, а в момент выполнения

os.getppid\(\) — один и тот же родительский PID. 

Есть пара строк кода, которые в данной конкретной

программе, строго говоря, не нужны. Если не добавить

заключительный оператор join:

for p in processes:

p.join\(\)

то родительский процесс все равно завершится и его

дочерние процессы автоматически подойдут к концу. Однако

данное объединение необходимо, если после завершения этих

дочерних процессов вы захотите выполнить еще какой-либо

код. 

Например:

for p in processes:

p.start\(\)

print\('Program complete'\)

Если убрать оператор join, то результат будет следующим: Program complete

1

2

Если включить оператор join, то программа сначала

дождется завершения всех процессов и только потом

продолжит выполняться:

for p in processes:

p.start\(\)

 

for p in processes:

p.join\(\)

print\('Program complete'\)

 

... 

Fizz

99

Buzz

100

Program complete

Желая 

преждевременно 

остановить 

выполнение

программы, можно, конечно, нажать **Ctrl**\+**C**, чтобы завершить

родительский процесс. При его завершении также подойдут к

концу и все дочерние процессы, которые были им порождены, поэтому можно спокойно использовать **Ctrl**\+**C**, не беспокоясь о

том, что некоторые процессы случайно могут остаться работать

в фоновом режиме. 

**Пример многопроцессного веб-краулинга**

Пример многопоточного веб-краулинга «Википедии» можно

изменить, чтобы использовать отдельные не потоки, а

процессы:

from urllib.request import urlopen

from bs4 import BeautifulSoup

import re

import random

 

from multiprocessing import Process

import os

import time

 

visited = \[\]

def get\_links\(bs\):

 

 

 

 

print\('Getting 

links 

in

\{\}'.format\(os.getpid\(\)\)\)

 

 

 

 

links 

= 

bs.find\('div', 

\{'id':'bodyContent'\}\).find\_all\('a', 

href=re.compile\('^\(/wiki/\)\(\(?\!:\).\)\*$'\)\)

return \[link for link in links if link not

in visited\]

 

def scrape\_article\(path\):

visited.append\(path\)

 

 

 

 

html 

=

urlopen\('http://en.wikipedia.org\{\}'.format\(path

\)\)

time.sleep\(5\)

bs = BeautifulSoup\(html, 'html.parser'\)

title = bs.find\('h1'\).get\_text\(\)

 

 

 

 

print\('Scraping 

\{\} 

in 

process

\{\}'.format\(title, os.getpid\(\)\)\)

links = get\_links\(bs\)

if len\(links\) > 0:

newArticle = links\[random.randint\(0, 

len\(links\)-1\)\].attrs\['href'\]

print\(newArticle\)

scrape\_article\(newArticle\)



processes = \[\]

processes.append\(Process\(target=scrape\_article, 

args=\('/wiki/Kevin\_Bacon',\)\)\)

processes.append\(Process\(target=scrape\_article, 

args=\('/wiki/Monty\_Python',\)\)\)

 

for p in processes:

p.start\(\)

Мы снова искусственно замедляем процесс веб-скрапера, добавляя строку time.sleep\(5\), чтобы его можно было

использовать в качестве примера без чрезмерной нагрузки на

серверы «Википедии». 

Здесь мы заменяем определенную пользователем

переменную thread\_name, передаваемую в качестве

аргумента, на результат вызова функции os.getpid\(\), который не нужно передавать как аргумент и к которому

можно получить доступ в любой момент. 

В результате получим примерно такой результат:

Scraping Kevin Bacon in process 84275

Getting links in 84275

/wiki/Philadelphia

Scraping Monty Python in process 84276

Getting links in 84276

/wiki/BBC

Scraping BBC in process 84276

Getting links in 84276

/wiki/Television\_Centre,\_Newcastle\_upon\_Tyne

Scraping Philadelphia in process 84275

Теоретически веб-краулинг в отдельных процессах должен

выполняться немного быстрее, чем в отдельных потоках, по

двум основным причинам. 

• Процессы не подлежат блокировке GIL. Они могут выполнять

одни и те же строки кода и изменять один и тот же объект

\(на самом деле это разные экземпляры одного и того же

объекта\) одновременно. 

• Процессы могут выполняться на нескольких ядрах

процессора, что позволит обеспечить выигрыш в скорости, если каждый из процессов или потоков интенсивно

использует процессор. 

Однако 

здесь, 

наряду 

с 

преимуществами, 

есть

существенный недостаток. В рассмотренной программе все

найденные URL хранятся в глобальном списке visited. Когда

мы использовали несколько потоков, этот список был общим

для всех потоков; при этом один поток, за редким

исключением состояния гонки, не мог посетить страницу, уже

посещенную другим потоком. Однако теперь каждый процесс

получает 

собственную, 

независимую 

версию 

списка

посещенных страниц и вполне может посещать страницы, уже

просмотренные другими процессами. 

**Обмен данными между процессами**

Каждый процесс работает в собственной, независимой памяти, что может вызвать проблемы, если мы хотим, чтобы процессы

использовали общую информацию. 

Изменив предыдущий пример так, чтобы выводилась

текущая версия списка посещенных страниц, мы можем

увидеть, как работает этот принцип:

def scrape\_article\(path\):

visited.append\(path\)

print\("Process \{\} list is now:

\{\}".format\(os.getpid\(\), visited\)\)

В результате получим следующее:

Process 

84552 

list 

is 

now:

\['/wiki/Kevin\_Bacon'\]

Process 

84553 

list 

is 

now:

\['/wiki/Monty\_Python'\]

Scraping Kevin Bacon in process 84552

Getting links in 84552

/wiki/Desert\_Storm

Process 

84552 

list 

is 

now:

\['/wiki/Kevin\_Bacon', '/wiki/Desert\_Storm'\]

Scraping Monty Python in process 84553

Getting links in 84553

/wiki/David\_Jason

Process 

84553 

list 

is 

now:

\['/wiki/Monty\_Python', '/wiki/David\_Jason'\]

Но есть способ обмена информацией между процессами, работающими на одной машине, через два типа объектов

Python: очереди и каналы. 

*Очередь* похожа на рассмотренную ранее очередь потоков. 

Один процесс может поместить в нее информацию, а другой —

удалить ее оттуда. После удаления этой информации больше

нет в очереди. Поскольку очереди спроектированы как способ

временной передачи данных, они не особенно хорошо

подходят для хранения статических ссылок, таких как список

уже посещенных веб-страниц. 

Но что если заменить этот статический список веб-страниц

каким-либо делегатором веб-скрапинга? Веб-скраперы могут

извлекать задачу из одной очереди в виде пути для

продолжения 

веб-скрапинга 

\(например, 

/wiki/Monty\_Python\), а затем помещать список «найденных

URL» в другую очередь. А она будет обрабатываться

делегатором веб-скрапинга, который следит, чтобы в первую

очередь задач добавлялись только новые URL:

from urllib.request import urlopen

from bs4 import BeautifulSoup

import re

import random

from multiprocessing import Process, Queue

import os

import time

 

def task\_delegator\(taskQueue, urlsQueue\):

\# Инициализируем задачу для каждого

процесса. 

 

 

 

 

visited 

= 

\['/wiki/Kevin\_Bacon', 

'/wiki/Monty\_Python'\]

taskQueue.put\('/wiki/Kevin\_Bacon'\)

taskQueue.put\('/wiki/Monty\_Python'\)

 

while 1:

\# Проверяем, есть ли в urlsQueue

\# новые ссылки, доступные для

обработки. 

if not urlsQueue.empty\(\):

links = \[link for link in

urlsQueue.get\(\) if link not in visited\]

for link in links:

\# Добавляем в taskQueue новую

ссылку. 

taskQueue.put\(link\)

 

def get\_links\(bs\):

 

 

 

 

links 

= 

bs.find\('div', 

\{'id':'bodyContent'\}\).find\_all\('a', 

href=re.compile\('^\(/wiki/\)\(\(?\!:\).\)\*$'\)\)

return \[link.attrs\['href'\] for link in

links\]

 

def scrape\_article\(taskQueue, urlsQueue\):

while 1:

while taskQueue.empty\(\):

\# Засыпаем на 100 мс, ожидая

очередь задач. 

\# Это должно происходить редко. 

time.sleep\(.1\)

path = taskQueue.get\(\)

 

 

 

 

 

 

 

 

html 

=

urlopen\('http://en.wikipedia.org\{\}'.format\(path

\)\)

time.sleep\(5\)

bs = BeautifulSoup\(html, 'html.parser'\)

title = bs.find\('h1'\).get\_text\(\)

print\('Scraping \{\} in process

\{\}'.format\(title, os.getpid\(\)\)\)

links = get\_links\(bs\)

\# Отправляем это на обработку

делегатору. 

urlsQueue.put\(links\)



processes = \[\]

taskQueue = Queue\(\)

urlsQueue = Queue\(\)

processes.append\(Process\(target=task\_delegator, 

args=\(taskQueue, urlsQueue,\)\)\)

processes.append\(Process\(target=scrape\_article, 

args=\(taskQueue, urlsQueue,\)\)\)

processes.append\(Process\(target=scrape\_article, 

args=\(taskQueue, urlsQueue,\)\)\)

 

for p in processes:

p.start\(\)

У этого веб-скрапера есть ряд структурных отличий от

скриптов, созданных нами ранее. Если раньше каждый процесс

или поток выполнял случайный переход от назначенной ему

начальной точки, то теперь они работают совместно, производя полный сбор данных с сайта. Каждый процесс

может извлечь из очереди любую задачу, а не только те ссылки, которые он сам нашел. 

**Многопроцессный веб-краулинг: еще один подход**

Все обсуждаемые здесь подходы многопоточного и

многопроцессного веб-краулинга предполагают необходимость

некоего родительского контроля для дочерних потоков и

процессов. Мы можем их все одновременно запустить и

завершить, передавать сообщения между ними или выделить

для них общую память. 

А если веб-скрапер спроектирован таким образом, что не

нуждается в каких-либо указаниях или обмене данными? Тогда

у вас едва ли найдутся причины, чтобы озадачиваться

импортом \_thread на данном этапе. 

Например, мы хотим параллельно просканировать два

схожих сайта. У нас есть веб-краулер, который может

просканировать любой из них, для чего потребуется небольшое

изменение конфигурации или, вероятно, другие аргументы

командной строки. Нет совершенно никаких причин, по

которым мы не могли бы просто сделать следующее: $ python my\_crawler.py website1

$ python my\_crawler.py website2

И — вуаля\! — мы только что запустили многопроцессный

веб-краулер, сэкономив ресурсы процессора и избежав

издержек на хранение родительского процесса для загрузки\! 

Конечно, представленный подход имеет свои недостатки. 

Если мы хотим запустить таким образом два веб-краулера на

*одном* сайте, то нам понадобится какой-то способ, позволяющий убедиться, что эти веб-краулеры случайно не

станут выполнять веб-скрапинг одних и тех же страниц. Для

решения этой проблемы можно создать URL-правило \(«краулер

1 выполняет веб-скрапинг страниц блога, а краулер 2 —

страниц товаров»\) или разделить сайт еще каким-либо

способом. 

Кроме того, можно организовать координацию через

какую-либо промежуточную базу данных. Прежде чем перейти

по новой ссылке, краулер может сделать запрос к ней и

спросить: «Эта страница уже просканирована?» Веб-краулер

использует базу в качестве системы межпроцессного

взаимодействия. Разумеется, если метод тщательно не

продумать, то он может привести к состоянию гонки или

задержкам в случае медленного соединения с базой данных

\(вероятно, такая проблема возможна только при подключении

к удаленной БД\). 

Кроме того, вы можете обнаружить, что этот метод

масштабируется хуже, чем предыдущие. Использование модуля

Process позволяет динамически увеличивать или уменьшать

количество процессов, сканирующих сайт или даже хранящих

данные. Для их отключения вручную понадобится либо

человек, физически выполняющий скрипт, либо отдельный

управляющий скрипт \(будь то скрипт bash, задание cron или

что-либо еще\). 

Тем не менее я не раз с успехом использовала данный

метод. Для небольших разовых проектов это отличный способ

быстро получить много информации, особенно с нескольких

сайтов. 

**Глава 17. Удаленный веб-скрапинг**

В предыдущей главе мы рассмотрели запуск веб-скраперов в

нескольких потоках и процессах, так что обмен данными

между ними был несколько ограничен или же его приходилось

тщательно планировать. В текущей главе мы доведем эту

концепцию до ее логического завершения: будем запускать

веб-краулеры не только в отдельных процессах, но и на разных

машинах. 

Этой теме не случайно посвящена последняя техническая

глава книги. До сих пор мы запускали все приложения Python из командной строки на своем домашнем компьютере. 

Конечно, мы могли установить MySQL, постаравшись

воспроизвести среду реального сервера. Но это не одно и то же. 

Как говорится, «если ты кого-то любишь — дай ему свободу». 

В данной главе мы рассмотрим несколько способов запуска

скриптов с разных компьютеров и даже с разных IP-адресов. У

вас может возникнуть соблазн отложить этот шаг до лучших

времен, поскольку сейчас он вам *не нужен*. Однако вы будете

удивлены тем, как легко начать работу с уже имеющимися

инструментами \(такими как персональный сайт на платном

хостинге\) и насколько станет проще жить, стоит перестать

пытаться запускать написанные на Python веб-скраперы с

личного ноутбука. 

**Зачем использовать удаленные серверы**

При запуске веб-приложения, предназначенного для широкой

аудитории, использование удаленного сервера может

показаться очевидным шагом, однако инструмент, созданный

для наших целей, мы часто так и продолжаем запускать

локально. Те, кто решил перейти на удаленную платформу, обычно руководствуются двумя основными соображениями: потребностью в большей мощности и гибкости и

необходимостью применять другой IP-адрес. 

**Как избежать блокировки IP-адреса**

При создании веб-скраперов главное правило гласит: подделать можно практически все. Вы можете отправлять

электронные письма с не принадлежащих вам адресов, передавать из командной строки автоматизированные данные

о перемещениях мыши и даже, к ужасу веб-администраторов, передавать трафик их сайтов из Internet Explorer 5.0. 

Единственное, что нельзя подделать, — это ваш IP-адрес. 

Кто угодно может прислать вам письмо с обратным адресом

«Соединенные Штаты Америки, Вашингтон \(округ Колумбия\), Пенсильвания-авеню, 1600, Президент». Однако если письмо

отправлено из Альбукерке, штат Нью-Мексико, вы можете быть

уверены, что не состоите в переписке с президентом

Соединенных Штатов28. 

Большинство усилий, направленных на то, чтобы не

предоставить 

веб-скраперам 

доступ 

к 

сайтам, 

концентрируются на отличии людей от ботов. Дойти до

блокировки IP-адресов — это примерно как фермеру

отказаться от распыления пестицидов и взамен просто

поджечь поле. Это крайняя, хотя и эффективная мера

отбрасывания пакетов, отправленных с проблемных IP-адресов. Однако у такого решения есть свои нюансы. 

• Списки доступа по IP-адресам трудно поддерживать. У

крупных сайтов чаще всего есть собственные программы, автоматизирующие некоторые рутинные операции с такими

списками \(боты, блокирующие боты\!\). Однако все равно кто-

то должен время от времени проверять эти списки или хотя

бы отслеживать их рост и устранять проблемы. 

• Каждый адрес — это дополнительное время обработки, на

которое увеличится время получения пакетов, поскольку

сервер должен будет проверить полученные пакеты по

списку и решить, следует ли их утверждать. Большое

количество адресов, умноженное на большое количество

пакетов, дает быстрый рост затрат времени. Чтобы

сэкономить время и уменьшить сложность обработки, администраторы часто группируют IP-адреса в блоки и

устанавливают правила вроде «заблокировать все 256

адресов из данного диапазона», если найдут несколько тесно

сгруппированных нарушителей. Это приводит нас к

третьему пункту. 

• Блокировка IP-адреса также может привести к блокировке

«хороших парней». Например, когда я училась в

Инженерном колледже им. Франклина В. Олина, один

студент написал программу, которая пыталась подделать

голосование за широко известный контент на сайте

**http://digg.com** \(это было еще до пика популярности Reddit\). 

Один заблокированный IP-адрес привел к тому, что все

общежитие лишилось доступа к сайту. Тот студент просто

перенес свою программу на другой сервер; тем временем

страницы Digg потеряли многих посетителей из своей

основной целевой аудитории. 

Несмотря на все свои недостатки, блокировка IP-адресов

остается чрезвычайно распространенным методом, которым

пользуются 

администраторы, 

чтобы 

запретить

подозрительным веб-скраперам доступ к серверам. Если IP-адрес будет заблокирован, то единственным действенным

решением остается веб-скрапинг с другого IP-адреса. Для этого

можно либо переместить веб-скрапер на другой сервер, либо

построить маршрутизацию трафика через другой сервер с

помощью сервиса наподобие Tor. 

**Портируемость и расширяемость**

Некоторые задачи слишком велики для домашнего компьютера

и подключения к Интернету. И пусть вы не намерены создавать

большую нагрузку на какой-либо сайт, но, возможно, собираете

данные на разных сайтах, для чего потребуются гораздо

большие пропускная способность и объем памяти, чем может

обеспечить ваша текущая конфигурация. 

Более того, избавившись от вычислительно интенсивной

обработки данных, можно высвободить ресурсы домашнего

компьютера для более важных задач \(есть здесь любители

World of Warcraft?\). Вам не придется думать о том, как

обеспечить бесперебойное электропитание и подключение к

Интернету \(можно зайти в Starbucks, запустить приложение, закрыть ноутбук и уйти, зная, что все продолжает надежно

работать\), и вы получите доступ к собранным данным в любом

месте, где есть подключение к Интернету. 

Если ваше приложение требует такой вычислительной

мощности, что его не удовлетворит даже один большой

вычислительный узел Amazon, то можно воспользоваться

*распределенными вычислениями*. Тогда на достижение ваших

целей будут работать параллельно несколько машин. Приведу

простой пример: у вас есть один компьютер, который

сканирует один набор сайтов, и второй компьютер, который

сканирует второй набор сайтов; оба компьютера сохраняют

собранные данные в общей базе. 

Конечно, как отмечалось в предыдущих главах, кто угодно

может скопировать систему поиска Google, но мало кто сможет

повторить масштаб, в котором Google выполняет свой поиск. 

Распределенные 

вычисления 

— 

обширная 

область

информатики, выходящая за рамки данной книги. Однако

знание того, как запустить приложение на удаленном сервере, является необходимым первым шагом, и вы будете удивлены

тем, на что способны современные компьютеры. 

**Tor**

Сеть Onion Router, более известная под аббревиатурой *Tor*, представляет собой сеть волонтерских серверов, настроенных

на маршрутизацию и перенаправление трафика через многие

уровни \(именно поэтому в названии присутствует слово Onion

— «репчатый лук»\) разных серверов, чтобы скрыть

происхождение этого трафика. Перед поступлением в сеть

данные зашифровываются, и поэтому, если окажется, что

какой-нибудь сервер прослушивается, природа коммуникации

не будет раскрыта. Кроме того, хоть входящий и исходящий

поток 

данных 

любого 

сервера 

и 

может 

быть

скомпрометирован, чтобы расшифровать истинные начальные

и конечные точки соединения, необходимо знать детали о

входящих и исходящих данных для *всех* серверов

коммуникационного пути — подвиг на грани невозможного. 

Tor часто используют правозащитники и анонимные

политические информаторы для общения с журналистами, вследствие чего значительную часть финансирования эта сеть

получает от правительства США. Конечно же, она также широко

используется для незаконной деятельности, из-за чего

является постоянным объектом государственного надзора

\(хотя пока что он имел в лучшем случае переменный успех\). 

![Image 74](images/000028.png)

**Ограничения анонимности Tor**

Несмотря на то что причина, по которой мы будем использовать

Tor в этой книге, состоит в изменении своего IP-адреса, а не в

получении полной анонимности как таковой, все же стоит

уделить время изучению некоторых сильных сторон и

ограничений возможностей Tor по созданию анонимного

трафика. 

Используя Tor, можно предположить, что IP-адрес, с которого

мы пришли на веб-сервер, не является для этого сервера тем

IP-адресом, по которому нас можно проследить. Однако нас

может выдать любая информация, предоставляемая нами веб-серверу. Например, если вы войдете в свою учетную запись

Gmail и затем выполните подозрительный поиск в Google, то

его можно будет привязать к вашей личности. 

Однако, помимо очевидного, сама аутентификация в Tor способна раскрыть вашу анонимность. В декабре 2013 года

студент Гарвардского университета, чтобы не сдавать

выпускные экзамены, отправил через сеть Tor сообщение о том, что учебное заведение заминировано, используя анонимную

учетную запись электронной почты. Когда сотрудники IT-отдела

Гарварда просмотрели свои журналы, то обнаружили

следующее: во время отправки сообщения об угрозе взрыва

трафик, поступивший в сеть Tor, попадал туда только с одной

машины, зарегистрированной на студента, имя которого было

известно. Несмотря на то что исходный пункт этого трафика

определить не удалось \(только его передачу через Tor\), самого

факта совпадения времени и того, что в это время был

зарегистрирован вход в систему только с одной машины, было

достаточно для того, чтобы на этого студента подали в суд. 

Аутентификация в Tor — не автоматическая мантия-невидимка, которая предоставляла бы вам полную свободу делать в

Интернете все что угодно. Да, это полезный инструмент, однако

всегда применяйте его с умом, осторожностью и, конечно же, с

благими намерениями. 

Как вы узнаете в следующем подразделе, для применения

Tor в Python необходимо установить и запустить Tor. К счастью, установить и запустить сервис Tor очень легко. Просто

перейдите 

на 

страницу 

скачивания 

Tor

\(**https://www.torproject.org/download/download**\), скачайте, 

установите, откройте и подключите его. И учтите, что при

использовании Tor скорость соединения с Интернетом может

показаться ниже обычной. Будьте терпеливы — возможно, данные, прежде чем попасть к вам, несколько раз обогнут

земной шар\! 

**PySocks. ** PySocks — удивительно простой модуль Python, способный маршрутизировать трафик через прокси-серверы и

просто фантастически эффективный в сочетании с Tor. PySocks можно 

скачать 

с 

его 

сайта

\(**https://pypi.python.org/pypi/PySocks/1.5.0**\) или использовать для

его установки любой из многочисленных сторонних

менеджеров модулей. 

У этого модуля не очень много документации, однако

пользоваться им чрезвычайно просто. Перед выполнением

следующего кода необходимо запустить сервис Tor на порте

9150 \(порте, применяемом по умолчанию\):

import socks

import socket

from urllib.request import urlopen

 

socks.set\_default\_proxy\(socks.SOCKS5, 

"localhost", 9150\)

socket.socket = socks.socksocket

print\(urlopen\('http://icanhazip.com'\).read\(\)\)

Сайт **http://icanhazip.com** показывает только IP-адрес

клиента, подключающегося к серверу, и может быть полезен

для тестирования. При выполнении этот скрипт должен

отображать 

IP-адрес, 

который 

не 

является 

вашим

собственным. 

Если вы хотите использовать Selenium и ChromeDriver в

сочетании с Tor, то PySocks вообще не нужен — просто не

забудьте сначала запустить Tor и укажите дополнительный

параметр proxy-server для Chrome, согласно которому

Selenium будет подключаться через порт 9150 по протоколу

socks5:

from selenium import webdriver

from selenium.webdriver.chrome.options import

Options

 

chrome\_options = Options\(\)

chrome\_options.add\_argument\("--headless"\)

chrome\_options.add\_argument\("--proxy-server=socks5://127.0.0.1:9150"\)

driver 

=

webdriver.Chrome\(executable\_path='drivers/chrom

edriver', 

options=chrome\_option

s\)

 

driver.get\('http://icanhazip.com'\)

print\(driver.page\_source\)

driver.close\(\)

Этот скрипт также должен выводить не ваш IP-адрес — на

сей раз тот, который в данный момент использует работающий

у вас клиент Tor. 

**Удаленный хостинг**

Хотя на полную анонимность в Интернете рассчитывать не

стоит — она теряется, стоит вам лишь указать данные

кредитной карты, — однако удаленное размещение веб-скраперов также способно значительно повысить скорость их

работы. Это объясняется не только возможностью покупать

машинное время на гораздо более мощных компьютерах, чем

ваш, но и тем, что по дороге к точке назначения соединение

больше не будет проходить через слои сети Tor. 

**Запуск веб-скрапера из учетной записи веб-хостинга**

Если у вас есть личный или корпоративный сайт, то, вероятно, уже есть все, что нужно для запуска веб-скрапера с внешнего

сервера. Даже на веб-серверах с относительно ограниченным

доступом, где нет возможности выполнять команды из

командной строки, можно запускать и останавливать скрипты

через веб-интерфейс. 

В случае размещения вашего сайта на Linux-сервере тот, скорее всего, сам работает на Python. Если же вы используете

хостинг на Windows-сервере, то вам повезло меньше: придется

специально проверить, установлен ли там Python или

согласится ли администратор сервера его установить. 

Большинство 

мелких 

провайдеров 

веб-хостинга

предоставляют программное обеспечение под названием

cPanel, которое используется для выполнения простейших

операций по администрированию, а также для предоставления

информации о вашем сайте и связанных с ним сервисах. При

наличии доступа к cPanel для проверки того, установлен ли на

вашем сервере Python, нужно перейти в раздел Apache Handlers и добавить новый обработчик \(если его там еще нет\): Handler: cgi-script

Extension\(s\): .py

Эти строки дают указание вашему серверу выполнять все

скрипты Python как скрипты CGI \(Common Gateway Interface —

общий шлюзовой интерфейс\). Это любая программа, которая

может выполняться на сервере и динамически генерировать

контент для отображения на сайте. Явно определяя скрипты

Python как скрипты CGI, мы разрешаем серверу их выполнять, а не просто отображать содержимое в браузере или позволять

пользователю его скачивать. 

Напишите скрипт Python, загрузите его на сервер и

определите права доступа к файлу как 755, то есть разрешите

выполнять этот файл как программу. Для выполнения скрипта

перейдите в тот каталог, в который вы его загрузили, через

браузер \(или, что еще лучше, напишите веб-скрапер, который

будет делать это вместо вас\). Если вы не хотите, чтобы скрипт

попал в открытый доступ и его мог выполнить любой

желающий, то есть два варианта этого избежать. 

• Храните скрипт под нелогичным или скрытым URL и следите

за тем, чтобы никогда не указывать ссылку на скрипт с

какого-либо другого доступного URL во избежание его

индексации поисковыми системами. 

• Защитите скрипт паролем или потребуйте, чтобы перед

выполнением скрипта ему передавался пароль или

секретный маркер. 

Конечно, запуск скрипта Python из сервиса, изначально

предназначенного для отображения сайтов, напоминает

хакерский прием. В частности, вы, вероятно, заметите, что ваш

«веб-скрапер через сайт» загружается немного медленно. На

самом деле страница вообще будет отображаться лишь после

выполнения всего веб-скрапинга \(и только тогда произойдет

вывод данных, который вы запрограммировали с помощью

операторов print\). Это может занять несколько минут, часов

или вообще не завершится никогда, в зависимости от того, как

написан скрипт. Несмотря на то что он, безусловно, выполняет

свою работу, вам может потребоваться более подробный вывод

результатов в реальном времени. Для этого понадобится

сервер, предназначенный не только для сайтов. 

**Запуск из облака**

На заре компьютерных технологий программисты платили за

машинное время или резервировали его для выполнения кода. 

С 

появлением 

персональных 

компьютеров 

такая

необходимость отпала: вы просто пишете и выполняете код на

своей машине. Но сейчас амбиции приложений опережают

разработку микропроцессоров до такой степени, что

программисты снова стали возвращаться к почасовой плате за

вычисления. 

Однако на сей раз пользователи платят не за время одной

физической машины, а за эквивалентную вычислительную

мощность, которая часто распределяется между несколькими

компьютерами. Облачная структура этой системы позволяет

определять стоимость вычислительной мощности по времени

пикового спроса. Например, Amazon позволяет делать наценку

на «точечные вычислительные узлы», когда низкая цена

важнее реагирования в реальном времени. 

Виртуальные вычислительные узлы также являются более

специализированными, их можно выбирать в зависимости от

потребностей приложения — в большем объеме оперативной

памяти или высокой скорости вычислений. Хотя веб-скраперы

обычно не используют много памяти, возможно, при

размещении приложения веб-скрапера стоит выбрать не

универсальный вычислительный узел, а вариант с большим

хранилищем для данных или более быстрыми вычислениями. 

Если вам предстоит обработка больших объемов текстов на

естественном языке, распознавание текста или поиск путей в

графах \(например, при решении задачи «Шесть шагов по

“Википедии”»\), то вам может подойти вычислительный узел с

высокой скоростью вычислений. При сборе больших объемов

данных, сохранении файлов или выполнении обширной

аналитики, возможно, стоит выбрать вариант с оптимизацией

хранилища. 

Хоть нас не ограничивает ничто, кроме денег, на момент

написания этой книги стоимость аренды вычислительного узла

начиналась всего с 1,3 цента в час \(для микроузла Amazon EC2\), а самый дешевый вычислительный узел Google стоил 4,5 цента

в час с минимальным временем аренды всего десять минут. 

Благодаря экономии на масштабе арендовать небольшой

вычислительный узел в крупной компании стоит почти столько

же, сколько собственная физическая выделенная машина, за

исключением того, что не придется нанимать IT-специалиста, который будет за ней присматривать. 

Конечно, пошаговые инструкции по настройке и запуску

вычислительных узлов для облачных вычислений несколько

выходят за рамки этой книги, но вы, скорее всего, обнаружите, что пошаговые инструкции и не нужны. Поскольку Amazon и

Google \(не говоря уже о бесчисленных мелких компаниях в

данной индустрии\) борются за доллары облачных вычислений, они упростили настройку новых вычислительных узлов, так

что вам остается лишь следовать простым подсказкам, придумать имя приложения и предоставить номер кредитной

карты. Кроме того, на момент написания этой книги Amazon и

Google предлагали бесплатное машинное время на сотни

долларов для дальнейшего привлечения новых клиентов. 

Настроив вычислительный узел, вы станете гордым

владельцем IP-адреса, имени пользователя, а также открытого

и закрытого ключей, которые можно будет применять для

подключения к вашему вычислительному узлу через SSH. С

этого момента все будет аналогично работе на сервере, которым вы физически владеете, за исключением того, что

вам, естественно, больше не придется беспокоиться о

техническом обслуживании оборудования или использовании

многочисленных собственных инструментов мониторинга. 

Я обнаружила, что если нужно что-то сделать «дешево и

сердито», особенно при отсутствии большого опыта работы с

SSH и парами ключей, то проще наладить и сразу запустить

вычислительные узлы Google Cloud Platform. У них есть

простой загрузчик и даже кнопка, доступная сразу после

![Image 75](images/000055.png)

запуска, которая позволяет просматривать терминал SSH

прямо в браузере, как показано на рис. 17.1. 

 

**Рис. 17.1. ** Открытый в браузере терминал работающего вычислительного узла виртуальной

машины Google Cloud Platform

**Дополнительные ресурсы**

Много лет назад в облаке работали главным образом те, кто

захотел и смог пробраться сквозь дебри документации и уже

имел опыт администрирования серверов. Однако сегодня

благодаря возросшей популярности и конкуренции среди

поставщиков облачных вычислений инструменты стали

гораздо лучше. 

Но все же для создания крупномасштабных или более

сложных веб-скраперов и краулеров вам могут понадобиться

дополнительные рекомендации по созданию платформы для

сбора и хранения данных. 

Книга *Google Compute Engine* Марка Коэна \(Marc Cohen\), Кэтрин Херли \(Kathryn Hurley\) и Пола Ньюсона \(Paul Newson\) \(издательство O’Reilly\) \(**http://oreil.ly/1FVOw6y**\) — это простой

источник информации о применении облачных вычислений

Google на Python и JavaScript. В ней рассматривается не только

пользовательский интерфейс Google, но и инструменты

командной строки и скрипты, которые можно задействовать

для повышения гибкости приложений. 

Если вы предпочитаете работать с Amazon, то обратите

внимание на книгу *Python and AWS Cookbook* Митча Гарната

\(Mitch Garnaat\) \(издательство O’Reilly\) \(**http://oreil.ly/VSctQP**\) —

краткое, но чрезвычайно полезное руководство, которое

поможет начать работу с Amazon Web Services и покажет, как

настроить и запустить масштабируемое приложение. 

28 Технически IP-адреса можно подделать в исходящих пакетах. Этот метод

используется в распределенных атаках типа «отказ в обслуживании», когда

злоумышленникам неважно получить ответные пакеты \(которые если и будут

отправлены, то доставятся по неправильному адресу\). Но веб-скрапинг по

определению является действием, в котором ожидается ответ от веб-сервера, поэтому для нас IP-адрес — то, что подделать невозможно. 

**Глава 18. Законность и этичность веб-скрапинга**

В 2010 году инженер-программист Пит Уорден \(Pete Warden\) разработал веб-краулер для сбора данных из Facebook. Он

собрал данные примерно 200 миллионов пользователей

соцсети: имена, места жительства, сведения о друзьях и

интересах. Конечно же, в Facebook это заметили и стали слать

Уордену 

письма-предупреждения 

о 

нарушении 

прав

интеллектуальной собственности, к которым он прислушался. 

Когда Уордена спросили, почему он подчинился требованиям

прекратить нарушать правила, он ответил: «Большие данные

дешевы, а вот адвокаты — нет». 

В этой главе вы познакомитесь с законами США \(и

некоторыми 

международными 

законами\), 

имеющими

отношение к веб-скрапингу, и узнаете, как оценить степень

законности и этичности веб-скрапинга в той или иной

ситуации. 

Прежде чем приступить к чтению раздела ниже, констатирую очевидный факт: я не юрист, я инженер-программист. Не интерпретируйте то, что вы прочтете здесь

или в любой другой главе этой книги, как профессиональную

юридическую консультацию. Я действительно считаю, что могу

компетентно обсуждать законность и этичность веб-скрапинга. 

Однако прежде, чем начинать какие-либо юридически

неоднозначные проекты по веб-копированию, вам следует

проконсультироваться с юристом \(а не с инженером-программистом\). 

Цель главы — дать вам основы, которые позволят изучать и

обсуждать различные аспекты законности веб-скрапинга, такие 

как 

интеллектуальная 

собственность, 

несанкционированный доступ к компьютеру и использование

![Image 76](images/000003.png)

![Image 77](images/000032.png)

сервера. Однако это не заменит вам настоящую юридическую

консультацию. 

**Торговые марки, авторские права, патенты… спасите-помогите\! **

Пора пройти краткий курс по основам интеллектуальной

собственности\! Есть три основных ее типа: торговые марки

\(обозначаемые символом TM или 

\), авторские права

\(вездесущий \) и патенты \(иногда обозначаемые текстом, где

написано, что данное изобретение защищено патентом, или

номером патента, но часто не обозначаемые вообще никак\). 

Патенты используются только для объявления прав

собственности на изобретения. Нельзя запатентовать

изображение, текст или любую другую информацию. Впрочем, некоторые патенты, например на программное обеспечение, менее материальны, чем то, что мы привыкли считать

изобретениями, однако имейте в виду: патентуется *вещь* \(или

методика\), а не информация, содержащая ся в патенте. Если вы

не создаете что-то по чертежам, собранным с помощью веб-скрапинга, и не пользуетесь методом веб-скрапинга, который

запатентовал кто-то другой, то вряд ли случайно нарушите

какой-либо патент, просто выполнив веб-скрапинг сайта. 

Торговые марки также вряд ли будут проблемой, однако их

уже необходимо учитывать. Патентное ведомство США

утверждает следующее:

Торговая марка — это слово, фраза, символ и/или дизайн, которые идентифицируют и отличают источник товаров одной

компании от товаров другой. Знак обслуживания — это слово, фраза, символ и/или дизайн, который идентифицирует и

отличает источник услуг, а не товаров. Термином «торговая

марка» часто обозначают не только сами торговые марки, но и

знаки обслуживания. 

Помимо традиционного брендинга с помощью слов и

символов, который мы имеем в виду, когда говорим о торговых

марках, бывают и другие описательные атрибуты. Это может

быть, например, форма контейнера \(вспомните бутылки Coca-Cola\) или даже цвет \(сразу вспоминается розовый цвет

стекловолоконной изоляции Owens Corning Pink Panther\). 

В отличие от патентов, право собственности на торговую

марку сильно зависит от контекста, в котором она

используется. Например, захотев опубликовать пост в блоге и

сопроводить его изображением логотипа Coca-Cola, я могу это

сделать \(если только не намекаю на то, что спонсором или

автором моего поста является Coca-Cola\). Захоти я изготовить

новый безалкогольный напиток и поставить на упаковке такой

же логотип, как у Coca-Cola, — это было бы явно незаконным

использованием торговой марки. Точно так же я могу

изобразить на упаковке своего нового безалкогольного напитка

Розовую Пантеру, однако не могу задействовать этот же цвет

при создании утеплителя для стен. 

**Закон об авторском праве. ** У торговых марок и патентов

есть общая черта: их признают только в том случае, если они

официально зарегистрированы. Вопреки распространенному

мнению, это не относится к материалам, защищенным

авторским правом. Что делает изображения, текст, музыку и

т.п. защищенными авторским правом? Не предупреждение

«Все права защищены», написанное внизу страницы, и вообще

ничего, что отличало бы опубликованные материа лы от

неопубликованных. Каждый созданный вами материал

автоматически подпадает под действие закона об авторском

праве сразу же после того, как вы его создали. 

Международным стандартом авторского права является

Бернская 

конвенция 

об 

охране 

литературных 

и

художественных произведений, получившая свое название

благодаря городу Берн в Швейцарии, где была принята в 1886

году. Эта конвенция, по сути, гласит: все страны, которые

являются ее участницами, должны признавать охрану

авторских прав произведений, созданных гражданами других

стран — участниц конвенции, так, как если бы они были

гражданами данной страны. На практике это значит, что, будучи гражданином США, вы можете быть привлечены к

ответственности в Соединенных Штатах за нарушение

авторских прав на материалы, написанные кем-либо, скажем, во Франции \(и наоборот\). 

Очевидно, что для веб-скраперов авторские права являются

проблемой. Если я соберу контент чьего-то блога и опубликую

его в своем, то с уверенностью могу рассчитывать на судебный

процесс. К счастью, у меня есть несколько уровней защиты, которые позволят сделать мой проект веб-скрапинга блога

оправданным, в зависимости от того, как он функционирует. 

Во-первых, защита авторских прав распространяется только

на творческие произведения, но не на статистику или факты. К

счастью, большая часть того, за чем охотятся веб-скраперы, —

*именно* статистика и факты. Веб-скрапер, собирающий стихи со

всего Интернета и отображающий всю эту поэзию на вашем

личном сайте, пожалуй, нарушал бы закон об авторском праве, но веб-скрапер, который собирает информацию о частоте

стихотворных публикаций в разное время, не нарушает закон. 

Поэзия в чистом виде — творческое произведение. Однако

среднее количество слов в стихах, опубликованных на сайте в

разные месяцы, — не творческое произведение, а фактические

данные. 

Материалы, которые публикуются дословно \(в отличие от

агрегированных или вычисленных на основе необработанных

данных веб-скрапинга\), могут не нарушать закон об авторском

праве, если эти данные представляют собой цены, имена

руководителей компаний или какую-либо другую фактическую

информацию. 

Даже материал, защищенный авторским правом, может

быть использован напрямую — в разумных пределах — в

соответствии с Законом о защите авторских прав в цифровую

эпоху \(Digital Millennium Copyright Act, DMCA\). В Законе

изложены некоторые правила автоматической обработки

материалов, защищенных авторским правом. Текст DMCA длинный, со множеством подробных правил, регулирующих

все: от электронных книг до телефонов. Тем не менее в нем

есть два основных момента, которые могут иметь особое

отношение к веб-скрапингу. 

• Вы в «зоне безопасности», если извлекаете информацию из

источника, который, по вашему мнению, содержит только

материалы, не защищенные авторским правом, однако

пользователь предоставил защищенные сведения. Все в

порядке, если вы удалили данные, защищенные авторским

правом, сразу после получения уведомления. 

• Собирая контент, вы не имеете права обходить меры

безопасности \(такие как защита паролем\). 

Короче говоря, вы никогда не должны публиковать

защищенные авторским правом материалы напрямую, без

разрешения первоначального автора или правообладателя. 

Если вы храните защищенный авторским правом материал, к

которому у вас есть свободный доступ, в своей непубличной

базе данных с целью анализа — все в порядке. А вот

опубликовав эту базу на своем сайте для просмотра или

скачивания, вы поступите нехорошо. Если вы анализируете эту

БД и публикуете статистику по количеству слов, список авторов

в порядке плодовитости или выводите другие результаты

метаанализа данных — это нормально. Сопровождение этого

метаанализа несколькими избранными цитатами или

краткими образцами данных с целью подтвердить свою точку

зрения — вероятно, тоже допустимо, но для надежности стоит

свериться с положением о правомерном использовании в своде

законов США. 

**Посягательство на движимое имущество**

*Посягательство на движимое имущество* принципиально

отличается от того, что мы называем «нарушением закона о

владении собственностью», в том смысле, что оно

распространяется не на недвижимость или землю, а на

движимое имущество \(такое как сервер\). Этот закон

применяется в тех случаях, когда ваш доступ к

собственности нарушается неким образом: вы не можете либо

получить к ней доступ, либо использовать ее. 

В эпоху облачных вычислений заманчиво забыть, что веб-серверы — это реальные, материальные ресурсы. Однако

серверы не только состоят из дорогих компонентов. Их еще

нужно хранить, контролировать, охлаждать и снабжать

огромным количеством электроэнергии. По ряду оценок, 10 %

мирового потребления электроэнергии приходится на

компьютеры29. \(Если вы подсчитали, сколько потребляет вся

ваша электроника, и все еще в чем-то сомневаетесь, то

посмотрите на необозримые серверные фермы Google, которые

необходимо подключать к крупным электростанциям.\)

Серверы являются дорогостоящими ресурсами, однако с

юридической точки зрения интересны следующим: веб-мастера, как правило, действительно *хотят*, чтобы люди

применяли их ресурсы \(то есть получали доступ к их сайтам\); они лишь не хотят, чтобы пользователи потребляли *слишком*

*много* ресурсов. Одно дело — посмотреть сайт через браузер, и

совсем другое — запустить полномасштабную DDoS-атаку. 

Есть три критерия, определяющих, совершил ли веб-скрапер посягательство на движимое имущество. 

• *Отсутствие согласия. * Поскольку веб-серверы являются

общедоступными, они, как правило, «дают согласие» и на

веб-скрапинг. Однако на многих сайтах соглашения о

предоставлении услуг прямо запрещают использование веб-скраперов. Кроме того, любые доставленные вам

предупреждения о нарушении прав интеллектуальной

собственности явно аннулируют это согласие. 

*• Фактический вред. * Серверы стоят дорого. Помимо стоимости

самого сервера, если из-за ваших веб-скраперов сайт выйдет

из строя или ограничится его способность обслуживать

других пользователей, то это может быть расценено как

вред, который вы причинили. 

*• Преднамеренность. * Если вы написали код, то наверняка

знали, что он делает\! 

Чтобы на вас подали заявление о нарушении правил

использования, вы должны соблюсти все три эти критерия. 

Однако если вы нарушите соглашение об условиях

обслуживания, но не причините реального вреда, то не

думайте, что застрахованы от судебных исков. Вы вполне

можете нарушить закон об авторском праве, DMCA, Акт о

компьютерном мошенничестве и злоупотреблении \(подробнее

о нем см. далее\) или еще какой-либо из множества других

законов, применимых к веб-скраперам. 

**Придержите ваши боты\! **

Были времена, когда веб-серверы были гораздо мощнее, чем

персональные компьютеры. Фактически под *сервером* всегда

подразумевался *большой компьютер*. Сейчас положение

немного изменилось. Например, частота процессора моего

персонального компьютера составляет 3,5 ГГц, а объем

оперативной памяти — 8 Гбайт. И напротив, средний

вычислительный узел Amazon \(на момент написания этой

книги\) имел 4 Гбайт оперативной памяти и около 3 Ггерц

вычислительной мощности. 

При хорошей скорости подключения к Интернету и наличии

выделенного компьютера достаточно одного персонального

компьютера, чтобы создать большую нагрузку на многие

сайты и даже причинить им вред или полностью вывести из

строя. Если речь не идет о скорой медицинской помощи, при

которой только сбор всех данных с сайта Васи Пупкина за две

секунды спасет жизнь пациента, — нет причин

бомбардировать сайт своими запросами. 

Управляемый бот никогда не завершает работу. Иногда лучше

включить веб-краулер на ночь, чем днем или вечером, и на то

есть следующие причины. 

Имея в распоряжении около восьми часов, даже при

черепашьей скорости две секунды на страницу можно

просмотреть более 14 000 страниц. Когда время не

является проблемой, нет соблазна увеличивать скорость

веб-краулеров. 

Если предположить, что целевая аудитория сайта

географически находится там же, где и вы \(для

удаленных целевых аудиторий внесите соответствующие

коррективы\), то в ночное время нагрузка на сайт должна

значительно снижаться. Это значит, что ваш сбор данных

не совпадет с временем пикового трафика. 

Вы экономите время — спите вместо того, чтобы то и дело

проверять журналы на наличие новой информации. 

Подумайте о том, какой сюрприз вас будет ждать утром в

виде порции свежих данных\! 

Рассмотрим следующие сценарии:

у вас есть веб-краулер, просматривающий сайт Васи

Пупкина, собирая с него некоторые или все данные; у вас есть веб-краулер, просматривающий несколько сотен

небольших сайтов, собирая с них некоторые или все

данные; 

у вас есть веб-краулер, просматривающий очень большой

сайт, такой как «Википедия». 

В первом случае лучше оставить бот работать на ночь и

медленно. 

Во втором лучше обращаться к каждому сайту по очереди —

вместо того чтобы работать медленно, перебирать их по

одному. В зависимости от количества сканируемых сайтов это

значит, что вы можете собирать данные с той скоростью, которую способны обеспечить ваше интернет-соединение и

компьютер, и при этом нагрузка на каждый из удаленных

серверов останется в разумных пределах. Это можно сделать

программно, используя несколько потоков \(так что каждый из

них будет проверять свой сайт и приостанавливать свое

выполнение\), или же задействовать списки Python для

отслеживания сайтов. 

В третьем варианте нагрузка, которую ваше интернет-соединение и домашний компьютер могут создать для такого

сайта, как «Википедия», вряд ли будет замечена или вызовет

беспокойство. Но вот если вы используете распределенную

сеть машин — тогда, очевидно, другое дело. Соблюдайте

осторожность и по возможности обратитесь к представителю

компании. 

**Акт о компьютерном мошенничестве и злоупотреблении**

В начале 1980-х компьютеры начали выходить из

академических аудиторий и проникать в мир бизнеса. Впервые

вирусы и «черви» стали восприниматься как нечто большее, чем просто неудобство \(или даже забавное хобби\), — серьезное

уголовное преступление, способное причинить материальный

ущерб. Наконец, в 1986 году появился Акт о компьютерном

мошенничестве и злоупотреблении \(Computer Fraud and Abuse Act, CFAA\). 

На первый взгляд может показаться, что этот закон касается

только обычных злостных хакеров, распространяющих вирусы, однако он имеет серьезные последствия и для веб-скраперов. 

Представьте скрапер, который сканирует сеть в поисках форм

аутентификации с легко угадываемыми паролями или же

собирает государственные секреты, случайно оказавшиеся в

скрытом, но доступном месте. В соответствии с CFAA все

описанные действия являются незаконными \(и это

справедливо\). 

Закон определяет семь основных уголовных преступлений, которые можно кратко описать следующим образом:

• намеренный несанкционированный доступ к компьютерам, принадлежащим правительству США, и получение с них

информации; 

• намеренный несанкционированный доступ к компьютеру и

получение финансовой информации; 

• намеренный несанкционированный доступ к компьютеру, принадлежащему правительству США, повлиявший на

использование этого компьютера правительством; 

• намеренный доступ к любому защищенному компьютеру с

попыткой обмана; 

• намеренный доступ к компьютеру без разрешения и

причинение ущерба этому компьютеру; 

• разглашение или передача паролей либо информации об

авторизации 

на 

компьютерах, 

используемых

правительством США, или компьютерах, которые оказывают

влияние на межгосударственную или внешнюю торговлю; 

• попытки вымогательства денег или «чего-либо ценного»

путем нанесения ущерба или угрозы причинить вред

любому защищенному компьютеру. 

Короче говоря: держитесь подальше от защищенных

компьютеров, не заходите на компьютеры \(в том числе на веб-серверы\), к которым у вас нет доступа, и особенно держитесь

подальше от компьютеров, принадлежащих правительству или

финансовым организациям30. 

**Файл robots.txt и условия использования**

С юридической точки зрения условия использования сайта и

файлы robots.txt находятся на интересной территории. Если

сайт общедоступен, то право веб-мастера заявлять о том, что

ПО может или не может получить к нему доступ, спорно. Было

бы странным заявить, что «для просмотра данного сайта вы

можете использовать браузер, но не программу, которую сами

написали с этой целью». 

На большинстве сайтов в нижней части каждой страницы

есть ссылка на страницу «Условия использования» \(Terms of Service, TOS\). Это нечто большее, чем просто правила для веб-краулеров и автоматического доступа; здесь часто содержатся

сведения о том, какая информация собрана на веб-сайте, что с

ней здесь делается, и, как правило, юридическое уведомление

о том, что услуги, предоставляемые сайтом, не подразумевают

неких явных или скрытых гарантий. 

Если вас интересуют оптимизация сайта для поисковых

систем \(Search Engine Optimization, SEO\) или технологии

поиска, то вы, вероятно, слышали о файле robots.txt. Зайдя

почти на любой крупный сайт и поискав там файл

robots.txt, его можно обнаружить в корневой веб-папке: http://website.com/robots.txt. 

Синтаксис файлов robots.txt был разработан в 1994 году, еще во время первой волны технологии веб-поиска. Примерно

в это же время поисковые системы, обыскивающие весь

Интернет, такие как AltaVista и DogPile, начали всерьез

конкурировать 

с 

простыми 

списками 

сайтов, 

организованными по темам, которые велись, в частности, на

Yahoo\!. Развитие поиска в Интернете привело к взрывному

росту не только количества веб-краулеров, но и доступности

для обычных людей информации, собираемой этими

краулерами. 

Несмотря на то что сегодня мы привыкли считать такой вид

доступности чем-то само собой разумеющимся, некоторые

веб-мастера 

были 

шокированы, 

когда 

информация, 

опубликованная ими глубоко в файловой структуре своего

сайта, стала появляться на первых страницах результатов

поиска в основных поисковых системах. В ответ появился

синтаксис файлов robots.txt под названием Robots Exclusion Standard — «стандарт исключения для роботов». 

В отличие от TOS, где о веб-краулерах часто говорится в

широком смысле и на вполне человеческом языке, файл

robots.txt может быть очень легко проанализирован и

использован автоматизированными программами. На первый

взгляд может показаться, что это идеальное средство

окончательного решения проблемы нежелательных ботов, однако учтите следующее. 

• Никакого официального руководящего органа по синтаксису

robots.txt нет. Это всего лишь широко распространенное

и в целом хорошо соблюдаемое соглашение, но ничто не

мешает кому угодно создать собственную версию файла

robots.txt \(кроме того факта, что ни один бот не сможет

его распознать или выполнить, пока данный формат не

станет популярным\). Другими словами, мы имеем дело с

широко распространенным соглашением — главным

образом потому, что оно относительно простое и у

компаний нет причин изобретать собственный стандарт или

пытаться улучшить существующий. 

• Нет способа заставить кого-либо выполнять файл

robots.txt. Файл — лишь знак, говорящий: «Пожалуйста, не заходите в эти части сайта». Есть множество библиотек

веб-скрапинга, соблюдающие условия robots.txt \(хотя это

часто всего лишь настройка по умолчанию, которую можно

изменить\). Кроме того, часто встречаются и другие

препятствия для выполнения условий robots.txt \(в конце

концов, вам нужно провести веб-скрапинг и синтаксический

анализ содержимого страницы и применить к этому

контенту логику вашего кода\), которые не позволяют просто

продвигаться вперед и выполнять веб-скрапинг всех нужных

вам страниц. 

Синтаксис Robot Exclusive Standard очень прост. Как и в

Python \(и во многих других языках\), комментарии здесь

начинаются с символа \#, заканчиваются символом новой

строки и могут использоваться в любом месте файла. 

Первая строка файла после всех комментариев должна

начинаться с User-agent:, после чего указывается

пользователь, к которому применяются следу ющие правила. 

Далее идет набор правил: Allow: или Disallow: в

зависимости от того, разрешается ли боту заходить в данный

раздел сайта. Звездочка \(\*\) является подстановочным знаком и

может использоваться для описания User-agent или URL. 

Если последовательность правил кажется противоречивой, то последнее правило имеет приоритет. Например:

\# Welcome to my robots.txt file\! 

User-agent: \*

Disallow: \*

 

User-agent: Googlebot

Allow: \*

Disallow: /private

В этом случае запрещается доступ любых ботов к любой

точке сайта, за исключением бота Google, которому

разрешается доступ куда угодно, кроме каталога **/private**. 

В Twitter файл robots.txt содержит подробные

инструкции для ботов Google, Yahoo\!, «Яндекса», Microsoft, а

также других ботов и поисковых систем, не относящихся ни к

одной из предыдущих категорий. Раздел Google \(который

практически не отличается от разрешений для остальных

категорий ботов\) выглядит следующим образом:

\# Google Search Engine Robot

User-agent: Googlebot

Allow: /?\_escaped\_fragment\_

 

Allow: /?lang=

Allow: /hashtag/\*?src=

Allow: /search?q=%23

Disallow: /search/realtime

Disallow: /search/users

Disallow: /search/\*/grid

 

Disallow: /\*? 

Disallow: /\*/followers

Disallow: /\*/following

Обратите внимание: Twitter ограничивает доступ к

разделам своего сайта, имеющим API. Поскольку у Twitter есть

хорошо отрегулированный API \(на котором можно

зарабатывать деньги за счет лицензирования\), в интересах

компании запретить использование любых «самодельных API», собирающих информацию, самостоятельно сканируя сайт. 

На первый взгляд файл, сообщающий веб-краулеру, куда

нельзя заходить, может показаться ограничительным, однако

на самом деле это просто счастье разработчика. Обнаружение

файла robots.txt, который запрещает сбор данных

определенного раздела сайта, по сути, можно расценить как

сообщение веб-мастера о том, что он разрешает доступ веб-краулеров ко всем остальным разделам сайта \(в конце концов, если бы это было не так, то вам бы в первую очередь

ограничили доступ при написании robots.txt\). 

Например, раздел файла robots.txt из «Википедии», который относится к обычным веб-скраперам \(в отличие от

поисковых систем\), на удивление снисходителен. Он даже

доходит до того, что содержит доступный человеку текст

приветствия ботов \(это для нас\!\) и блокирует доступ только к

нескольким страницам, таким как страницы аутентификации, поиска и «случайной статьи»:

\#

\# Friendly, low-speed bots are welcome viewing article pages, but not

\# dynamically generated pages please. 

\#

\# Inktomi's "Slurp" can read a minimum delay between hits; if your bot supports

\# such a thing using the 'Crawl-delay' or another instruction, please let us

\# know. 

\#

\# There is a special exception for API

mobileview to allow dynamic mobile web

\# & app views to load section content. 

\# These views aren't HTTP-cached but use parser

cache aggressively and don't

\# expose special: pages etc. 

\#

User-agent: \*

Allow: /w/api.php?action=mobileview& 

Disallow: /w/

Disallow: /trap/

Disallow: /wiki/Especial:Search

Disallow: /wiki/Especial%3ASearch

Disallow: /wiki/Special:Collection

Disallow: /wiki/Spezial:Sammlung

Disallow: /wiki/Special:Random

Disallow: /wiki/Special%3ARandom

Disallow: /wiki/Special:Search

Disallow: /wiki/Special%3ASearch

Disallow: /wiki/Spesial:Search

Disallow: /wiki/Spesial%3ASearch

Disallow: /wiki/Spezial:Search

Disallow: /wiki/Spezial%3ASearch

Disallow: /wiki/Specjalna:Search

Disallow: /wiki/Specjalna%3ASearch Disallow: /wiki/Speciaal:Search

Disallow: /wiki/Speciaal%3ASearch

Disallow: /wiki/Speciaal:Random

Disallow: /wiki/Speciaal%3ARandom

Disallow: /wiki/Speciel:Search

Disallow: /wiki/Speciel%3ASearch

Disallow: /wiki/Speciale:Search

Disallow: /wiki/Speciale%3ASearch

Disallow: /wiki/Istimewa:Search

Disallow: /wiki/Istimewa%3ASearch

Disallow: /wiki/Toiminnot:Search

Disallow: /wiki/Toiminnot%3ASearch

При написании веб-краулеров лишь вам решать, станут ли

они подчиняться правилам robots.txt. Однако я

настоятельно советую это делать, особенно если ваши веб-краулеры будут проверять все сайты подряд. 

**Три веб-скрапера**

Поскольку вариации веб-скрапинга безграничны, есть

множество способов попасть в юридические неприятности. В

данном разделе мы рассмотрим три случая, которые так или

иначе стали нарушением закона, обычно применяемого к веб-скраперам, и узнаем, как именно закон был задействован в

каждом из этих случаев. 

**eBay против Bidder’s Edge и посягательство на движимое имущество**

В 1997 году игрушки Beanie Babies пользовались бешеным

успехом, технологический сектор бурлил, а онлайн-аукционы

были самой популярной новинкой Интернета. Компания под

названием Bidder’s Edge основала и разработала новый вид

сайта метааукционов. Вместо того чтобы заставлять

пользователя переходить с одного аукционного сайта на

другой, сравнивая цены, она объединила данные всех

аукционов для определенного продукта \(например, только что

выпущенной куклы Ферби или нового экземпляра Spice World\) и давала ссылку на сайт, который предлагал самую низкую

цену. 

Компания Bidder’s Edge добилась этого с помощью целой

армии веб-скраперов, которые постоянно делали запросы на

веб-серверы различных аукционных сайтов с целью получить

информацию о цене и продукте. Самым крупным из всех

интернет-аукционов был eBay, и Bidder’s Edge посещала его

серверы примерно 100 000 раз в день. Даже по сегодняшним

меркам это большой трафик. По данным eBay, в то время он

составлял 1,53 % от общего интернет-трафика, что, безусловно, не доставляло радости владельцам аукциона. 

eBay направил Bidder’s Edge письмо-предупреждение о

нарушении 

прав 

интеллектуальной 

собственности 

с

предложением 

лицензировать 

свои 

данные. 

Однако

переговоры по этому лицензированию успеха не имели, и

Bidder’s Edge продолжала собирать данные с сайта eBay. 

Тогда 

eBay 

попытался 

заблокировать 

IP-адреса, 

используемые Bidder’s Edge. Было заблокировано 169 IP-адресов, но Bidder’s Edge удалось обойти блокировки с

помощью прокси-серверов \(серверов, которые пересылают

запросы от имени другого компьютера, но применяя

собственный IP-адрес прокси-сервера\). Вообразите, какое

разочарование и неуверенность испытывали от этого решения

обе стороны: Bidder’s Edge постоянно искала новые прокси-серверы и покупала новые IP-адреса, по мере того как eBay

блокировал старые, а eBay приходилось поддерживать большие

списки 

блокировки 

для 

брандмауэров 

\(и 

нести

дополнительные большие затраты на сравнение IP-адресов при

проверке каждого пакета\). 

Наконец, в декабре 1999 года eBay подал на компанию

Bidder’s Edge в суд по поводу посягательства на движимое

имущество. 

Поскольку серверы eBay были реальными, физическими

ресурсами, которыми владела компания, и то, как компания

Bidder’s Edge их эксплуатировала, нельзя было назвать

нормальным, посягательство на движимое имущество казалось

идеальным законом для применения в суде. На самом деле в

наше время посягательство на движимое имущество постоянно

используется в судебных исках по веб-скрапингу и его принято

считать законом, касающимся IT. 

Суд постановил: чтобы выиграть дело о посягательстве на

движимое имущество, eBay должен был доказать две вещи:

• у Bidder’s Edge не было разрешения на использование

ресурсов eBay; 

• в результате действий Bidder’s Edge компания eBay понесла

финансовые потери. 

Поскольку 

имелись 

запись 

писем 

от 

eBay 

с

предупреждением о нарушении прав интеллектуальной

собственности и отчеты IT-отдела компании об использовании

серверов и фактических расходах на серверы, eBay было

относительно легко это сделать. Конечно, крупные судебные

баталии никогда не заканчиваются легко и быстро: были

поданы встречные иски, оплачены услуги многих адвокатов, и

в итоге в марте 2001-го дело решилось вне суда на сумму, размер которой не раскрывается. 

Значит ли это, что любое несанкционированное

использование чужого сервера автоматически является

посягательством на движимое имущество? Не обязательно. 

История с Bidder’s Edge была крайним случаем; компания

задействовала так много ресурсов eBay, что тому пришлось

покупать дополнительные серверы, увеличить расходы на

электроэнергию и, возможно, нанять дополнительный

персонал \(может показаться, что 1,53 % — не так уж много, но в

крупных компаниях это может составить значительную сумму\). 

В 2003 году Верховный суд Калифорнии вынес решение по

другому делу — Корпорация Intel против Хамиди, в котором

бывший сотрудник Intel \(Хамиди\) отправлял сотрудникам Intel нежелательные для компании электронные письма через все ее

серверы. Суд постановил следующее: «Заявление Intel отклонено не потому, что электронная корреспонденция, передаваемая через Интернет, обладает уникальным

иммунитетом, а потому, что посягательство на движимое

имущество — в отличие от только что упомянутых оснований

для предъявления претензий — не может быть доказано в

Калифорнии без доказательств причинения вреда личной

собственности истца или его законным интересам». 

По сути, Intel не удалось доказать, будто затраты на

передачу шести электронных писем, разосланных Хамиди всем

сотрудникам \(в каждом из которых, что характерно, предусматривалась возможность удаления из списка рассылки

Хамиди — по крайней мере он был вежлив\!\), причинили

компании какой-либо финансовый вред. Затраты не лишили

Intel никакой собственности и не помешали использованию

собственности компании. 

**Соединенные Штаты Америки против Ауэрнхаймера и Акт о**

**компьютерном мошенничестве и злоупотреблении**

Если информация легкодоступна в Интернете для человека, использующего 

браузер, 

то 

маловероятно, 

что 

за

автоматическим доступом к той же самой информации сразу

последуют неприятности с федералами. Однако стоит

любопытному человеку найти в системе безопасности

небольшую утечку, как она может быстро вырасти в огромную

дыру и стать гораздо более опасной, когда до нее доберутся

автоматические веб-скраперы. 

В 2010 году Эндрю Ауэрнхаймер \(Andrew Auernheimer\) и

Дэниел Спитлер \(Daniel Spitler\) обратили внимание на

интересную функцию iPad: при посещении сайта AT&T с этого

устройства вас перенаправляли на URL, содержащий

уникальный идентификационный номер вашего iPad:

https://dcp2.att.com/OEPClient/openPage?ICCID=

<idNumber>&IMEI=

Данная страница содержала форму аутентификации с

адресом 

электронной 

почты 

пользователя, 

чей

идентификационный номер содержался в URL. Это позволяло

пользователям получить доступ к их учетным записям, просто

введя пароль. 

Несмотря на то что потенциальных идентификационных

номеров iPad было очень много, при наличии достаточного

количества веб-скраперов оказалось возможным перебрать все

вероятные номера, притом собирая адреса электронной почты. 

Предоставляя 

пользователям 

эту 

удобную 

функцию

аутентификации, AT&T, по сути, опубликовала в Интернете

адреса электронной почты своих клиентов. 

Ауэрнхаймер и Спитлер создали веб-скрапер, который

собрал 114 000 этих адресов, в том числе частные адреса

электронной почты знаменитостей, генеральных директоров

корпораций и правительственных чиновников. Затем

Ауэрнхаймер \(но не Спитлер\) отправил данный список и

информацию о том, как он был получен, в интернет-таблоид

Gawker Media, опубликовавший историю \(но не список\) под

заголовком: «Наихудшее нарушение системы безопасности

Apple: обнародована информация о 114 000 владельцах iPad». 

В июне 2011 года года ФБР провело обыск в доме

Ауэрнхаймера в связи со сбором адресов электронной почты, хотя в итоге его арестовали по обвинению в торговле

наркотиками. В ноябре 2012 года Ауэрнхаймер был признан

винов ным в хищении персональных данных и заговоре с целью

получить доступ к компьютеру без разрешения, а затем был

приговорен к 41 месяцу лишения свободы в федеральной

тюрьме и выплате 73 000 долларов в качестве компенсации. 

Дело Ауэрнхаймера привлекло внимание адвоката по

гражданским правам Орина Керра \(Orin Kerr\), который

присоединился к его адвокатской группе и обжаловал дело

Ауэрнхаймера в апелляционном суде третьего округа. В итоге

11 апреля 2014 года \(эти юридические процессы обычно

занимают довольно много времени\) суд третьего округа

удовлетворил апелляцию, заявив следу ющее: «Осуждение

Ауэрнхаймера по пункту 1 должно быть отменено, поскольку в

соответствии с Актом о компьютерном мошенничестве и

злоупотреблении \(18 U.S.C. § 1030\(a\)\(2\)\(C\)\) посещение

общедоступного сайта не является несанкционированным

доступом. AT&T не использовала пароли или какие-либо иные

средства защиты для контроля доступа к адресам электронной

почты своих клиентов. То, что AT&T субъективно желала, чтобы посторонние не обнаружили эти данные, или что

Ауэрнхаймер, гиперболизируя, охарактеризовал свои действия

как “воровство”, не имеет значения. Компания настроила свои

серверы таким образом, что информация была доступна

каждому, и тем самым разрешила всем желающим

просматривать эту информацию. Доступ к адресам

электронной почты через открытый сайт AT&T был разрешен в

соответствии с CFAA и поэтому не является преступлением». 

Таким 

образом, 

в 

правосудии 

восторжествовало

здравомыслие, Ауэрнхаймер был в тот же день освобожден из

тюрьмы, и все жили долго и счастливо. 

Хоть в итоге суд и вынес решение, что Ауэрнхаймер не

нарушил 

Акт 

о 

компьютерном 

мошенничестве 

и

злоупотреблении, ФБР провело обыск в его доме, а сам он

потратил многие тысячи долларов на оплату юридических

услуг, провел три года в залах заседаний и тюрьмах. Какие

уроки можем извлечь из этого мы, разработчики веб-скраперов, чтобы избежать подобных ситуаций? 

Веб-скрапинг любой конфиденциальной информации, будь

то персональные данные \(в данном случае адреса электронной

почты\), коммерческие или правительственные секреты, —

вероятно, не то, с чем стоит связываться, не имея в под рукой

телефона адвоката. Даже если эта информация общедоступна, подумайте вот о чем: «Могли бы обычные пользователи

компьютера легко получить доступ к данной информации при

желании ее увидеть? Компания действительно хочет, чтобы

пользователи видели это?»

Мне неоднократно приходилось звонить в компании и

сообщать им об уязвимостях, найденных в системе

безопасности их сайтов и веб-приложений. Такое поведение

способно творить чудеса: «Привет, я специалист по

безопасности, и я обнаружила потенциальную уязвимость на

вашем сайте. Не могли бы вы направить меня к кому-нибудь, 

![Image 78](images/000064.png)

кому я могла бы сообщить об этом и решить проблему?» В вас

не только немедленно признают \(доброго\) хакерского гения —

вы также можете получить бесплатные подписки, денежные

вознаграждения и многие другие вкусности\! 

Ко всему прочему то, что Ауэрнхаймер передал

информацию Gawker Media \(прежде чем об этом узнала AT&T\) и устроил себе рекламу за счет найденной уязвимости, сделало

его еще более привлекательной целью для адвокатов AT&T. 

Обнаружив уязвимости в системе безопасности сайта, лучше сообщите о них владельцам данного сайта, а не СМИ. У

вас может возникнуть желание написать сообщение в блоге и

объявить об этом всему миру, особенно если проблема не будет

решена немедленно. Однако помните: это ответственность

компании, а не ваша. Лучшее, что вы можете сделать, — убрать

веб-скраперы \(и, насколько возможно, вообще уйти\) с сайта\! 

**Филд против Google: авторские права и robots.txt** Однажды адвокат Блейк Филд \(Blake Field\) подал иск против

Google на том основании, что функция кэширования сайта

Google нарушала закон об авторском праве, поскольку

показывала копию книги Филда после того, как он удалил ее со

своего сайта. Закон об авторском праве позволяет создателю

оригинального творческого произведения контролировать его

распространение. Аргумент Филда состоял в том, что

кэширование Google \(после того как Филд удалил книгу со

своего сайта\) лишило его возможности контролировать

распространение его книги. 

**Веб-кэш Google**

Сканируя сайты, веб-скраперы Google \(также известные как

боты Google\) делают копии этих сайтов и размещают в

Интернете. Доступ к данному кэшу может получить любой

желающий, используя URL следующего формата:

http://webcache.googleusercontent.com/searc

h?q=cache:http://pythonscraping.com/

Если сайт, который вы ищете или для которого хотите

выполнить веб-скрапинг, недоступен, то можете проверить, нет

ли у Google его полезной копии\! 

То, что Филд знал о функции кэширования Google и не

предпринял никаких действий, ему не помогло. В конце

концов, он мог бы запретить ботам Google кэшировать свой

сайт, просто создав файл robots.txt с простыми указаниями

о том, какие страницы веб-скраперам следует обрабатывать, а

какие — нет. 

И главное, суд постановил, что положение DMCA о зоне

безопасности позволило Google легально кэшировать и

отображать сайты, такие как сайт Филда: «\[a\] поставщик услуг

не несет ответственности за денежную компенсацию... за

нарушение авторских прав по причине промежуточного или

временного хранения материалов в системе или сети, контролируемой или управляемой поставщиком услуг либо для

него». 

29  *Walsh B. * The Surprisingly Large Energy Footprint of the Digital Economy \[UPDATE\], TIME.com, August 14, 2013 \(http://ti.me/2IFOF3F\). 

30 В 2019 г. апелляционный суд 9-го округа США принял решение, что скрапинг

публичных сайтов не противоречит закону CFAA \(https://bit.ly/39R5TcV\). Что касается

России, автоматизированный сбор информации с сайтов является законным, если в

процессе не нарушается законодательство РФ. — *Примеч. ред. *

**Движемся дальше**

Сеть продолжает развиваться. Технологии, которые открыли

нам доступ к изображениям, видео, текстам и другим файлам

данных, постоянно изменяются и обновляются. Чтобы идти в

ногу со временем, должен развиваться и набор технологий, используемых для веб-скрапинга данных. 

Кто знает, что будет дальше? Возможно, из последующих

редакций данной книги полностью исчезнет JavaScript как

устаревшая и редко используемая технология, а вместо этого

много внимания получит анализ голограмм HTML8. Но

сохранится главное: образ мышления и общий подход, необходимые для успешного веб-скрапинга любого сайта \(или

того, что мы станем понимать под «сайтами» в будущем\). 

Столкнувшись с любым веб-проектом, всегда следует задать

себе такие вопросы. 

• На какой вопрос я хочу получить ответ или какую задачу хочу

решить? 

• Какие данные помогут мне этого достичь и где они

находятся? 

• Каким образом сайт отображает эти данные? Могу ли я точно

определить, в какой части кода сайта содержится нужная

информация? 

• Как изолировать данные и извлечь их? 

• Какую обработку или анализ необходимо выполнить, чтобы

получить от этих данных еще больше пользы? 

• Как улучшить этот процесс, сделать его более быстрым и

надежным? 

Вам также необходимо понять не только то, как

использовать представленные в книге инструменты по

отдельности, но и то, как они могут работать совместно для

решения более масштабной проблемы. Иногда данные

легкодоступны и хорошо отформатированы, что позволяет

решить задачу с помощью простого веб-скрапера. В других

случаях приходится хорошенько подумать. 

Например, в главе 11 мы объединили библиотеку Selenium, позволяющую 

идентифицировать 

Ajax-изображения, 

загруженные в Amazon, и Tesseract, чтобы прочитать их с

помощью OCR. В задаче «Шесть шагов по “Википедии”» мы

использовали регулярные выражения с целью написать веб-краулер, который хранил информацию о ссылках в базе

данных, а затем — графовый алгоритм, чтобы ответить на

вопрос: «Каков кратчайший путь по ссылкам между

страницами Кевина Бейкона и Эрика Айдла?»

Когда речь идет об автоматическом сборе данных в

Интернете, неразрешимые задачи встречаются редко. Просто

запомните: Интернет — это один гигантский API с не очень

хорошим пользовательским интерфейсом. 

**Об авторе**

**Райан Митчелл** \(Ryan Mitchell\) — старший инженер-программист в бостонской компании HedgeServ, в которой она

разрабатывает API и инструменты для анализа данных. Райан

окончила Инженерно-технический колледж им. Франклина В. 

Олина, имеет степень магистра в области разработки

программного обеспечения и сертификат по анализу и

обработке данных, полученный на курсах повышения

квалификации при Гарвардском университете. До прихода в

HedgeServ Райан трудилась в компании Abine, где

разрабатывала веб-скраперы и средства автоматизации на

Python. Регулярно выступает консультантом проектов по веб-скрапингу для розничной торговли, сферы финансов и

фармацевтики. По совместительству работает консультантом и

внештатным 

преподавателем 

в 

Северо-Восточном

университете и Инженерно-техническом колледже им. 

Франклина В. Олина. 

**Об обложке**

Животное, изображенное на обложке, — степной ящер, или

саванный панголин *\(Smutsia temminckii\)*. Это одиночное

млекопитающее, ведущее ночной образ жизни. Панголины —

близкие родственники броненосцев, ленивцев и муравьедов. 

Они встречаются в Южной и Восточной Африке. На континенте

водятся еще три вида этих ящеров, и все они находятся под

угрозой исчезновения. 

Взрослые панголины достигают в среднем 30–100

сантиметров в длину и весят от 15 до 33 килограммов. Внешне

они похожи на броненосцев и покрыты защитными чешуйками

темного, светло-коричневого или оливкового цвета. У молодых

ящеров чешуйки розоватые. Когда панголину что-то угрожает, его чешуйки на хвосте превращаются в оружие нападения, они

могут порезать и поранить противника. У панголинов также

есть стратегия защиты, похожая на имеющуюся у скунсов, —

они выделяют неприятно пахнущую кислоту из желез, расположенных возле ануса. Это служит предупреждением для

потенциальных врагов, а также помогает панголину пометить

территорию. Брюхо панголина покрыто не чешуей, а коротким

мехом. 

Подобно 

своим 

родичам-муравьедам, 

панголины

придерживаются диеты, состоящей из муравьев и термитов. Их

невероятно длинные языки позволяют извлекать еду из бревен

и муравейников. Язык панголина длиннее его тела; в

расслабленном состоянии он втягивается в грудную полость. 

Поскольку панголины — одиночные животные, после

взросления они живут в норах, которые уходят глубоко под

землю. Эти норы прежде часто принадлежали трубкозубам и

бородавочникам, 

а 

панголины 

просто 

захватывают

заброшенные жилища. Однако с помощью трех длинных

изогнутых когтей на передних лапах ящер при необходимости

может легко выкопать собственную нору. 

Многие животные, изображенные на обложках книг O’Reilly, находятся под угрозой исчезновения; все они важны для

нашего мира. Чтобы больше узнать о том, как вы можете им

помочь, посетите сайт **animals.oreilly.com**. 

Изображение на обложке этой книги взято из книги *Royal* *Natural History* Ричарда Лидеккера \(Richard Lydekker\). 



# **Document Outline**

+ Введение  

+ Что такое веб-скрапинг 
+ Почему это называется веб-скрапингом 
+ Об этой книге 
+ Условные обозначения 
+ Использование примеров кода 
+ Благодарности 
+ От издательства 

+ Часть I. Разработка веб-скраперов 
+ Глава 1. Ваш первый веб-скрапер  

+ Установка соединения 
+ Знакомство с BeautifulSoup 

+ Глава 2. Углубленный синтаксический анализ HTML-кода  

+ Иногда молоток не требуется 
+ Еще одна тарелка BeautifulSoup 
+ Регулярные выражения 
+ Регулярные выражения и BeautifulSoup 
+ Доступ к атрибутам 
+ Лямбда-выражения 

+ Глава 3. Разработка веб-краулеров  

+ Проход отдельного домена 
+ Сбор информации со всего сайта 
+ Сбор информации с нескольких сайтов 

+ Глава 4. Модели веб-краулинга  

+ Планирование и определение объектов 
+ Работа с различными макетами сайтов 
+ Структурирование веб-краулеров 
+ Размышления о моделях веб-краулеров 

+ Глава 5. Scrapy  

+ Установка Scrapy 
+ Пишем простой веб-скрапер 
+ «Паук» с правилами 
+ Создание объектов Item 
+ Вывод объектов Item 
+ Динамический конвейер 
+ Ведение журнала Scrapy 
+ Дополнительные ресурсы 

+ Глава 6. Хранение данных  

+ Медиафайлы 
+ Хранение данных в формате CSV 
+ MySQL 
+ Электронная почта 

+ Часть II. Углубленный веб-скрапинг 
+ Глава 7. Чтение документов  

+ Кодировка документов 
+ Текст 
+ CSV 
+ PDF 
+ Microsoft Word и файлы .docx 

+ Глава 8. Очистка «грязных» данных  

+ Очистка данных в коде 
+ Очистка задним числом 

+ Глава 9. Чтение и запись текстов на естественных языках  

+ Обобщение данных 
+ Модели Маркова 
+ Natural Language Toolkit 
+ Дополнительные ресурсы 

+ Глава 10. Сбор данных из форм и проверка авторизации  

+ Библиотека Requests 
+ Отправка простейшей формы 
+ Переключатели, флажки и другие поля ввода 
+ Передача файлов и изображений 
+ Обработка данных авторизации и параметров cookie 
+ Другие проблемы с формами 

+ Глава 11. Веб-скрапинг данных JavaScript  

+ Краткое введение в JavaScript 
+ Ajax и динамический HTML 
+ Обработка перенаправлений 
+ Последнее замечание о JavaScript 

+ Глава 12. Веб-краулинг с помощью API  

+ Краткое введение в API 
+ Синтаксический анализ JSON 
+ Недокументированные API 
+ Объединение API с другими источниками данных 
+ Дополнительные сведения об API 

+ Глава 13. Обработка изображений и распознавание текста  

+ Обзор библиотек 
+ Обработка хорошо отформатированного текста 
+ Чтение капчи и обучение Tesseract 
+ Получение капчи и отправка решений 

+ Глава 14. Как избежать ловушек веб-скрапинга  

+ Этический момент 
+ Выдать скрипт за человека 
+ Основные средства защиты форм 
+ Контрольный список: как выдать программу за человека 

+ Глава 15. Тестирование сайтов с помощью веб-скраперов  

+ Основы тестирования 
+ Что такое юнит-тесты 
+ Python-модуль unittest 
+ Тестирование с помощью Selenium 
+ unittest или Selenium 

+ Глава 16. Параллельный веб-краулинг  

+ Процессы или потоки 
+ Многопоточный веб-краулинг 
+ Многопроцессный веб-краулинг 
+ Многопроцессный веб-краулинг: еще один подход 

+ Глава 17. Удаленный веб-скрапинг  

+ Зачем использовать удаленные серверы 
+ Tor 
+ Удаленный хостинг 
+ Дополнительные ресурсы 

+ Глава 18. Законность и этичность веб-скрапинга  

+ Торговые марки, авторские права, патенты… спасите-помогите\!  
+ Посягательство на движимое имущество 
+ Акт о компьютерном мошенничестве и злоупотреблении 
+ Файл robots.txt и условия использования 
+ Три веб-скрапера 

+ Движемся дальше 
+ Об авторе 
+ Об обложке



