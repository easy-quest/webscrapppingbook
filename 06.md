**Глава 7. Чтение документов**

Заманчиво представлять себе Интернет главным образом как

коллекцию 

текстовых 

сайтов, 

слегка 

разбавленных

новомодным мультимедийным контентом web 2.0, на который

при веб-скрапинге в большинстве случаев можно не обращать

внимания. Однако при этом мы игнорируем то, чем в своей

основе является Интернет: средство передачи файлов, независимо от их контента. 

Хотя сам Интернет в той или иной форме существует еще с

конца 1960-х, HTML появился лишь в 1992 году. До тех пор

Интернет состоял главным образом из электронной почты и

систем передачи файлов. Концепции веб-страниц в том виде, в

каком мы их сегодня знаем, тогда не было. Другими словами, Интернет — это не коллекция HTML-файлов, а множество

документов различных типов, причем HTML-файлы часто

используются в качестве рамки, в которую вставляются эти

документы. Не имея возможности читать документы разных

типов, в том числе текст, PDF, изображения, видео, электронную почту и т.д., мы упускаем огромную часть

доступных данных. 

В этой главе рассматривается работа с документами, независимо от того, скачиваем мы их в локальную папку или

читаем по сети и извлекаем данные. Вы также познакомитесь с

различными текстовыми кодировками, что позволит читать

HTML-страницы на иностранных языках. 

**Кодировка документов**

Кодировка документа указывает приложению — будь то

операционная система компьютера или написанный вами код

на Python, — как следует читать этот документ. Узнать

кодировку обычно можно из расширения файла, хотя оно не

обязательно соответствует кодировке. Я могу, например, сохранить файл myImage.jpg как myImage.txt, и проблем не

возникнет — по крайней мере до тех пор, пока я не попытаюсь

открыть его в текстовом редакторе. К счастью, подобные

ситуации встречаются редко; как правило, достаточно знать

расширение файла, в котором хранится документ, чтобы

прочитать его правильно. 

В своей основе любой документ содержит только нули и

единицы. Затем вступают в действие алгоритмы кодирования. 

Они определяют такие вещи, как «сколько битов приходится на

один символ» или «сколько битов занимает цвет пиксела» \(в

случае файлов с изображениями\). Далее может подключаться

уровень сжатия или некий алгоритм сокращения занимаемого

места в памяти, как в случае с PNG-файлами. 

Поначалу перспектива иметь дело с файлами, не

относящимися к формату HTML, может выглядеть пугающе, однако будьте уверены: при подключении соответствующей

библиотеки Python располагает всеми необходимыми

средствами для работы с любым форматом информации, который вам попадется. Единственное различие между

файлами с текстом, видео и изображениями состоит в том, как

интерпретируются их нули и единицы. В этой главе мы

рассмотрим следующие часто встречающиеся типы файлов: текст, CSV, PDF и документы Word. 

Обратите внимание: во всех этих файлах, по сути, хранится

текст. Если вы хотите узнать, как работать с изображениями, то

я рекомендую прочитать данную главу, чтобы привыкнуть

обрабатывать и хранить различные типы файлов, а затем

перейти к главе 13, в которой вы получите дополнительную

информацию об обработке изображений. 

**Текст**

Хранить файлы в виде обычного текста в Интернете кажется

несколько необычным, однако есть много простых сайтов и

сайтов старого образца с обширными хранилищами текстовых

файлов. Например, на сайте Инженерного совета Интернета

\(Internet Engineering Task Force, IETF\) все опубликованные

документы хранятся в формате HTML, PDF и текстовых файлов

\(см., 

например, 

**https://www.ietf.org/rfc/rfc1149.txt**\). 

В

большинстве браузеров эти текстовые файлы отлично

отображаются, так что веб-скрапинг для них должен

выполняться без проблем. 

Для большинства простейших текстовых документов, таких

как 

учебный 

файл, 

расположенный 

по 

адресу

**http://www.pythonscraping.com/pages/warandpeace/chapter1.txt**, можно использовать следующий метод:

from urllib.request import urlopen

textPage 

=

urlopen\('http://www.pythonscraping.com/'\\

'pages/warandpeace/chapter1.txt'\)

print\(textPage.read\(\)\)

Обычно, получая страницу с помощью urlopen, мы

превращаем ее в объект BeautifulSoup, чтобы выполнить

синтаксический анализ HTML-кода. В данном случае мы можем

прочитать страницу напрямую. Мы могли бы превратить ее в

объект BeautifulSoup, однако это было бы нерационально: здесь

нет HTML-разметки, которую стоило бы анализировать, поэтому библиотека будет бесполезной. Прочитав текстовый

файл как строку, мы можем лишь проанализировать его

аналогично любой другой строке, прочитанной в Python. 

Правда, здесь не получится использовать HTML-теги в качестве

контекстных подсказок, указывающих на то, какой текст нам

действительно нужен, а какой можно отбросить. Это может

стать проблемой, если из текстовых файлов требуется извлечь

лишь определенную информацию. 

**Текстовые кодировки и глобальный Интернет**

Помните, раньше я говорила, что расширение в имени файла —

это все, что нужно для правильного прочтения файла? Так вот, как ни странно, данное правило не распространяется на самый

простой из всех видов документов: файлы с расширением

.txt. 

В девяти случаях из десяти описанные выше методы чтения

текста будут работать. Однако иногда прочитать текст из

Интернета бывает непросто. Далее я расскажу об основных

кодировках для английского и других языков, от ASCII до

Unicode и ISO, и о том, как с ними справляться. 

**История текстовых кодировок**

Кодировка ASCII появилась еще в 1960-х, когда на счету был

каждый бит и отсутствовали причины кодировать что-либо, кроме латинского алфавита и нескольких знаков препинания. 

Поэтому использовались всего 7 бит, позволявшие

закодировать 128 символов, включая заглавные и строчные

буквы, а также знаки препинания. При всем воображении

разработчиков в любом случае оставались еще 33 непечатных

символа, из которых одни использовались, а другие со

временем, по мере развития технологий, были заменены и/или

устарели. Места было предостаточно, не правда ли? 

Как известно любому программисту, семерка — странное

число. Это не степень двойки, но заманчиво близко к ней. В

1960-х специалисты по информатике спорили о том, следует ли

![Image 45](images/000036.png)

![Image 46](images/000068.png)

![Image 47](images/000037.png)

![Image 48](images/000056.png)

![Image 49](images/000004.png)

вводить еще один бит, чтобы получить удобное круг лое число, или же оставить как есть, чтобы для хранения файлов

требовалось меньше места. В итоге победили 7 бит. Однако в

современных 

вычислениях 

каждая 

семибитная

последовательность дополняется ведущим нулем9,  поэтому мы

получаем оба недостатка: и файлы на 14 % больше, и доступны

всего 128 символов. 

В начале 1990-х человечество наконец обнаружило, что, помимо английского, существуют и другие языки и было бы

очень хорошо, если бы компьютеры могли отображать и их. 

Некоммерческая организация под названием «Консорциум

Unicode» попыталась создать универсальную кодировку текста, присвоив 

код 

каждому 

символу, 

который 

может

использоваться в любом текстовом документе на любом языке. 

Цель организации состояла в том, чтобы включить в кодировку

все, от латинского алфавита, на котором была написана эта

книга, до кириллицы \(щ, ъ, ы\), китайских иероглифов \(

\), 

математических и логических символов \( , ≥\) и даже

смайликов и прочих символов, таких как знак биологической

опасности \( \) и «пацифик» \( \). 

Получившаяся кодировка, как вы, возможно, уже поняли, получила 

название 

UTF-8, 

что, 

как 

ни 

странно, 

расшифровывается следующим образом: Universal Character Set

— Transformation Format 8 bit — «Универсальный набор

символов — восьмиразрядная форма представления». Слово

«восьмиразрядная» здесь относится не к размеру символа, а к

наименьшему размеру, который требуется для отображения

символа. 

Реальное количество битов для представления символа в

UTF-8 является переменной величиной. Оно может изменяться

от 1 до 4 байт, в зависимости от положения символа в списке

возможных символов \(чем чаще используется символ, тем

меньше байтов он занимает, более редкие символы занимают

больше байтов\). 

Как достигается эта гибкость кодировки? Использование 7

бит с возможно бесполезным ведущим нулем поначалу

выглядело как недостаток структуры ASCII, но для UTF-8

оказалось огромным преимуществом. Поскольку кодировка

ASCII была очень популярна, Консорциум Unicode решил

задействовать преимущество этого ведущего нулевого бита, объявив: если байт начинается с нуля, то это значит, что

символ занимает только этот один байт, благодаря чему схемы

кодирования для ASCII и UTF-8 оказались идентичными. Таким

образом, следующие символы допустимы как в UTF-8, так и в

ASCII:

01000001 - A

01000010 - B

01000011 - C

А эти символы допустимы только в UTF-8. При

интерпретации документа в кодировке ASCII они будут

отображаться как непечатные:

11000011 10000000 - À

11000011 10011111 - ß

11000011 10100111 - ç

Кроме UTF-8, есть и другие стандарты UTF, такие как UTF-16, UTF-24 и UTF-32, хотя документы, закодированные в этих

форматах, встречаются редко, за исключением необычных

обстоятельств, которые выходят за рамки данной книги. 

Несмотря на то что этот оригинальный «конструктивный

недостаток» ASCII оказался большим преимуществом для UTF8, полностью он не исчез. Первые 8 бит информации для

каждого символа по-прежнему позволяют кодировать только

128 символов, а не все 256. В символе UTF-8, требующем

нескольких байтов, дополнительные начальные биты не

расходуются 

на 

кодировку 

символов, 

а 

являются

проверочными и служат для предотвращения повреждения

кода. Из 32 \(8 × 4\) бит в четырехбайтовых символах только 21

бит используется для кодирования символов. В общей

сложности это составляет 2 097 152 возможных символа, из

которых в настоящее время применяются 1 114 112. 

Очевидно, проблема всех универсальных стандартов

кодирования языков заключается в том, что любой документ, написанный на каком-то одном иностранном языке, занимает

намного больше места, чем мог бы. Даже если ваш язык

насчитывает не более 100 символов, для представления

каждого символа все равно потребуется 16 бит, а не 8, как в

случае кодировки ASCII, привязанной к английскому языку. 

Из-за этого текстовые документы, написанные на иностранных

языках в кодировке UTF-8, занимают примерно в два раза

больше места, чем текстовые документы на английском, — по

крайней мере это касается тех языков, в которых не

используется латиница. 

ISO решает эту проблему путем создания отдельной

кодировки для каждого языка. Подобно Unicode, здесь

используются те же кодировки, что и для ASCII, но

дополнительный ведущий нулевой бит в начале каждого

символа позволяет создать 128 специальных символов для всех

языков, требующих этого. Данный вариант лучше всего

подходит для европейских языков, чей алфавит построен на

основе латиницы \(символы которой в этой кодировке

продолжают занимать позиции от 0 до 127\), но имеет

дополнительные специальные символы. Благодаря этому в

кодировке ISO-8859-1 \(разработанной для латинского

алфавита\) появились такие символы, как дроби \(например, 1/2\) или символ авторского права \(©\). 

В Интернете нередко встречаются и другие наборы

символов ISO, такие как ISO-8859-9 \(турецкий алфавит\), ISO-8859-2 \(в том числе немецкий\), а также ISO-8859-15 \(в том

числе французский\). 

Несмотря на то что в последние годы популярность

документов в кодировке ISO снижается, около 9 % сайтов в

Интернете по-прежнему используют в качестве кодировки

одну из разновидностей ISO10,  поэтому нам все равно

необходимо помнить о существовании разных кодировок и

проверять кодировку перед веб-скрапингом сайта. 

**Использование кодировок на практике**

В предыдущем разделе мы использовали стандартные

параметры настройки urlopen для чтения текстовых

документов, которые встречаются в Интернете. Эти параметры

прекрасно подходят для большинства текстов на английском

языке. Но если вам встретится документ на русском или

арабском или даже всего лишь отдельное слово наподобие

*re'sume' * — могут возникнуть проблемы. 

Рассмотрим, к примеру, такой код:

from urllib.request import urlopen

textPage 

=

urlopen\('http://www.pythonscraping.com/'\\

'pages/warandpeace/chapter1-ru.txt'\)

print\(textPage.read\(\)\)

Этот код читает первую главу романа Л. Толстого «Война и

мир» \(где встречается текст на русском и французском языках\)

![Image 50](images/000033.png)

и выводит ее на экран. В частности, на экран будет выведено

следующее:

b"\\xd0\\xa7\\xd0\\x90\\xd0\\xa1\\xd0\\xa2\\xd0\\xac

\\xd0\\x9f\\xd0\\x95\\xd0\\xa0\\xd0\\x92\\xd0\\

x90\\xd0\\xaf\\n\\nI\\n\\n\\xe2\\x80\\x94 Eh bien, mon prince. 

Открыв данную страницу в большинстве браузеров, мы

тоже увидим абракадабру \(рис. 7.1\). 

Это не поймут даже те, для кого русский язык является

родным. Проблема в том, что Python пытается прочитать

документ в кодировке ASCII, а браузер — в кодировке ISO-8859-1. И ни тот ни другой, конечно же, не предполагают, что

кодировка данного документа — UTF-8. 

 

**Рис. 7.1. ** Текст на русском и французском языках в кодировке в ISO-8859-1, которая во

многих браузерах используется по умолчанию для текстовых документов

Но можно явно задать кодировку строки как UTF-8, и тогда

кириллические символы будут выведены правильно:

from urllib.request import urlopen

 

textPage 

=

urlopen\('http://www.pythonscraping.com/'\\

'pages/warandpeace/chapter1-ru.txt'\) print\(str\(textPage.read\(\), 'utf-8'\)\)

Применение этой концепции в BeautifulSoup и Python 3.x выглядит так:

html 

=

urlopen\('http://en.wikipedia.org/wiki/Python\_\(p

rogramming\_language\)'\)

bs = BeautifulSoup\(html, 'html.parser'\)

content = bs.find\('div', \{'id':'mw-content-

text'\}\).get\_text\(\)

content = bytes\(content, 'UTF-8'\)

content = content.decode\('UTF-8'\)

В Python 3.x все символы по умолчанию кодируются в UTF8. У вас может возникнуть желание оставить все как есть и

использовать кодировку UTF-8 во всех веб-скраперах, которые

вам доведется писать. В конце концов, UTF-8 будет одинаково

хорошо поддерживать и символы в кодировке ASCII, и текст на

иностранных языках. Однако важно помнить о 9 % сайтов, использующих ту или иную версию кодировки ISO, из-за

которых вам не удастся полностью избежать этой проблемы. 

К сожалению, в случае с текстовыми документами

невозможно точно определить кодировку документа. Есть

библиотеки, которые позволяют исследовать документ и

сделать правильное предположение \(используя некую логику, способную сделать вывод, что ÑˆÐ°ÑÑÐoÐ°Ð·Ñ, вероятно, не

является словом\), но они часто ошибаются. 

К счастью, в случае с HTML-страницами кодировка обычно

обозначена в теге, расположенном в разделе <head> сайта. У

большинства сайтов, особенно англоязычных, есть такой тег:

<meta charset="utf-8" /> А, например, на сайте ECMA International \(**http://www.ecma-international.org/**\) есть такой тег11:

<META 

HTTP-EQUIV="Content-Type" 

CONTENT="text/html; charset=iso-8859-1"> Если вы планируете выполнять активный веб-скрапинг, особенно многоязычных сайтов, то, вероятно, имеет смысл

найти этот метатег и использовать рекомендованную в нем

кодировку для чтения контента страницы. 

**CSV**

В процессе веб-скрапинга вам, скорее всего, либо встретятся

CSV-файлы, либо придется работать с коллегой, которому

нравится представлять данные в этом формате. К счастью, у

Python 

есть 

отличная 

библиотека

\(**https://docs.python.org/3.4/library/csv.html**\) для чтения и записи

CSV-файлов. Она способна обрабатывать многие варианты

CSV, но в этом разделе мы уделим основное внимание

стандартному формату. Если вам попадется особый случай, с

которым придется разбираться, то обратитесь к документации\! 

**Чтение **

**CSV-файлов. ** 

CSV-библиотека 

Python

ориентирована в первую очередь на работу с локальными

файлами, поскольку исходит из предположения, что

необходимые CSV-данные уже хранятся на вашем компьютере. 

К сожалению, это не всегда так, особенно при веб-скрапинге. 

Есть несколько способов обойти данное условие. 

• Скачайте файл на компьютер вручную, и пусть Python работает с локальным файлом. 

• Напишите скрипт Python, который будет скачивать файл, читать его и \(возможно\) удалять после извлечения данных. 

• Получите файл из Интернета в виде строки и оберните строку

в объект StringIO, который ведет себя как файл. 

Первые два варианта вполне работоспособны, однако

занимать место на жестком диске файлами вместо того, чтобы

хранить их в памяти, — плохая идея. Гораздо лучше прочитать

файл как строку и обернуть его в объект, который Python будет

обрабатывать как файл, притом не сохраняя данные в виде

файла. Следующий скрипт получает CSV-файл из Интернета \(в

данном случае это список альбомов Monty Python, расположенный по адресу **http://pythonscraping.com/files/Monty-PythonAlbums.csv**\) и выводит его построчно в окно терминала: from urllib.request import urlopen

from io import StringIO

import csv

 

data 

=

urlopen\('http://pythonscraping.com/files/MontyP

ythonAlbums.csv'\)

.read\(\).decode\('ascii', 'ignore'\)

dataFile = StringIO\(data\)

csvReader = csv.reader\(dataFile\)

 

for row in csvReader:

print\(row\)

Результат выглядит так:

\['Name', 'Year'\]

\["Monty Python's Flying Circus", '1970'\]

\['Another Monty Python Record', '1971'\]

\["Monty Python's Previous Record", '1972'\]

... 

Как видно из данного примера кода, объект reader, возвращаемый csv.reader, является итерируемым и состоит

из объектов-списков Python. Поэтому доступ к строкам объекта

csvReader осуществляется следующим образом:

for row in csvReader:

print\('The album "'\+row\[0\]\+'" was released in '\+str\(row\[1\]\)\)

Результат выглядит так:

The album "Name" was released in Year

The album "Monty Python's Flying Circus" was released in 1970

The album "Another Monty Python Record" was released in 1971

The album "Monty Python's Previous Record" was released in 1972

... 

Обратите 

внимание 

на 

первую 

строку:

Thealbum"Name"wasreleasedinYear. 

При 

написании

данного примера ее вполне можно было бы пропустить, но вы

же не хотите, чтобы она попала в ваши данные в реальности\! 

Менее опытный программист мог бы просто пропустить

первую строку в объекте csvReader или написать

специальный код для ее обработки. К счастью, у функции

csv.reader есть альтернатива, которая позаботится обо всем

этом автоматически. Знакомьтесь — DictReader:

from urllib.request import urlopen from io import StringIO

import csv

 

data 

=

urlopen\('http://pythonscraping.com/files/MontyP

ythonAlbums.csv'\)

.read\(\).decode\('ascii', 'ignore'\)

dataFile = StringIO\(data\)

dictReader = csv.DictReader\(dataFile\)

 

print\(dictReader.fieldnames\)

for row in dictReader:

print\(row\)

Функция csv.DictReader возвращает значения всех строк

CSV-файла в виде объектов не списка, а словаря, с именами

полей, хранящимися в переменной dictReader.fieldnames и в качестве ключей каждого объекта словаря:

\['Name', 'Year'\]

\{'Name': 'Monty Python's Flying Circus', 

'Year': '1970'\}

\{'Name': 'Another Monty Python Record', 'Year':

'1971'\}

\{'Name': 'Monty Python's Previous Record', 

'Year': '1972'\}

Здесь, конечно, есть недостаток: создание, обработка и

вывод объектов DictRea der занимает немного больше

времени, чем объектов csvReader, однако удобство и простота

использования часто того стоят. Учтите также, что при веб-скрапинге затраты на запрос и извлечение данных сайта с

внешнего 

сервера 

почти 

всегда 

будут 

главным

ограничивающим фактором в любой создаваемой вами

программе, поэтому едва ли стоит беспокоиться о том, что

какая-то технология замедлит ваш компьютер еще на

несколько микросекунд\! 

**PDF**

Мне как пользователю Linux до боли обидно видеть

присланные файлы .docx, безбожно исковерканные в

текстовом редакторе, не принадлежащем Microsoft, и

мучительно подбирать кодеки для интерпретации очередного

нового медиаформата Apple. В определенном смысле Adobe совершила прорыв, когда в 1993 году разработала

переносимый формат документов \(Portable Document Format, PDF\). PDF-файлы обеспечили единый способ просмотра

изображений и текстовых документов для всех пользователей

независимо от платформы, на которой открывается документ. 

Несмотря на то что принцип хранения PDF-файлов в

Интернете несколько устарел \(зачем хранить контент в

статическом, медленно загружаемом формате, если его можно

представить в виде HTML-кода?\), формат PDF по-прежнему

встречается повсеместно, особенно когда речь идет об

официальных документах и анкетах. 

В 2009 году британец Ник Иннес \(Nick Innes\) прославился в

новостях, когда запросил у муниципального совета

Бэкингемшира открытую информацию о результатах

экзаменов, которая была доступна в соответствии с законом о

свободе информации, принятом в Соединенном Королевстве. 

После нескольких запросов — и отказов — он наконец получил

желаемую информацию в виде 184 PDF-документов. 

Конечно, Иннес настоял на своем и в итоге получил более

правильно отформатированную базу данных, однако, если бы

на его месте был специалист по веб-скрапингу, он, скорее

всего, сэкономил бы много времени, потраченного на судебные

разбирательства, вместо этого напрямую обработав PDF-документы с помощью одного из многочисленных модулей

Python для синтаксического анализа PDF. 

К сожалению, многие библиотеки синтаксического анализа

PDF, созданные для Python 2.x, не получили обновлений после

выхода Python 3.x. Однако, поскольку PDF является

относительно простым и открытым форматом документов, есть много хороших библиотек Python, в том числе для Python 3.x, которые позволяют читать подобные документы. 

К одной из таких относительно простых в использовании

библиотек относится PDFMiner3K. Это гибкий инструмент, который можно применять из командной строки или

интегрировать в код. Данная библиотека также поддерживает

различные языковые кодировки, что опять же в Интернете

часто бывает очень кстати. 

Библиотеку PDFMiner3K можно установить обычным

способом, через pip, или скачать этот модуль Python \(**https://pypi.python.org/pypi/pdfminer3k**\) и установить его, распаковав папку и выполнив следующую команду:

$ python setup.py install

Документация PDFMiner3K находится в извлеченной папке

по 

адресу 

/pdfminer3k-1.3.0/docs/index.html 

—

впрочем, существующая документация больше ориентирована

на интерфейс командной строки, чем на интеграцию с кодом

Python. 

Ниже показан простейший способ использования

библиотеки, позволяющий читать произвольные PDF-файлы

\(исходный объект — локальный файл\) и представлять их в виде

строки:

from urllib.request import urlopen

from 

pdfminer.pdfinterp 

import

PDFResourceManager, process\_pdf

from pdfminer.converter import TextConverter

from pdfminer.layout import LAParams

from io import StringIO

from io import open

 

def readPDF\(pdfFile\):

rsrcmgr = PDFResourceManager\(\)

retstr = StringIO\(\)

laparams = LAParams\(\)

device = TextConverter\(rsrcmgr, retstr, 

laparams=laparams\)

 

process\_pdf\(rsrcmgr, device, pdfFile\)

device.close\(\)

 

content = retstr.getvalue\(\)

retstr.close\(\)

return content

 

pdfFile = urlopen\('http://pythonscraping.com/' 

'pages/warandpeace/chapter1.pdf'\)

outputString = readPDF\(pdfFile\)

print\(outputString\)

pdfFile.close\(\)

В результате получим знакомый текст:

CHAPTER I

"Well, Prince, so Genoa and Lucca are now just family estates of

the Buonapartes. But I warn you, if you don't tell me that this

means war, if you still try to defend the infamies and horrors

perpetrated by that Antichrist- I really

believe he is Antichrist- I will

Преимущество этой программы для чтения PDF-документов

состоит в том, что при работе с локальными файлами мы

можем заменить обычный файловый объект Python объектом, возвращаемым urlopen, и использовать такую строку: pdfFile 

=

open\('../pages/warandpeace/chapter1.pdf', 'rb'\)

Результат может быть неидеальным, особенно для PDF-файлов, 

содержащих 

изображения, 

необычно

отформатированный текст, таблицы или диаграммы. Однако

для большинства PDF-файлов, содержащих только текст, результат должен быть таким же, как если бы PDF был

текстовым файлом. 

**Microsoft Word и файлы .docx**

Рискуя обидеть моих друзей из Microsoft, все же скажу: я не

люблю Microsoft Word. Не потому, что это плохая программа, а

потому, что пользователи ею часто злоупотребляют. У Microsoft Word есть особый талант превращать простые текстовые

документы или PDF-файлы в больших, медленных, трудно

открываемых монстров, которые при переносе с одного

компьютера на другой часто теряют все форматирование и по

непонятной 

причине 

оказываются 

доступными 

для

редактирования, хотя подразумевается, что их контент должен

быть статичным. 

Файлы Word предназначены для создания контента, а не

для обмена им. Тем не менее ряд сайтов постоянно используют

эти файлы для представления важных документов, информации и даже диаграмм и мультимедиа — в общем, всего

того, что можно и нужно представлять с помощью HTML-кода. 

Примерно до 2008 года в продуктах Microsoft Office использовался собственный формат файлов .doc. Этот

двоичный файловый формат было трудно читать, и он плохо

поддерживался другими текстовыми процессорами. Стремясь

идти в ногу со временем и создать стандарт, который бы

применялся во многих других программах, компания Microsoft решила задействовать стандарт Open Office на основе XML, благодаря которому файлы стали совместимыми с другим ПО, в

том числе построенным по принципу открытого исходного

кода. 

К сожалению, Python все еще не очень хорошо

поддерживает этот формат файлов, используемый в Google Docs, Open Office и Microsoft Office. Существует библиотека

python-docx 

\(**http://python-docx.readthedocs.org/en/latest/**\), однако она позволяет только создавать документы и читать

лишь основные данные файла, такие как размер и заголовок, а

не реальный контент. Чтобы прочитать содержимое файла

Microsoft Office, вам потребуется разработать собственное

решение. 

Первым шагом к этому является чтение XML из файла: from zipfile import ZipFile

from urllib.request import urlopen

from io import BytesIO

 

wordFile 

=

urlopen\('http://pythonscraping.com/pages/AWordD

ocument.docx'\).read\(\)

wordFile = BytesIO\(wordFile\)

document = ZipFile\(wordFile\)

xml\_content 

=

document.read\('word/document.xml'\)

print\(xml\_content.decode\('utf-8'\)\)

Этот код считывает удаленный документ Word в виде

двоичного файлового объекта \(BytesIO аналогичен StringIO, который использовался ранее в этой главе\), распаковывает его

с помощью библиотеки ядра Python zipfile \(по соображениям

экономии места все файлы .docx упаковываются в архив\), а

затем читает распакованный файл, имеющий формат XML. 

На рис. 7.2 показан документ Word, размещенный по адресу

**http://pythonscraping.com/pages/AWordDocument.docx**. 

![Image 51](images/000066.png)

 

**Рис. 7.2. ** Документ Word, полное содержимое которого вы, возможно, очень захотите

прочитать, но к нему трудно получить доступ, поскольку я разместила его на своем сайте в

виде файла .docx, вместо того чтобы опубликовать в формате HTML

Результат работы скрипта Python, прочитавшего мой

простой документ Word, выглядит так:

<\!--?xml 

version="1.0" 

encoding="UTF-8" 

standalone="yes"?--><w:document

mc:ignorable="w14 

w15 

wp14" 

xmlns:m="http://schemas.openxmlformats.org/offi ceDocument/2006/math" 

xmlns:mc="http://schemas.openxmlformats.org/mar kup-compatibility/2006" 

xmlns:o="urn:schemas-

microsoft-com:office:office" 

xmlns:r="http://schemas.openxmlformats.org/offi ceDocument/2006/relationships" 

xmlns:v="urn:schemas-microsoft-

com:vml"xmlns:w="http://schemas.openxmlformats. 

org/wordprocessingml/2006/main" 

xmlns:w10="urn:schemas-microsoft-

com:office:word" 

xmlns:w14="http://schemas.microsoft.com/office/

word/2010/wordml" 

xmlns:w15="http://schemas.microsoft.com/office/

word/2012/wordml" 

xmlns:wne="http://schemas.microsoft.com/office/

word/2006/wordml" 

xmlns:wp="http://schemas.openxmlformats.org/dra wingml/2006/wordprocessingDrawing" 

xmlns:wp14="http://schemas.microsoft.com/office

/word/2010/wordprocessingDrawing" 

xmlns:wpc="http://schemas.microsoft.com/office/

word/2010/wordprocessingCanvas" 

xmlns:wpg="http://schemas.microsoft.com/

office/word/2010/wordprocessingGroup" 

xmlns:wpi="http://schemas.microsoft.com/

office/word/2010/wordprocessingInk" 

xmlns:wps="http://schemas.microsoft.com/office/

word/2010/wordprocessingShape"><w:body><w:p w:rsidp="00764658" 

w:r 

sidr="00764658" 

w:rsidrdefault="00764658"><w:ppr><w:pstyle w:val="Title"> 

</w:pstyle></w:ppr><w:r><w:t>A Word Document on a Website</w:t> 

</w:r><w:bookmarkstart 

w:id="0" 

w:name="\_GoBack"></w:bookmarkstart> 

<w:bookmarkend w:id="0"></w:bookmarkend></w:p> 

<w:p 

w:rsidp="00764658" 

w:rsidr="00764658" 

w:rsidrdefault="00764658"></w:p><w:p w:rsidp="00764658" 

w:rsidr="00764658" 

w:rsidrdefault="00764658" w:rsidrpr="00764658"> 

<w: r> <w:t>This is a Word document, full of

content that you want very much. Unfortunately, it's difficult to access because I'm putting it

on my website as a .</w:t></w:r><w:prooferr w:type="spellStart"></w:prooferr><w:r> 

<w:t>docx</w:t></w:r><w:prooferr w:type="spellEnd"></w:prooferr> 

<w:r> 

<w:t

xml:space="preserve"> file, 

rather than just publishing it as HTML</w:t> 

</w:r> </w:p> <w:sectpr w:rsidr=

"00764658"w:rsidrpr="00764658"> 

<w:pgszw:h="15840" w:w="12240"></w:pgsz> 

<w:pgmar 

w:bottom="1440" 

w:footer="720" 

w:gutter="0" 

w:header="720" 

w:left="1440" 

w:right="1440" w:top="1440"></w:pgmar> <w:cols w:space="720"></w:cols&g; 

<w:docgrid

w:linepitch="360"></w:docgrid> 

</w:sectpr> 

</w:body> </w:document> 

Здесь явно слишком много метаданных, в которых утонул

реально полезный для нас текст. К счастью, весь текст этого

документа, включая расположенный сверху заголовок, заключен в теги w:t, благодаря чему его легко извлечь: from zipfile import ZipFile

from urllib.request import urlopen

from io import BytesIO

from bs4 import BeautifulSoup

 

wordFile 

=

urlopen\('http://pythonscraping.com/pages/AWordD

ocument.docx'\).read\(\)

wordFile = BytesIO\(wordFile\)

document = ZipFile\(wordFile\)

xml\_content 

=

document.read\('word/document.xml'\)

 

wordObj 

=

BeautifulSoup\(xml\_content.decode\('utf-8'\), 

'xml'\)

textStrings = wordObj.find\_all\('w:t'\)

 

for textElem in textStrings:

print\(textElem.text\)

Обратите внимание: вместо синтаксического анализатора

html.parser, который обычно передается в объект

BeautifulSoup, мы используем синтаксический анализатор

xml. Дело в том, что стандарт HTML не предусматривает

двоеточия в именах тегов, и html.parser не распознает теги

наподобие w:t. 

Результат неидеален, но приемлем, а благодаря тому, что

каждый тег w:t выводится в отдельной строке, мы можем

легко отследить разделение текста в Word:

A Word Document on a Website

This is a Word document, full of content that you want very much. Unfortunately, 

it's difficult to access because I'm putting it

on my website as a . 

docx

file, rather than just publishing it as HTML

Обратите внимание: слово docx оказалось в отдельной

строке. В исходном XML оно заключено в тег

<w:proofErrw:type="spellStart"/>. Этим способом Word

снабжает слово docx красным волнистым подчеркиванием, указывая на то, что в названии его собственного формата

файла есть орфографическая ошибка. 

Заголовку документа предшествует тег дескриптора стиля

<w:pstylew:val="Title">. Несмотря на то что этот тег не

позволяет однозначно идентифицировать заголовки \(или

другой стиль текста\), использование средств навигации

BeautifulSoup может оказаться полезным:

textStrings = wordObj.find\_all\('w:t'\)

 

for textElem in textStrings:

 

 

 

 

style 

=

textElem.parent.parent.find\('w:pStyle'\)

if style is not None and style\['w:val'\] ==

'Title':

 

 

 

 

 

 

 

 

print\('Title 

is:

\{\}'.format\(textElem.text\)\)

else:

print\(textElem.text\)

Эту функцию можно легко дополнить, чтобы текст разных

стилей заключался в теги или выделялся другим способом. 

9 Этот бит «отступа» еще вернется, чтобы преследовать нас в стандартах ISO. 

10 Согласно W3Techs

\(http://w3techs.com/technologies/history\_overview/character\_encodin\), где для сбора

статистики подобного рода используются веб-краулеры. 

11 Организация ECMA одной из первых присоединилась к стандарту ISO, поэтому

неудивительно, что ее сайт написан в кодировке ISO. 

**Глава 8. Очистка «грязных» данных**

До сих пор в книге мы игнорировали проблему плохо

отформатированных данных, используя источники, в которых

данные отформатированы в целом хорошо, и полностью

отбрасывая данные, если они отличались от ожидаемых. Но

часто при просмотре веб-страниц не приходится быть слишком

разборчивыми в том, откуда берутся данные или как выглядят. 

Ошибки в пунктуации, неправильно поставленные

прописные буквы, разрывы строк и опечатки приводят к тому, что «грязные» данные в Интернете могут стать большой

проблемой. В текущей главе мы рассмотрим несколько

инструментов и методов, которые помогут пресечь эту

проблему в корне, изменив способ написания кода и очищая

данные после их попадания в базу. 

**Очистка данных в коде**

Подобно тому как мы пишем код для обработки явных

исключений, следует взять за правило защитное кодирование

для обработки непредвиденных ситуаций. 

В 

лингвистике 

есть 

понятие 

*«n-грамма»* 

—

последовательность из *n* слов, используемых в тексте или речи. 

При анализе естественного языка часто бывает удобно разбить

текст на общеупотребительные *n*-граммы или повторяющиеся

наборы слов, которые часто применяются вместе. 

В этом разделе я лишь покажу, как получать правильно

отформатированные 

*n*-граммы, 

не 

затрагивая 

их

использование для какого-либо анализа. Позже, в главе 9, мы

рассмотрим применение 2- и 3-грамм для обобщения и

анализа текста. 

Следующий код возвращает список 2-грамм, найденных в

статье «Википедии» о языке программирования Python: from urllib.request import urlopen

from bs4 import BeautifulSoup

 

def getNgrams\(content, n\):

content = content.split\(' '\)

output = \[\]

for i in range\(len\(content\)-n\+1\):

output.append\(content\[i:i\+n\]\)

return output

 

html 

=

urlopen\('http://en.wikipedia.org/wiki/Python\_\(p

rogramming\_language\)'\)

bs = BeautifulSoup\(html, 'html.parser'\)

content = bs.find\('div', \{'id':'mw-content-

text'\}\).get\_text\(\)

ngrams = getNgrams\(content, 2\)

print\(ngrams\)

print\('2-grams count is: '\+str\(len\(ngrams\)\)\)

Функция getNgrams принимает исходную строку, разбивает ее на последовательность слов \(при условии, что все

слова разделены пробелами\) и заносит в массив все *n*-граммы

\(в данном случае 2-граммы\), которые начинаются с каждого

слова строки. 

В результате из этого текста можно получить несколько

действительно любопытных и полезных 2-грамм:

\['of', 'free'\], \['free', 'and'\], \['and', 'open-

source'\], \['open-source', 'software'\]

Но, кроме этого, код также возвращает много мусора:

\['software\\nOutline\\nSPDX\\n\\n\\n\\n\\n\\n\\n\\n\\nOper

ating', 

'system\\nfamilies\\n\\n\\n\\nAROS\\nBSD\\nDarwin\\neCo

s\\nFreeDOS\\nGNU\\nHaiku\\nInferno\\nLinux\\nMach\\nM

INIX\\nOpenSolaris\\nPlan'\], 

\['system\\nfamilies\\n\\n\\n\\nAROS\\nBSD\\nDarwin\\neC

os\\nFreeDOS\\nGNU\\nHaiku\\nInferno\\nLinux\\nMach\\n

MINIX\\nOpenSolaris\\nPlan', 

'9\\nReactOS\\nTUD:OS\\n\\n\\n\\n\\n\\n\\n\\n\\nDevelopmen

t\\n\\n\\n\\nBasic'\], 

\['9\\nReactOS\\nTUD:OS\\n\\n\\n\\n\\n\\n\\n\\n\\nDevelopme

nt\\n\\n\\n\\nBasic', 'For'\]

Вдобавок, поскольку 2-граммы создаются для каждого слова

в тексте \(кроме последнего\), на момент написания этой книги

из статьи получилось 7411 2-грамм. Не особенно хорошо

управляемый набор данных\! 

Используя 

регулярные 

выражения 

для 

удаления

управляющих символов \(таких как \\n\) и фильтрацию для

удаления любых символов Unicode, можно немного очистить

этот результат:

import re

 

def getNgrams\(content, n\):

content = re.sub\('\\n|\[\[\\d\+\\\]\]', ' ', 

content\)

content = bytes\(content, 'UTF-8'\)

content = content.decode\('ascii', 'ignore'\)

content = content.split\(' '\)

content = \[word for word in content if word

\!= ''\]

output = \[\]

for i in range\(len\(content\)-n\+1\):

output.append\(content\[i:i\+n\]\)

return output

В этом коде все символы новой строки заменяются

пробелами, удаляются цитаты наподобие \[123\] и

отфильтровываются все пустые строки, из-за которых в тексте

появляется несколько пробелов подряд. Затем удаляются

экраниру ющие символы путем представления контента в

кодировке UTF-8. 

Эти шаги значительно улучшают результат функции, но

некоторые проблемы все равно остаются:

\['years', 'ago\('\], \['ago\(', '-'\], \['-', '-'\], 

\['-', '\)'\], \['\)', 'Stable'\]

Ситуацию можно улучшить, удалив все знаки препинания, стоящие до и после каждого слова \(вообще убрав всю

пунктуацию\). При этом остаются такие элементы, как дефисы, но удаляются не только пустые строки, но и строки, состоящие

из единственного знака препинания. 

Конечно, пунктуация тоже важна и, просто удалив ее, можно потерять некую ценную информацию. Например, можно предположить, что точка, после которой стоит пробел, означает конец предложения или утверждения. Мы можем

запретить *n*-граммы, которые пересекают подобные точки, и

рассматривать только *n*-граммы в пределах одного

предложения. 

Например, для следующего текста:

Python features a dynamic type system and

automatic memory management. 

It supports multiple programming paradigms... 

2-грамма \['memory','management'\] корректна, а 2-грамма \['management','It'\] — нет. 

Теперь, после того как у нас появился более длинный список

«задач очистки», мы ввели концепцию «предложений» и вся

наша программа немного усложнилась, лучше разделить эти

задачи на четыре самостоятельные функции:

from urllib.request import urlopen

from bs4 import BeautifulSoup

import re

import string

 

def cleanSentence\(sentence\):

sentence = sentence.split\(' '\)

 

 

 

 

sentence 

=

\[word.strip\(string.punctuation\+string.whitespac

e\)

for word in sentence\]

sentence = \[word for word in sentence if

len\(word\) > 1

or \(word.lower\(\) == 'a' or word.lower\(\)

== 'i'\)\]

return sentence

 

def cleanInput\(content\):

content = re.sub\('\\n|\[\[\\d\+\\\]\]', ' ', 

content\)

content = bytes\(content, 'UTF-8'\)

content = content.decode\('ascii', 'ignore'\)

sentences = content.split\('. '\)

return \[cleanSentence\(sentence\) for sentence in sentences\]

 

def getNgramsFromSentence\(content, n\):

output = \[\]

for i in range\(len\(content\)-n\+1\):

output.append\(content\[i:i\+n\]\)

return output

 

def getNgrams\(content, n\):

content = cleanInput\(content\)

ngrams = \[\]

for sentence in content:

ngrams.extend\(getNgramsFromSentence\(sen

tence, n\)\)

return\(ngrams\)

Функция getNgrams остается основной точкой входа в

программу; cleanInput, как и прежде, удаляет переводы

строк и цитаты, а также разбивает текст на «предложения» в

зависимости от местоположения точек, за которыми следует

пробел. 

Функция 

cleanInput 

также 

вызывает

cleanSentence, которая разбивает предложение на слова, удаляет знаки препинания и пробелы, а также односимвольные

слова наподобие I и a. 

Главные строки кода, собственно, и создающие *n*-граммы, перемещаются 

в 

функцию 

getNgramsFromSentence, 

вызываемую из getNgrams для каждого предложения. Это

гарантирует, что не будут создаваться *n*-граммы, части которых

принадлежат разным предложениям. 

Обратите внимание на использование функций Python string.punctuation и string.whitespace, позволяющих

получить список всех знаков препинания. Результат

выполнения функции string.punctuation можно увидеть в

терминале Python, введя следующий код:

>>> import string

>>> print\(string.punctuation\)

\!"\#$%&'\(\)\*\+,-./:;<=>?@\[\\\]^\_\`\{|\}~

Результат 

выполнения 

функции

print\(string.whitespace\) гораздо менее интересен \(в

конце концов, это же пробелы\), но в нем присутствуют

пробельные символы, включая неразрывные пробелы, табуляции и переводы строк. 

Функция

item.strip\(string.punctuation\+string.whitespace\), используемая в цикле, перебирающем все слова исходного

текста, удаляет все знаки препинания по обе стороны от слова, однако слова с дефисами остаются нетронутыми \(поскольку в

них по обе стороны знака препинания стоят буквы\). 

Результат наших усилий приводит к созданию намного

более чистых 2-грамм:

\[\['Python', 'Paradigm'\], \['Paradigm', 'Object-oriented'\], \['Object-oriented', 'imperative'\], 

\['imperative', 'functional'\], \['functional', 

'procedural'\], \['procedural', 'reflective'\],... 

**Нормализация данных. ** Все мы не раз сталкивались с

плохо разработанными веб-формами наподобие такой:

«Введите номер телефона в формате “ххх-ххх-хххх”». 

Вы, как хороший программист, могли бы спросить: «Почему

бы им самим просто не удалять нецифровые символы, которые

я 

ввожу?» *Нормализация данных* 

— 

это 

процесс, 

гарантирующий, 

что 

лингвистически 

или 

логически

эквивалентные строки, такие как телефонные номера \(555\) 123-4567 и 555.123.4567, будут отображаться эквивалентно или

по крайней мере считаться эквивалентными при сравнении. 

Взяв за основу код создания *n*-грамм из предыдущего

раздела, мы можем добавить к нему функции нормализации

данных. 

Первая очевидная проблема данного кода состоит в том, что

он выдает много повторяющихся 2-грамм. Все 2-граммы, генерируемые этим кодом, заносятся в список без учета

частоты появления. Интересно было бы фиксировать их

частоту, вместо того чтобы просто констатировать их

существование, — это может оказаться полезным для

составления графика последствий изменений, которые

вносятся в алгоритмы очистки и нормализации данных. Если

данные хорошо нормализованы, то общее количество

уникальных *n*-грамм уменьшится, в то время как общее

количество всех найденных *n*-грамм \(то есть уникальных и

неуникальных элементов, идентифицированных как *n*-

граммы\) не изменится. Другими словами, будет меньше

«групп» при том же количестве *n*-грамм. 

Для этого мы можем изменить код таким образом, чтобы *n*-

граммы собирались не в список, а в объект Counter: from collections import Counter

 

def getNgrams\(content, n\):

content = cleanInput\(content\)

ngrams = Counter\(\)

for sentence in content:

newNgrams = \[' '.join\(ngram\) for ngram

in

getNgramsFromSentence\(sentence, 2\)\]

ngrams.update\(newNgrams\)

return\(ngrams\)

Есть много других способов сделать это — например, вносить *n*-граммы в словарный объект, в котором значение

элемента списка показывает, сколько раз данная *n*-грамма

встречается в тексте. Недостаток такого подхода заключается в

том, что он требует немного больше управления и усложняет

сортировку. Однако у использования объекта Counter тоже

есть недостаток: данный объект не позволяет хранить списки

\(они не хешируются\), поэтому необходимо сначала

преобразовать 

их 

в 

строки, 

используя 

операцию

''.join\(ngram\) при генерации списка для каждой *n*-граммы. 

Получим следующие результаты:

Counter\(\{'Python 

Software': 

37, 

'Software

Foundation': 37, 'of the': 34, 'of Python': 28, 

'in Python': 24, 'in the': 23, 'van Rossum': 20, 'to the': 20, 'such as': 19, 'Retrieved February': 19, 'is a': 16, 'from the': 16, 

'Python Enhancement': 15,... 

На момент написания этой книги в статье о Python насчитывалось 7275 2-грамм, из которых 5628 были

уникальными, причем наиболее популярной 2-граммой была

Software Foundation, второй по популярности — Python Software. Однако анализ результатов показывает, что Python Software еще два раза встречается в виде Python software. 

Аналогично van Rossum и Van Rossum являются разными

элементами списка. 

Если добавить в функцию cleanInput строку:

content = content.upper\(\)

то общее количество найденных 2-грамм останется равным

7275, но количество уникальных сократится до 5479. 

Кроме того, обычно бывает полезным на минуту

остановиться и прикинуть, сколько вычислительной мощности

мы готовы потратить на нормализацию данных. В некоторых

случаях разные варианты написания слов эквивалентны, но

для установления этой эквивалентности необходимо

проверить каждое слово и определить, соответствует ли оно

какой-либо 

из 

предварительно 

запрограммированных

эквивалентностей. 

Например, в списке 2-грамм присутствуют варианты Python 1st и Python first. Однако создание общего правила, гласящего, что «Все слова first, second, third и т.д. преобразуются в 1st, 2nd, 3rd и т.д. \(или наоборот\)», потребует еще приблизительно

десяти проверок для каждого слова. 

Аналогичным образом, непоследовательное использование

дефисов \(например, наличие в одном тексте слов «иррегулярный» и «иррегулярный»\), орфографические ошибки и

другие несовпадения, встречающиеся в естественных языках, повлияют на группировки *n*-грамм и в случае достаточно

распространенных несовпадений могут привести к искажению

результатов программы. 

Одним из решений в случае разрыва слов при переносах

может быть полное удаление дефисов и обработка каждого

слова как отдельной строки, что потребует только одной

операции. Однако это значит следующее: словосочетания, содержащие дефисы \(что встречается слишком часто\) тоже

будут рассматриваться как одно слово. Возможно, лучше пойти

по другому пути и рассматривать дефисы как пробелы. Просто

будьте готовы к внезапному нашествию «ир регулярных» и

«регулярных» слов\! 


