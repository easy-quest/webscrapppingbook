**Будьте осторожны\! **

Помните, что бывает с теми, кто скачивает неизвестные файлы

из Интернета? Представленный выше скрипт скачивает на

жесткий диск вашего компьютера все подряд, включая

случайные скрипты bash, .exe-файлы и другие потенциально

вредоносные программы. 

Считаете, что с вами не случится ничего плохого, поскольку вы

никогда не запускаете ничего из того, что попало в папку

Загрузки? Тогда вы просто напрашиваетесь на неприятности, особенно запуская программу с правами администратора. Что

произойдет, если на каком-нибудь сайте вам попадется файл, который отправляет сам себя в папку ../../../../usr/bin/python? В

следующий раз, запустив скрипт Python из командной строки, вы можете случайно установить на свой компьютер

вредоносную программу\! 

Рассмотренная выше программа написана исключительно для

примера; ее нельзя устанавливать в рабочей среде без более

тщательной проверки имен файлов и ее следует запускать

только из учетной записи с ограниченными правами. И самое

главное: помните, что нам всегда помогут резервное

копирование 

файлов, 

хранение 

конфиденциальной

информации вне жесткого диска и здравый смысл. 

В этом скрипте с помощью лямбда-функции \(см. главу 2\) мы

выбираем на главной странице сайта все теги, имеющие

атрибут src, а затем очищаем и нормализуем URL, чтобы

получить абсолютный путь для каждого скачиваемого файла \(и

гарантированно отсеять все внешние ссылки\). Затем каждый

файл скачивается по своей ссылке и помещается на компьютер

в локальную папку downloaded. 

Обратите внимание: для быстрой ссылки на каталог, в

который помещаются результаты каждого скачивания, а также

для создания при необходимости недостающих каталогов

используется Python-модуль os. Он играет роль интерфейса

между Python и операционной системой, позволяя ей

управлять путями к файлам, создавать каталоги, получать

информацию о работающих процессах и переменных среды и

т.п. 

**Хранение данных в формате CSV**

*CSV* \(comma-separated values — «значения, разделенные

запятыми»\) — один из самых популярных форматов файлов

для хранения табличных данных. Благодаря своей простоте

этот формат поддерживается Microsoft Excel и многими

другими приложениями. Ниже показан пример совершенно

правильного содержимого CSV-файла:

fruit,cost

apple,1.00

banana,0.30

pear,1.25

Как и в Python, здесь важны разделители: строки

отделяются друг от друга символом новой строки, а столбцы

внутри строки — запятыми \(отсюда и название «разделенные

запятыми»\). В других вариантах CSV-файлов \(иногда называ-емых *character-separated values* — «файлы значений, разделенных символами»\) для разделения строк используются

табуляции и другие символы, но эти форматы менее

распространены и поддерживаются не так широко. 

Если вы хотите скачивать CSV-файлы прямо из Интернета и

хранить их локально, не анализируя и не изменяя, то данный

раздел вам не нужен. Скачайте эти файлы, как любые другие, и

сохраните их в формате CSV, используя методы, описанные в

предыдущем разделе. 

Python позволяет легко изменять и даже создавать CSV-файлы с нуля с помощью библиотеки csv:

import csv

 

csvFile = open\('test.csv', 'w\+'\)

try:

writer = csv.writer\(csvFile\)

writer.writerow\(\('number', 'number plus 2', 

'number times 2'\)\)

for i in range\(10\):

writer.writerow\( \(i, i\+2, i\*2\)\)

finally:

csvFile.close\(\)

На всякий случай предупреждаю: создание файла в Python

— практически безошибочная процедура. При отсутствии

файла test.csv Python автоматически создаст его \(но не

каталог\). Если же такой файл уже есть, то Python перезапишет

test.csv, внеся в него новые данные. 

После выполнения этой программы должен получиться

следующий CSV-файл:

number,number plus 2,number times 2

0,2,0

1,3,2

2,4,4

... 

Одна из самых популярных задач веб-скрапинга — скачать

HTML-таблицу и сохранить ее в виде CSV-файла. Сравнение

текстовых 

редакторов 

в 

«Википедии»

\(**https://en.wikipedia.org/wiki/Comparison\_of\_text\_editors**\) представляет собой весьма сложную HTML-таблицу с

выделением ячеек разными цветами, а также со ссылками, сортировкой и другим HTML-мусором, который необходимо

будет вычистить, прежде чем записывать данные в формат

CSV. Активно используя BeautifulSoup и функцию get\_text\(\), мы можем сделать это менее чем за 20 строк кода: import csv

from urllib.request import urlopen

from bs4 import BeautifulSoup

 

html = urlopen\('http://en.wikipedia.org/wiki/' 

'Comparison\_of\_text\_editors'\)

bs = BeautifulSoup\(html, 'html.parser'\)

\# 

Основная 

сравнительная 

таблица 

сейчас

является первой на странице. 

table 

= 

bs.find\_all\('table', 

\{'class':'wikitable'\}\)\[0\]

rows = table.find\_all\('tr'\)

 

csvFile = open\('editors.csv', 'wt\+'\)

 

![Image 42](images/000011.png)

writer = csv.writer\(csvFile\)

try:

for row in rows:

csvRow = \[\]

for cell in row.find\_all\(\['td', 

'th'\]\):

csvRow.append\(cell.get\_text\(\)\)

writer.writerow\(csvRow\)

finally:

csvFile.close\(\)

 

**Более простой способ получить отдельную таблицу**

Данный скрипт отлично интегрируется в веб-скрапер, если

нужно преобразовать много HTML-таблиц в CSV-файлы или

объединить таблицы в один CSV-файл. Но если нужно сделать

это только один раз, то имеется средство получше: копирование

и вставка. Выделив и скопировав весь контент HTML-таблицы и

вставив его в файл Excel или Google Docs, мы как раз получим

CSV-файл, не запуская скрипт\! 

В 

результате 

должен 

получиться 

правильно

отформатированный CSV-файл, сохраненный на локальном

компьютере под именем editors.csv. 

**MySQL**

*MySQL* \(официально произносится как «май эс-кью-эль», хотя

многие говорят «май сиквел»\) — на сегодняшний день самая

популярная система управления реляционными базами

данных с открытым исходным кодом. Это несколько

необычная ситуация: проект с открытым исходным кодом

успешно конкурирует с большими программными продуктами. 

Однако популярность MySQL исторически была связана с

двумя другими крупнейшими системами управления базами

данных с закрытым исходным кодом: Microsoft SQL Server и

СУБД Oracle. 

MySQL популярна не без причин. Ее можно смело выбирать

для большинства приложений. Это масштабируемая, надежная

и полнофункциональная СУБД, используемая ведущими

сайтами, в том числе YouTube5,  Twitter6 и Facebook7. 

Благодаря 

повсеместному 

распространению, 

цене

\(«бесплатно» — очень хорошая цена\) и удобству применения

«как есть», без дополнительной настройки, MySQL — просто

фантастическая база данных для веб-скраперов, и в этой книге

мы будем использовать именно ее. 

**Что такое реляционная база данных**

Реляционные данные — это данные, между которыми

установлены некие отношения \(от англ. relation —

«отношение», «зависимость», «связь»\). Я рада, что мы

прояснили это\! 

Ладно, шучу. Говоря о реляционных данных, специалисты по

информатике имеют в виду данные, которые не существуют

сами по себе — у них есть свойства, связывающие их с

другими данными. Например, «пользователь A идет учиться в

учебное заведение Б», причем пользователя A можно найти в

таблице «Пользователи» базы данных, а учебное заведение Б

— в таблице «Учебные заведения» этой же базы. 

В данной главе вы познакомитесь с моделированием

различных типов отношений и эффективным хранением

данных в MySQL \(или любой другой реляционной базе

данных\). 

**Установка MySQL**

При отсутствии знаний о MySQL установка базы данных может

показаться несколько страшноватой \(если вы хорошо знакомы

с MySQL, то можете спокойно пропустить данный раздел\). На

самом деле это не сложнее, чем установка практически любого

другого программного обеспечения. По сути, MySQL работает с

набором файлов, в которых содержатся данные. Эти файлы

размещены на сервере или локальном компьютере и вмещают

всю информацию, хранящуюся в базе данных. Кроме того, программный уровень MySQL обеспечивает удобный способ

взаимодействия с данными через интерфейс командной

строки. Например, следующая команда просматривает файлы

данных и возвращает список всех пользователей базы данных

по имени Ryan:

SELECT \* FROM users WHERE firstname = "Ryan" 

Если вы используете дистрибутив Linux на основе Debian \(или другую операционную систему с командой apt-get\), то

можете легко установить MySQL с помощью следующей

команды:

$ sudo apt-get install mysql-server

Просто 

присматривайте 

за 

процессом 

установки, 

согласитесь со всеми требованиями к памяти и введите пароль

для нового пользователя **root** при появлении соответствующей

подсказки. 

В macOS и Windows все немного сложнее. Перед

скачиванием пакета необходимо создать учетную запись

Oracle, если это не было сделано ранее. 

Если вы используете macOS, то сначала нужно скачать

дистрибутив \(**http://dev.mysql.com/downloads/mysql/**\). 

Чтобы загрузить файл, выберите пакет .dmg и

зарегистрируйтесь в системе под существующей учетной

записью Oracle или создайте ее. Затем запустится довольно

простой мастер установки \(рис. 6.1\). 

![Image 43](images/000034.png)

 

**Рис. 6.1. ** Мастер установки MySQL в macOS

Вам должно быть достаточно варианта установки, предлагаемого по умолчанию. В этой книге предполагается, что вы установили MySQL именно так. 

Если скачивание и запуск мастера установки кажется вам

несколько утомительным занятием и вы используете Mac, то

всегда можно обратиться к менеджеру пакетов Homebrew \(**http://brew.sh/**\). После установки Homebrew можно установить

MySQL следующим образом:

$ brew install mysql

Homebrew — отличный проект с открытым исходным кодом

и с хорошей интеграцией пакетов Python. Большинство

сторонних модулей Python, используемых в данной книге, легко устанавливаются с помощью Homebrew. Если он у вас еще

не установлен, то настоятельно рекомендую сделать это\! 

![Image 44](images/000006.png)

После установки MySQL на macOS можно запустить сервер

MySQL следу ющим образом:

$ cd /usr/local/mysql

$ sudo ./bin/mysqld\_safe

В Windows установка и запуск MySQL немного сложнее, однако, на наше счастье, удобный мастер установки

\(**http://dev.mysql.com/downloads/windows/installer/**\) упрощает

этот процесс. После загрузки он проведет вас по операциям, которые необходимо выполнить \(рис. 6.2\). 

 

**Рис. 6.2. ** Мастер установки MySQL в Windows Здесь тоже можно установить MySQL, используя параметры

по умолчанию, за единственным исключением: на странице

**Setup Type** \(Тип установки\) я рекомендую выбрать вариант

**Server Only** \(Только сервер\), чтобы не устанавливать множество

дополнительных программ и библиотек Microsoft. После этого

можно использовать параметры установки по умолчанию и

следовать инструкциям для запуска сервера MySQL. 

**Несколько основных команд**

После запуска сервера MySQL появляется множество вариантов

взаимодействия с базой данных. Есть много программных

инструментов, играющих роль посредников, вследствие чего

вам не обязательно иметь дело именно с командами MySQL

\(или по крайней мере не придется делать это часто\). Такие

инструменты, как phpMyAdmin и MySQL Workbench, позволяют

без труда просматривать, сортировать и вставлять данные. Но

все равно важно уметь использовать командную строку. 

За исключением имен переменных, MySQL нечувствительна

к регистру; например, SELECT — то же самое, что и sElEcT. 

Однако по соглашению все ключевые слова в MySQL принято

писать заглавными буквами. И наоборот, большинство

разработчиков предпочитают при именовании таблиц и баз

данных использовать строчные буквы, хотя этот стандарт часто

игнорируется. 

Когда вы впервые войдете в MySQL, там не будет базы

данных, в которую можно было бы вставить данные. Но ее

можно создать:

> CREATE DATABASE scraping; 

Поскольку в каждом экземпляре MySQL может содержаться

несколько баз данных, прежде чем начать взаимодействовать с

БД, необходимо указать MySQL, какую именно базу вы хотите

использовать:

> USE scraping; 

С этого момента \(и пока вы не закроете соединение с MySQL

или не переключитесь на другую базу\) все вводимые команды

будут выполняться для созданной нами базы данных

scraping. 

На первый взгляд все довольно просто. Наверное, так же

легко можно создать и таблицу в базе данных? Попробуем

создать таблицу для хранения коллекции веб-страниц, собранных веб-скрапером:

> CREATE TABLE pages; 

И получим ошибку:

ERROR 1113 \(42000\): A table must have at least

1 column

В отличие от базы данных, которая может существовать без

таблиц, таблица MySQL не может существовать без столбцов. 

Чтобы определить столбцы в MySQL, необходимо перечислить

их в виде списка, разделенного запятыми, в скобках после

оператора CREATETABLE<имя\_таблицы>:

> CREATE TABLE pages \(id BIGINT\(7\) NOT NULL

AUTO\_INCREMENT, 

title VARCHAR\(200\), content VARCHAR\(10000\), 

created TIMESTAMP DEFAULT CURRENT\_TIMESTAMP, 

PRIMARY KEY\(id\)\); 

Каждое определение столбца состоит из трех частей:

• имя \(id, title, created и т.д.\); 

• тип переменной \(BIGINT\(7\), VARCHAR, TIMESTAMP\); 

• также можно указать различные дополнительные атрибуты

\(NOTNULLAUTO\_INCREMENT\). 

В конце списка столбцов нужно указать *ключ* таблицы. В

MySQL ключи обеспечивают удобное представление контента

таблицы для быстрого поиска. Далее в главе я покажу, как

использовать эти ключи для ускорения работы базы данных. 

Пока просто запомните, что наилучшим вариантом ключа

таблицы, как правило, является столбец идентификаторов \(id\). 

После выполнения этого запроса можно воспользоваться

командой DESCRIBE и посмотреть, как выглядит структура

таблицы:

> DESCRIBE pages; 

\+---------\+----------------\+------\+-----\+------

-------------\+----------------\+

| Field | Type | Null | Key |

Default | Extra |

\+---------\+----------------\+------\+-----\+------

-------------\+----------------\+

| id | bigint\(7\) | NO | PRI |

NULL | auto\_increment |

| title | varchar\(200\) | YES | |

NULL | |

| content | varchar\(10000\) | YES | |

NULL | |

| created | timestamp | NO | |

CURRENT\_TIMESTAMP | |

\+---------\+----------------\+------\+-----\+------

-------------\+----------------\+

4 rows in set \(0.01 sec\)

Конечно, эта таблица все еще пуста. Чтобы вставить в

таблицу pages тестовые данные, можно воспользоваться

следующей командой:

> INSERT INTO pages \(title, content\) VALUES

\("Test page title", 

"This is some test page content. It can be up to 10,000 characters

long."\); 

Обратите внимание: хоть эта таблица и имеет четыре

столбца \(id, title, content, created\), для вставки строки

нужно определить только два из них \(title и content\). Дело в

том, что столбец id получается автоматически, путем

приращения \(при вставке новой строки MySQL автоматически

прибавляет 1 к идентификатору предыдущей строки\), так что

нам об этом заботиться практически не приходится. В столбец

timestamp по умолчанию записывается текущее время. 

Безусловно, мы *можем* задать эти значения явно:

> INSERT INTO pages \(id, title, content, created\) VALUES \(3, 

"Test page title", 

"This is some test page content. It can be up to 10,000 characters

long.", "2014-09-21 10:25:32"\); 

При отсутствии в базе данных столбца, у которого id равен

заданному целому числу, подобное переопределение вполне

будет работать. Однако, как правило, поступать так — плохая

идея; лучше доверить MySQL автоматическое заполнение

столбцов id и timestamp, если только у вас нет веских причин

поступить иначе. 

Теперь, когда в нашей таблице появились данные, мы

можем использовать различные методы их выбора. Вот

несколько примеров операторов SELECT:

> SELECT \* FROM pages WHERE id = 2; 

Этот оператор дает MySQL такую команду: «Выбрать все

страницы, у которых id равен 2». Звездочка \(\*\) —

подстановочный символ, он требует возвратить все строки

таблицы, для которых условие \(WHEREid=2\) является

истинным. Данный оператор возвращает вторую строку

таблицы либо пустой результат, если в таблице нет строки, у

которой id равен 2. Например, следующий запрос без учета

регистра возвращает все строки, в которых поле title содержит фрагмент *test* \(символ % в строках MySQL играет роль

подстановочного\):

> SELECT \* FROM pages WHERE title LIKE

"%test%"; 

А как быть, если в нашей таблице много столбцов и мы

хотим, чтобы оператор возвращал только определенный набор

данных? Тогда вместо выбора всех столбцов можно написать, например, так:

> SELECT id, title FROM pages WHERE content LIKE "%page content%"; 

Этот оператор возвращает только id и title для всех

строк, в которых столбец content содержит фразу *page content*. 

Синтаксис операторов DELETE почти такой же, как у

SELECT:

> DELETE FROM pages WHERE id = 1; 

Поэтому, особенно при работе с важными базами данных, которые нелегко восстановить, рекомендуется перед

выполнением DELETE сначала выполнить аналогичный

оператор 

SELECT 

\(в 

данном 

случае

SELECT\*FROMpagesWHEREid=1\), с целью убедиться, что

выбираются только те строки, которые вы хотите удалить, а

затем заменить SELECT\* на DELETE. Многие программисты

рассказывают ужасные истории о том, как они второпях

ошиблись в условии или, что еще хуже, совершенно

неправильно написали оператор DELETE и полностью

разрушили клиентские данные. Не позволяйте этому случиться

с вами\! 

Аналогичные меры предосторожности следует принимать и

с операторами UPDATE:

> UPDATE pages SET title="A new title", content="Some new content" WHERE id=2; В данной книге мы будем иметь дело только с основными

операторами MySQL — простейшим выбором, вставкой и

изменением. Если вы хотите узнать больше о командах и

приемах этого мощного инструмента для работы с базами

данных, то рекомендую прочитать книгу *MySQL Cookbook* Поля

Дюбуа 

\(Paul 

DuBois\)8 

\(издательство 

O’Reilly\)

\(**http://shop.oreilly.com/product/0636920032274.do**\). 

**Интеграция с Python**

К сожалению, в MySQL нет встроенной поддержки Python. 

Однако как для Python 2.x, так и для Python 3.x есть множество

библиотек с открытым исходным кодом, позволяющих

взаимодействовать с базами данных MySQL. Одной из самых

популярных 

таких 

библиотек 

является 

PyMySQL

\(**https://pypi.python.org/pypi/PyMySQL**\). 

Когда писалась эта книга, последней версией PyMySQL была

0.6.7, которая устанавливается с помощью pip следующим

образом:

$ pip install PyMySQL

Вы также можете скачать и установить данную библиотеку

из ее источника — это бывает удобно, если хотите использовать

определенную версию библиотеки:

$ 

curl 

-L

https://pypi.python.org/packages/source/P/PyMyS

QL/

PyMySQL-0.6.7.tar.gz\\ | tar xz

$ cd PyMySQL-PyMySQL-f953785/

$ python setup.py install

После установки библиотеки пакет PyMySQL должен стать

доступным автоматически. При работающем локальном

сервере MySQL должен успешно выполниться следующий

скрипт \(не забудьте указать пароль пользователя **root** для

вашей базы данных\):

import pymysql

conn 

= 

pymysql.connect\(host='127.0.0.1', 

unix\_socket='/tmp/mysql.sock', 

user='root', 

passwd=None, db='mysql'\)

cur = conn.cursor\(\)

cur.execute\('USE scraping'\)

cur.execute\('SELECT \* FROM pages WHERE id=1'\)

print\(cur.fetchone\(\)\)

cur.close\(\)

conn.close\(\)

В этом примере появляются два новых типа объектов: объект-соединение \(conn\) и объект-курсор \(cur\). 

Модели соединения и курсора широко применяются при

программировании баз данных, хотя поначалу некоторые

пользователи с трудом их различают. Понятно, что объект-соединение отвечает за само соединение с базой, а также за

отправку информации из нее, обработку откатов \(когда

необходимо прервать выполнение одного или нескольких

запросов и вернуть базу данных в исходное состояние\) и

создание новых объектов-курсоров. 

У соединения может быть несколько курсоров. Курсор

отслеживает определенную информацию *о состоянии*: например, то, какая база данных используется. Если у нас есть

несколько баз и нужно записать информацию во все, то для

обработки такого запроса можно создать ряд курсоров. Кроме

того, курсор содержит результаты последнего выполненного

запроса. Чтобы получить доступ к этой информации, можно

воспользоваться функциями для курсора, такими как

cur.fetchone\(\). 

Важно закрывать и курсор, и соединение после окончания

их использования. Игнорирование этого требования может

привести к *утечкам соединений* — накоплению незакрытых

неиспользуемых соединений, из-за которых программа не

может закрыться, поскольку ей кажется, что эти соединения

все еще задействованы. Именно подобные ошибки приводят к

постоянному отключению баз данных \(мне довелось допускать

и исправлять много ошибок утечки соединений\), поэтому не

забывайте закрывать соединения\! 

Прежде всего вы, вероятно, захотите реализовать

сохранение результатов веб-скрапинга в базе данных. 

Посмотрим, как это можно сделать, используя предыдущий

пример — веб-скрапер «Википедии». 

Работа с текстом в формате Unicode при просмотре веб-страниц может оказаться непростой задачей. По умолчанию

MySQL не поддерживает Unicode. К счастью, мы можем

активировать эту функцию \(только помните, что тогда

увеличится размер базы данных\). Поскольку в «Википедии»

нам встретится множество колоритных символов, самое время

сообщить базе, что ей придется иметь дело с текстом в

формате Unicode:

ALTER DATABASE scraping CHARACTER SET = utf8mb4

COLLATE = utf8mb4\_unicode\_ci; 

ALTER TABLE pages CONVERT TO CHARACTER SET

utf8mb4 COLLATE utf8mb4\_unicode\_ci; 

ALTER 

TABLE 

pages 

CHANGE 

title 

title

VARCHAR\(200\) CHARACTER SET utf8mb4

COLLATE utf8mb4\_unicode\_ci; 

ALTER TABLE pages CHANGE content content

VARCHAR\(10000\) CHARACTER SET utf8mb4

COLLATE utf8mb4\_unicode\_ci; 

Эти четыре строки изменяют кодировку, используемую в

базе данных по умолчанию, для таблицы и обоих ее столбцов, с

utf8mb4 \(технически это тоже Unicode, но с печально

известной ужасной поддержкой большинства символов

Unicode\) на utf8mb4\_unicode\_ci. 

Вы поймете, что у вас все получилось, если попробуете

вставить в поле title или content несколько умляутов или

китайских иероглифов и это удастся сделать без ошибок. 

Теперь, когда база данных готова принять все то, на что

способна «Википедия», можно выполнить следующий код: from urllib.request import urlopen

from bs4 import BeautifulSoup

import datetime

import random

import pymysql

import re

 

conn 

= 

pymysql.connect\(host='127.0.0.1', 

unix\_socket='/tmp/mysql.sock', 

user='root', 

passwd=None, db='mysql', charset='utf8'\)

cur = conn.cursor\(\)

cur.execute\('USE scraping'\)

 

random.seed\(datetime.datetime.now\(\)\)

 

def store\(title, content\):

cur.execute\('INSERT INTO pages \(title, 

content\) VALUES ' 

'\("%s", "%s"\)', \(title, content\)\) cur.connection.commit\(\)

 

def getLinks\(articleUrl\):

 

 

 

 

html 

=

urlopen\('http://en.wikipedia.org'\+articleUrl\)

bs = BeautifulSoup\(html, 'html.parser'\)

title = bs.find\('h1'\).get\_text\(\)

content = bs.find\('div', \{'id':'mw-content-

text'\}\).find\('p'\)

.get\_text\(\)

store\(title, content\)

 

 

 

 

return 

bs.find\('div', 

\{'id':'bodyContent'\}\).find\_all\('a', 

href=re.compile\('^\(/wiki/\)\(\(?\!:\).\)\*$'\)\)

 

links = getLinks\('/wiki/Kevin\_Bacon'\)

try:

while len\(links\) > 0:

newArticle = links\[random.randint\(0, 

len\(links\)-1\)\].attrs\['href'\]

print\(newArticle\)

links = getLinks\(newArticle\)

finally:

cur.close\(\)

conn.close\(\)

Здесь следует отметить несколько моментов. Во-первых, в

строку соединения с базой данных добавляется условие

charset='utf8'. Так мы сообщаем соединению, что всю

информацию в базу следует отправлять в формате UTF-8 \(само

собой разумеется, что БД уже должна быть настроена

соответствующим образом\). 

Во-вторых, обратите внимание на то, что появилась

функция store. Она принимает две строковые переменные: title и content — и вставляет их в оператор INSERT, который 

выполняется 

объектом-курсором 

и 

затем

принимается объектом-соединением этого курсора. Перед

нами отличный пример разделения курсора и соединения: курсор хранит информацию о базе данных и ее контексте, однако использует соединение, чтобы через него отправлять

информацию и вставлять данные в базу. 

Наконец, мы видим, что в конце основного цикла

программы добавился оператор finally. Он гарантирует, что

независимо от того, завершится ли программа нормально, будет ли прервана или во время ее выполнения возникнут

исключения \(а с учетом вечного беспорядка в Интернете будьте

уверены, что исключения станут возникать постоянно\), курсор

и соединение всегда будут закрываться непосредственно перед

завершением программы. Всегда использовать оператор

try...finally при веб-скрапинге и с открытым соединением

с базой данных — хорошая идея. 

PyMySQL — не слишком большой пакет, но у него так много

полезных функций, что все они не поместятся в этой книге. 

Подробнее о них см. в документации на сайте PyMySQL

\(**https://pymysql.readthedocs.io/en/latest/**\). 

**Приемы и рекомендуемые методики работы с базами данных**

Есть люди, посвятившие всю свою карьеру изучению, настройке и изобретению новых баз данных. Я к ним не

отношусь, и моя книга не об этом. Однако в данной области, как и во многих других разделах информатики, есть приемы, которые можно быстро освоить, чтобы ваши БД по крайней

мере достаточно хорошо и быстро работали для большинства

приложений. 

Во-первых, за редким исключением, всегда добавляйте в

свои таблицы столбцы идентификаторов. У любой таблицы

MySQL должен быть хотя бы один первичный ключ \(ключевой

столбец, по которому сортируется таблица\), подсказывающий

MySQL, как ее упорядочить. Часто бывает сложно выбрать эти

ключи рационально. 

Споры о том, что лучше задействовать в качестве ключа: искусственно созданный столбец идентификатора или

уникальный атрибут, такой как имя пользователя, — не

утихали в среде исследователей данных и разработчиков

программного обеспечения в течение многих лет. Однако я

склоняюсь к варианту с созданием столбцов идентификаторов. 

*Особенно* это касается веб-скрапинга и хранения чьих-то чужих

данных. Вы не можете знать заранее, какой элемент на самом

деле окажется уникальным или неуникальным. Несколько раз

действительность меня немало удивляла. 

Столбец идентификатора должен присутствовать во всех

ваших таблицах, иметь автоматическое приращение и

использоваться в качестве первичного ключа. 

Во-вторых, используйте интеллектуальную индексацию. 

Словарь \(тот, который книга, а не объект Python\) представляет

собой список слов, проиндексированных в алфавитном

порядке. Это позволяет быстро найти нужное слово, если

известно, как оно написано. Вдобавок можно представить

словарь, статьи которого расположены в алфавитном порядке

по определению слова. Он был бы не так полезен — разве что

для викторины наподобие «Своей игры», где нужно угадывать

слово по его определению. Но при поиске в базе данных

подобные ситуации тоже случаются. Например, в базе может

присутствовать поле, к которому вы часто обращаетесь:

>SELECT \* FROM dictionary WHERE definition="A small furry animal that says meow"; 

\+-----\+------\+---------------------------------

----\+

| 

id 

 

| 

word 

|

definition |

\+-----\+------\+---------------------------------

----\+

| 200 | cat | A small furry animal that says meow |

\+-----\+------\+---------------------------------

----\+

1 row in set \(0.00 sec\)

Возможно, в дополнение к предположительно уже

имеющемуся индексу по id вы захотите добавить в эту таблицу

индекс по столбцу definition, чтобы ускорить поиск по

данному столбцу. Однако имейте в виду: при добавлении еще

одной индексации потребуется место для нового индекса, а

также увеличится время вставки новых строк. Особенно это

касается ситуации, когда мы имеем дело с большими объемами

данных. Поэтому следует тщательно рассмотреть все

компромиссы использования индексов и то, что и сколько

нужно индексировать. Чтобы индекс определений стал

немного легче, можно индексировать только несколько первых

символов из значения столбца definition. Следующая

команда создает индекс для первых 16 символов из поля

definition:

CREATE INDEX definition ON dictionary \(id, 

definition\(16\)\); 

Благодаря данному индексу поиск слов по полному

определению будет выполняться намного быстрее \(особенно

если первые 16 символов определений в большинстве записей

существенно отличаются\) и при этом не потребует слишком

много дополнительного места в памяти и не очень замедлит

обработку. 

Что касается компромисса между временем выполнения

запроса и размером базы данных \(одна из фундаментальных

операций балансировки при разработке БД\), то одна из

наиболее распространенных ошибок, особенно при веб-скрапинге больших объемов данных, написанных на

естественных языках, заключается в хранении большого

количества повторяющихся данных. Предположим, вы хотите

измерить частоту появления определенных фраз, которые

присутствуют на разных сайтах. Эти фразы могут извлекаться

из заданного списка или автоматически генерироваться с

помощью алгоритма анализа текста. У вас может возникнуть

соблазн сохранить данные примерно так:

\+--------\+--------------\+------\+-----\+---------

\+----------------\+

| Field | Type | Null | Key | Default

| Extra |

\+--------\+--------------\+------\+-----\+---------

\+----------------\+

| id | int\(11\) | NO | PRI |

NULL | auto\_increment |

| url | varchar\(200\) | YES | |

NULL | |

| phrase | varchar\(200\) | YES | |

NULL | |

\+--------\+--------------\+------\+-----\+---------

\+----------------\+

Каждый раз при обнаружении фразы на сайте в эту таблицу

базы данных будет добавляться строка, а в нее — записываться

URL страницы, где была найдена эта фраза. Однако разделив

данные на три таблицы, можно значительно сократить набор

данных:

>DESCRIBE phrases

\+--------\+--------------\+------\+-----\+---------

\+----------------\+

| Field | Type | Null | Key | Default

| Extra |

\+--------\+--------------\+------\+-----\+---------

\+----------------\+

| id | int\(11\) | NO | PRI |

NULL | auto\_increment |

| phrase | varchar\(200\) | YES | |

NULL | |

\+--------\+--------------\+------\+-----\+---------

\+----------------\+

>DESCRIBE urls

\+-------\+--------------\+------\+-----\+---------

\+-----------------\+

| Field | Type | Null | Key | Default |

Extra |

\+-------\+--------------\+------\+-----\+---------

\+-----------------\+

| id | int\(11\) | NO | PRI | NULL |

auto\_increment |

| url | varchar\(200\) | YES | |

NULL | |

\+-------\+--------------\+------\+-----\+---------

\+-----------------\+

>DESCRIBE foundInstances

\+-------------\+---------\+------\+-----\+---------

\+----------------\+

| Field | Type | Null | Key | Default

| Extra |

\+-------------\+---------\+------\+-----\+---------

\+----------------\+

| id | int\(11\) | NO | PRI |

NULL | auto\_increment |

| urlId | int\(11\) | YES | |

NULL | |

| phraseId | int\(11\) | YES | |

NULL | |

| occurrences | int\(11\) | YES | |

NULL | |

\+-------------\+---------\+------\+-----\+---------

\+----------------\+

Хоть определения таблиц и занимают больше места, мы

видим, что большинство столбцов — это просто поля для

целочисленных id, которые занимают гораздо меньше места в

базе. Кроме того, каждый полный URL и каждая текстовая

фраза сохраняются ровно один раз. 

Если не установить сторонний пакет или не вести

подробный журнал, то будет невозможно определить, в какой

момент был добавлен, обновлен или удален каждый элемент

данных. В зависимости от наличия свободного места для

хранения данных, частоты их изменения и важности

определения того, когда оно произошло, можно рассмотреть

вопрос о сохранении временных меток создания, изменения и

удаления 

данных: 

created, 

updated 

и 

deleted

соответственно. 

**Шесть шагов в MySQL**

В главе 3 мы рассмотрели задачу «Шесть шагов по

“Википедии”», цель которой состояла в том, чтобы найти связь

между любыми двумя статьями «Википедии», построив цепь

ссылок \(то есть найти способ перемещаться от одной статьи к

другой, переходя по ссылкам с одной страницы на другую\). 

Чтобы решить данную задачу, необходимо создать боты, способные не только собирать данные с сайта \(это мы уже

сделали\), но и сохранять информацию архитектурно надежным

способом, который бы позволил упростить анализ данных

впоследствии. 

Для этого нам понадобятся столбцы автоинкрементируемых

идентификаторов, временные метки и несколько таблиц. 

Чтобы решить, как лучше хранить данную информацию, необходимо мыслить абстрактно. Ссылка — всего лишь то, что

соединяет страницу A со страницей Б. Ссылка тоже вполне

может соединять страницу Б со страницей A, но это будет уже

другая ссылка. Для однозначной идентификации ссылки

достаточно сказать: «На странице A существует ссылка, которая

соединяет ее со страницей Б», другими словами, INSERTINTOlinks\(fromPageId,toPageId\)VALUES\(A,B\); \(где A и B — уникальные идентификаторы этих двух страниц\). 

Систему, состоящую из двух таблиц и предназначенную для

хранения страниц и ссылок, а также дат создания и

уникальных идентификаторов записей, можно построить

следующим образом:

CREATE TABLÈwikipedià.\`pages\` \(

ìdÌNT NOT NULL AUTO\_INCREMENT, 

ùrl\` VARCHAR\(255\) NOT NULL, 

\`created\` TIMESTAMP NOT NULL DEFAULT

CURRENT\_TIMESTAMP, 

PRIMARY KEY \(ìd\`\)\); 

 

CREATE TABLÈwikipedià.\`links\` \(

ìdÌNT NOT NULL AUTO\_INCREMENT, 

\`fromPageIdÌNT NULL, 

\`toPageIdÌNT NULL, 

\`created\` TIMESTAMP NOT NULL DEFAULT

CURRENT\_TIMESTAMP, 

PRIMARY KEY \(ìd\`\)\); 

Обратите внимание: в отличие от предыдущих веб-краулеров, которые выводили заголовок страницы на экран, здесь мы даже не сохраняем заголовок в таблице pages. 

Почему? Прежде всего потому, что для сохранения заголовка

страницы нужно зайти на нее и получить ее содержимое. Если

мы хотим создать эффективный веб-краулер для заполнения

этих таблиц, то нам нужна возможность сохранять страницу и

ссылки на нее, не посещая ее саму. 

Это верно не для всех сайтов, однако у ссылок и заголовков

страниц «Википедии» есть одно хорошее свойство: ссылки

легко преобразуются в заголовки и наоборот. Например, ссылка **http://en.wikipedia.org/wiki/Monty\_Python** ведет на

страницу с заголовком Monty Python. 

Следующая 

программа 

сохраняет 

все 

страницы

«Википедии», у которых число Бейкона \(количество ссылок

между этой страницей и страницей Кевина Бейкона

включительно\) равно 6 или меньше:

from urllib.request import urlopen

from bs4 import BeautifulSoup

import re

import pymysql

from random import shuffle

 

conn 

= 

pymysql.connect\(host='127.0.0.1', 

unix\_socket='/tmp/mysql.sock', 

user='root', 

passwd=None, db='mysql', charset='utf8'\)

cur = conn.cursor\(\)

cur.execute\('USE wikipedia'\)

 

def insertPageIfNotExists\(url\):

cur.execute\('SELECT \* FROM pages WHERE url

= %s', \(url\)\)

if cur.rowcount == 0:

cur.execute\('INSERT INTO pages \(url\)

VALUES \(%s\)', \(url\)\)

conn.commit\(\)

return cur.lastrowid

else:

return cur.fetchone\(\)\[0\]

 

def loadPages\(\):

cur.execute\('SELECT \* FROM pages'\)

pages = \[row\[1\] for row in cur.fetchall\(\)\]

return pages

 

def insertLink\(fromPageId, toPageId\):

cur.execute\('SELECT \* FROM links WHERE

fromPageId = %s ' 

'AND toPageId = %s', \(int\(fromPageId\), 

int\(toPageId\)\)\)

if cur.rowcount == 0:

cur.execute\('INSERT INTO links

\(fromPageId, toPageId\) VALUES \(%s, %s\)', 

\(int\(fromPageId\), 

int\(toPageId\)\)\)

conn.commit\(\)

 

def getLinks\(pageUrl, recursionLevel, pages\):

if recursionLevel > 4:

return

 

pageId = insertPageIfNotExists\(pageUrl\) html 

=

urlopen\('http://en.wikipedia.org\{\}'.format\(page

Url\)\)

bs = BeautifulSoup\(html, 'html.parser'\)

 

 

 

 

links 

= 

bs.find\_all\('a', 

href=re.compile\('^\(/wiki/\)\(\(?\!:\).\)\*$'\)\)

links = \[link.attrs\['href'\] for link in

links\]

 

for link in links:

 

 

 

 

 

 

 

 

insertLink\(pageId, 

insertPageIfNotExists\(link\)\)

if link not in pages:

\# Мы обнаружили новую страницу, 

добавляем ее

\# и ищем ссылки. 

pages.append\(link\)

getLinks\(link, recursionLevel\+1, 

pages\)

 

getLinks\('/wiki/Kevin\_Bacon', 0, loadPages\(\)\)

cur.close\(\)

conn.close\(\)

Три представленные ниже функции используются PyMySQL

для взаимодействия с базой данных. 

• Функция insertPageIfNotExists, как видно из названия, вставляет запись о новой странице, если таковая еще не

существует. Это в сочетании с дополняемым списком всех

собранных страниц, хранящихся в таблице pages, гарантирует, что записи страниц не будут дублироваться. 

Данная функция также позволяет находить номера pageId для создания новых ссылок. 

• Функция insertLink создает новую запись о ссылке в базе

данных. При наличии такой ссылки запись не создается. 

Даже если на странице действительно есть *несколько*

одинаковых ссылок, для нас это одна и та же ссылка, представляющая одни и те же отношения, и ей должна

соответствовать только одна запись. Это также помогает

сохранять целостность базы данных, если программа будет

запускаться несколько раз для одних и тех же страниц. 

• Функция loadPages загружает все текущие страницы из

базы данных в список, чтобы можно было определить, следует ли посетить новую страницу. Список страниц также

пополняется во время выполнения программы. Если веб-краулер запускается только один раз, начиная с пустой базы

данных, то теоретически loadPage не требуется. Однако на

практике вероятны проблемы: сбой в сети или же сбор

ссылок за несколько приемов. Важно иметь возможность

перезагрузить веб-краулер, при этом не потеряв результаты

проделанной работы. 

Следует учитывать одну потенциально проблемную

особенность 

использования 

функции 

loadPages 

и

создаваемого ею списка pages, по которому мы определяем, стоит ли посещать страницу. После загрузки страницы все

ссылки, которые есть на ней, также сохраняются как страницы, хотя веб-краулер их еще не посещал, а лишь зафиксировал

ссылки на них. Если его остановить и перезапустить, то все эти

«зафиксированные, но не просмотренные» страницы так

никогда и не будут просмотрены, а размещенные на них

ссылки — записаны. Чтобы это исправить, нужно добавить в

каждой записи страницы логическую переменную visited, которая принимает значение True, только если эта страница

была загружена, а ее исходящие ссылки — записаны. 

Однако для наших целей такого решения достаточно. Если

вы способны обеспечить длительное время выполнения \(или

только один запуск\) программы и нет необходимости собрать

полный набор ссылок \(а нужен лишь довольно большой набор

данных для экспериментов\), то добавлять переменную

visited не обязательно. 

Продолжение и окончательное решение задачи перехода от

страницы 

Кевина 

Бейкона

\(**https://en.wikipedia.org/wiki/Kevin\_Bacon**\) к странице Эрика

Айдла \(**https://en.wikipedia.org/wiki/Eric\_Idle**\) вы найдете в

пункте «Шесть шагов по “Википедии”: заключение» на с. 172, посвященном решению задач направленных графов. 

**Электронная почта**

Подобно тому как веб-страницы передаются по протоколу

HTTP, электронная почта пересылается по протоколу SMTP

\(Simple Mail Transfer Protocol, простой протокол электронной

почты\). И точно так же, как для отправки веб-страниц по HTTP

мы используем клиент веб-сервера, для отправки и получения

электронной почты серверы задействуют различные почтовые

клиенты, например Sendmail, Postfix или Mailman. 

Несмотря на то что отправка электронной почты в Python осуществляется сравнительно просто, эта операция требует

доступа к серверу, на котором работает SMTP. Настройка SMTP-клиента на сервере или локальном компьютере довольно

сложна и выходит за рамки данной книги, однако есть

множество превосходных ресурсов, позволяющих решить эту

задачу, особенно если вы работаете на Linux или в macOS. 

В следующих примерах кода предполагается, что SMTP-клиент используется локально. \(Чтобы модифицировать этот

код для удаленного SMTP-клиента, замените localhost адресом удаленного сервера.\)

Для отправки электронного письма с помощью Python достаточно всего девяти строк кода:

import smtplib

from email.mime.text import MIMEText

 

msg = MIMEText\('The body of the email is here'\)

 

msg\['Subject'\] = 'An Email Alert' 

msg\['From'\] = 'ryan@pythonscraping.com' 

msg\['To'\] = 'webmaster@pythonscraping.com' 

 

s = smtplib.SMTP\('localhost'\)

s.send\_message\(msg\)

s.quit\(\)

В Python есть два важных пакета для отправки электронной

почты: smtplib и email. 

Python-модуль email содержит полезные функции

форматирования для создания готовых к отправке пакетов

электронной почты. Применяемый здесь объект MIMEText создает пустое электронное письмо, отформатированное для

передачи по низкоуровневому протоколу MIME \(Multipurpose Internet Mail Extensions, многоцелевые расширения почтовой

интернет-службы\), через который устанавливаются SMTP-соединения более высокого уровня. Объект MIMEText содержит

адреса электронной почты, тело и заголовок, служащие в

Python для создания правильно отформатированного

электронного письма. 

Пакет smtplib содержит информацию для обработки

соединения с сервером. Как и при соединении с сервером

MySQL, это соединение, будучи созданным и использованным, должно разрываться, чтобы не создавалось слишком большого

количества соединений. 

Эту простейшую операцию с электронной почтой можно

расширить и сделать более полезной, представив ее в виде

функции:

import smtplib

from email.mime.text import MIMEText

from bs4 import BeautifulSoup

from urllib.request import urlopen

import time

 

def sendMail\(subject, body\):

msg = MIMEText\(body\)

msg\['Subject'\] = subject

 

 

 

 

msg\['From'\]

='christmas\_alerts@pythonscraping.com' 

msg\['To'\] = 'ryan@pythonscraping.com' 

 

s = smtplib.SMTP\('localhost'\)

s.send\_message\(msg\)

s.quit\(\)

 

bs 

=

BeautifulSoup\(urlopen\('https://isitchristmas.co

m/'\), 'html.parser'\)

while\(bs.find\('a', 

\{'id':'answer'\}\).attrs\['title'\] == 'NO'\):

print\('It is not Christmas yet.'\)

time.sleep\(3600\)

 

 

 

 

bs 

=

BeautifulSoup\(urlopen\('https://isitchristmas.co

m/'\), 'html.parser'\)

 

sendMail\('It\\'s Christmas\!', 

'According to http://itischristmas.com, it

is Christmas\!'\)

Этот 

скрипт 

раз 

в 

час 

проверяет 

сайт

**https://isitchristmas.com**, главной функцией которого является

вывод гигантского слова YES или NO, в зависимости от дня

года. Если скрипт заметит на сайте что-либо отличное от NO, то отправит вам электронное письмо с предупреждением о

наступлении Рождества. 

Несмотря на то что эта конкретная программа выглядит не

намного более полезной, чем обычный настенный календарь, достаточно слегка ее изменить — и можно будет делать

множество чрезвычайно нужных вещей. Она может отправлять

по электронной почте сообщения о сбоях в работе сайта, неполадках при тестировании или даже информировать о

появлении в магазине Amazon товара, который вы ожидаете, —

ничего из этого настенный календарь делать не может. 

5  *Joab Jackson*. YouTube Scales MySQL with Go Code \(http://bit.ly/1LWVmc8\), PCWorld, December 15, 2012. 

6  *Jeremy Cole and Davi Arnaut. * MySQL at Twitter \(http://bit.ly/1KHDKns\), The Twitter Engineering Blog, April 9, 2012. 

7 MySQL and Database Engineering: Mark Callaghan \(http://on.fb.me/1RFMqvw\), Facebook Engineering, March 4, 2012. 

8  *Дюбуа П. * MySQL. Сборник рецептов. — СПб.: Символ-Плюс, 2016. 

**Часть II. Углубленный веб-скрапинг**

Итак, мы рассмотрели некоторые основы создания веб-скраперов; теперь начинается самое интересное. До сих пор

наши веб-скраперы были довольно примитивными. Они не

умели получать информацию, если сервер не предоставлял ее

сразу, в удобном формате. Они принимали все за чистую

монету и сохраняли, никак не анализируя. Они пасовали перед

формами, необходимостью взаимодействия с сайтом и даже

скриптами JavaScript. Короче говоря, эти веб-скраперы

бесполезны для извлечения информации, если только она сама

не хочет быть извлеченной. 

В этой части книги вы научитесь анализировать

необработанные данные и извлекать из них историю — ту

самую, которую сайты часто скрывают под слоями JavaScript-кода, формами входа в систему и средствами защиты от веб-скрапинга. Вы узнаете, как использовать веб-скраперы для

тестирования 

сайтов, 

автоматизации 

процессов 

и

широкомасштабного доступа к Интернету. К тому времени, как

вы закончите читать этот раздел, в вашем распоряжении

появятся инструменты для сбора и обработки практически

любого типа данных, представленных в любом виде и в любом

месте Интернета. 


