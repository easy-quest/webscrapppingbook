**Часть I. Разработка веб-скраперов**

В первой части этой книги основное внимание будет уделено

базовым механизмам веб-скрапинга: как на Python построить

запрос к веб-серверу, выполнить базовую обработку

полученного ответа и начать взаимодействие с сайтом в

автоматическом режиме. В итоге вы сможете легко

путешествовать по Интернету, создавая скраперы, способные

переходить от одного домена к другому, собирать и сохранять

информацию для последующего использования. 

Честно говоря, веб-скрапинг — фантастическая отрасль: вложенные в нее относительно небольшие начальные

инвестиции окупятся сторицей. Примерно 90 % проектов веб-скрапинга, которые вам встретятся, будут опираться на

методы, описанные в следующих шести главах. Эта часть

покажет, как обычные \(хотя и технически подкованные\) люди

представляют себе работу веб-скраперов:

• извлечение HTML-данных из имени домена; 

• анализ этих данных для получения требуемой информации; 

• сохранение этой информации; 

• возможен переход на другую страницу, чтобы повторить

процедуру. 

Прочитав эту часть, вы получите прочную основу, которая

позволит вам перей ти к более сложным проектам, описанным

в части II. Не думайте, что первая половина книги менее

важна, чем вторая, и более сложные проекты описаны во

второй половине. При написании веб-скраперов вам придется

каждый день использовать почти всю информацию, изложенную в первой половине данной книги\! 

**Глава 1. Ваш первый веб-скрапер**

С первых шагов веб-скрапинга вы сразу начнете ценить все те

мелкие услуги, которые нам оказывают браузеры. Без HTML-форматирования, стилей CSS, скриптов JavaScript и рендеринга

изображений Интернет с непривычки может показаться слегка

пугающим. Но в этой и следующей главах мы посмотрим, как

форматировать и интерпретировать данные, не прибегая к

помощи браузера. 

В этой главе мы начнем с основ отправки на веб-сервер

GET-запросов — запросов на выборку или получение

содержимого заданной веб-страницы, чтения полученного с

нее HTML-кода и выполнения ряда простых операций по

извлечению данных, чтобы выделить оттуда контент, который

вы ищете. 

**Установка соединения**

Если вам прежде не приходилось много работать с сетями или

заниматься вопросами сетевой безопасности, то механика

работы Интернета может показаться несколько загадочной. Вы

же не хотите каждый раз задумываться о том, что именно

делает сеть, когда вы открываете браузер и заходите на сайт

**http://google.com**. Впрочем, в наше время это и не нужно. 

Компьютерные интерфейсы стали настолько совершенными, что большинство людей, пользующихся Интернетом, не имеют

ни малейшего представления о том, как работает Сеть, и это, по-моему, здорово. 

Однако веб-скрапинг требует сбросить покров с этого

интерфейса — на уровне не только браузера \(то, как он

интерпретирует весь HTML-, CSS- и JavaScript-код\), но иногда и

сетевого подключения. 

Чтобы дать вам представление об инфраструктуре, необходимой для получения информации в браузере, рассмотрим следующий пример. У Алисы есть веб-сервер, а у

Боба — ПК, с которого он хочет подключиться к серверу Алисы. 

Когда одна машина собирается пообщаться с другой, происходит примерно следующий обмен данными. 

1. Компьютер Боба посылает поток битов — единиц и нулей, которым соответствует высокое и низкое напряжение в

кабеле. Из этих битов складывается некая информация, делимая на заголовок и тело. В заголовке содержится MAC-адрес ближайшего локального маршрутизатора и IP-адрес

конечной точки — сервера Алисы. В тело включен запрос

Боба к серверному приложению Алисы. 

2. Локальный маршрутизатор Боба получает все эти нули и

единицы и интерпретирует их как пакет, который

передается с MAC-адреса Боба на IP-адрес Алисы. 

Маршрутизатор Боба ставит свой IP-адрес на пакете в графе

«отправитель» и отправляет пакет через Интернет. 

3. Пакет Боба проходит через несколько промежуточных

серверов, которые по соответствующим кабелям передают

пакет на сервер Алисы. 

4. Сервер Алисы получает пакет по своему IP-адресу. 

5. Сервер Алисы считывает из заголовка пакета номер порта-приемника 

и 

передает 

пакет 

соответствующему

приложению веб-сервера. \(Портом-приемником пакета в

случае веб-приложений почти всегда является порт номер

80; это словно номер квартиры в адресе для пакетных

данных, тогда как IP-адрес аналогичен названию улицы и

номеру дома.\)

6. Серверное приложение получает поток данных от

серверного процессора. Эти данные содержат примерно

такую информацию:

• вид запроса: GET; 

• имя запрошенного файла: index.html. 

7. Веб-сервер находит соответствующий HTML-файл, помещает

его в новый пакет, созданный для Боба, и отправляет этот

пакет на локальный маршрутизатор, откуда он уже

знакомым нам способом передается на компьютер Боба. 

Вуаля\! Так работает Интернет. 

В чем же при этом обмене данными участвует браузер? 

Ответ: ни в чем. В действительности браузеры — сравнительно

недавнее изобретение в истории Интернета: Nexus появился

всего в 1990 году. 

Конечно, браузер — полезное приложение, которое создает

эти информационные пакеты, сообщает операционной

системе об их отправке и интерпретирует данные, превращая

их в красивые картинки, звуки, видео и текст. Однако браузер

— это всего лишь код, который можно разбить на части, выделить основные компоненты, переписать, использовать

повторно и приспособить для выполнения чего угодно. Браузер

может дать команду процессору отправлять данные в

приложение, поддерживающее беспроводной \(или проводной\) сетевой интерфейс, но то же самое можно сделать и на Python с

помощью всего трех строк кода:

from urllib.request import urlopen html 

=

urlopen\('http://pythonscraping.com/pages/page1. 

html'\)

print\(html.read\(\)\)

Для выполнения этого кода можно использовать оболочку

iPython, которая размещена в репозитории GitHub для главы 1

\(**https://github.com/REMitchell/python-**

**scraping/blob/master/Chapter01\_BeginningToScrape.ipynb**\), или же

сохранить код на компьютере в файле scrapetest.py и

запустить его в окне терминала с помощью следующей

команды:

**$ python scrapetest.py**

Обратите внимание: если на вашем компьютере, кроме

Python 3.x, также установлен Python 2.x, и вы используете обе

версии Python параллельно, то может потребоваться явно

вызвать Python 3.x, выполнив команду следующим образом: **$ python3 scrapetest.py**

По этой команде выводится полный HTML-код страницы

page1, 

расположенной 

по 

адресу

**http://pythonscraping.com/pages/page1.html**. Точнее, выводится

HTML-файл page1.html, размещенный в каталоге **<корневой**

**веб-каталог>/pages** на сервере с доменным именем

**http://pythonscraping.com**. 

Почему так важно представлять себе эти адреса как

«файлы», а не как «страницы»? Большинство современных веб-страниц связано с множеством файлов ресурсов. Ими могут

быть файлы изображений, скриптов JavaScript, стилей CSS и

любой другой контент, на который ссылается запрашиваемая

страница. 

Например, 

встретив 

тег

<imgsrc="cuteKitten.jpg">, 

браузер 

знает: 

чтобы

сгенерировать страницу для пользователя, нужно сделать еще

один запрос к серверу и получить данные из файла

cuteKitten.jpg. 

Разумеется, у нашего скрипта на Python нет логики, позволяющей вернуться и запросить несколько файлов \(пока

что\); он читает только тот HTML-файл, который мы запросили

напрямую:

from urllib.request import urlopen

Эта строка делает именно то, что кажется на первый взгляд: находит модуль Python для запросов \(в библиотеке urllib\) и

импортирует оттуда одну функцию — urlopen. 

Библиотека urllib — это стандартная библиотека Python \(другими словами, для запуска данного примера ничего не

нужно устанавливать дополнительно\), в которой содержатся

функции для запроса данных через Интернет, обработки

файлов cookie и даже изменения метаданных, таких как

заголовки и пользовательский программный агент. Мы будем

активно применять urllib в данной книге, так что я

рекомендую вам прочитать раздел документации Python, касающийся 

этой 

библиотеки

\(**https://docs.python.org/3/library/urllib.html**\). 

Функция urlopen открывает удаленный объект по сети и

читает его. Поскольку это практически универсальная функция

\(она одинаково легко читает HTML-файлы, файлы

изображений и другие файловые потоки\), мы будем довольно

часто использовать ее в данной книге. 

**Знакомство с BeautifulSoup**

*Прекрасный суп в столовой*

*ждет. *

*Из миски жирный пар идет. *

*Не любит супа тот, кто глуп\! *

*Прекрасный суп, вечерний суп\! *

*Прекрасный суп, вечерний суп\! *

*Алиса в Стране чудес. Издание 1958 г., пер. А. Оленича-Гнененко*

Библиотека BeautifulSoup \(«Прекрасный суп»\) названа так в

честь одноименного стихотворения Льюиса Кэрролла из книги

«Алиса в Стране чудес». В книге это стихотворение поет

Фальшивая Черепаха \(пародия на популярное викторианское

блюдо — фальшивый черепаховый суп, который варят не из

черепахи, а из говядины\). 

Приложение BeautifulSoup стремится найти смысл в

бессмысленном: помогает отформатировать и упорядочить

«грязные» сетевые данные, исправляя ошибки в HTML-коде и

создавая легко обходимые \(traversable\) объекты Python, являющиеся представлениями структур XML. 

**Установка BeautifulSoup**

Поскольку BeautifulSoup не является стандартной библиотекой

Python, ее необходимо установить. Если у вас уже есть опыт

установки библиотек Python, то используйте любимый

установщик и пропустите этот подраздел, сразу перейдя к

следующему — «Работа с BeautifulSoup» на с. 29. 

Для тех же, кому еще не приходилось устанавливать

библиотеки Python \(или кто подзабыл, как это делается\), представлен общий подход, который мы будем использовать

для установки библиотек по всей книге, так что впоследствии

вы можете обращаться к данному подразделу. 

В этой книге мы будем использовать библиотеку

BeautifulSoup 4 \(также известную как BS4\). Подробная

инструкция по установке BeautifulSoup 4 размещена на сайте

Crummy.com

\(**http://www.crummy.com/software/BeautifulSoup/bs4/doc/**\); здесь

же описан самый простой способ для Linux:

$ sudo apt-get install python-bs4

И для Mac:

$ sudo easy\_install pip

Эта команда устанавливает менеджер пакетов Python *pip*. 

Затем выполните следующую команду для установки

библиотеки:

$ pip install beautifulsoup4

Еще раз подчеркну: если на вашем компьютере установлены

версии Python 2.x и 3.x, то лучше явно вызвать python3: $ python3 myScript.py

То же самое касается установки пакетов, иначе пакеты

могут установиться не для Python 3.x, а в Python 2.x: $ sudo python3 setup.py install

При использовании pip с целью установки пакета для Python 3.x также можно вызвать pip3:

$ pip3 install beautifulsoup4

Установка пакетов для Windows практически не отличается

от данного процесса для Mac и Linux. Скачайте последнюю

версию BeautifulSoup 4 со страницы скачивания библиотеки

\(**http://www.crummy.com/software/BeautifulSoup/\#Download**\), перейдите в каталог, в котором вы ее распаковали, и

выполните следующую команду:

> python setup.py install

Готово\! Теперь компьютер будет распознавать BeautifulSoup как библиотеку Python. Чтобы в этом убедиться, откройте

терминал Python и импортируйте ее:

$ python

> from bs4 import BeautifulSoup

Импорт должен выполниться без ошибок. 

Кроме того, для Windows есть программа-установщик

менеджера пакетов pip \(**https://pypi.python.org/pypi/setuptools**\), с

помощью которой можно легко устанавливать пакеты и

управлять ими:

> pip install beautifulsoup4

**Хранение библиотек непосредственно в виртуальных**

**окружениях**

Если вы собираетесь работать параллельно с несколькими

проектами Python, или вам нужен удобный способ связать

проекты с их библиотеками, или вы хотите исключить

потенциальные 

конфликты 

между 

установленными

библиотеками, то можете установить виртуальное окружение

Python, где все разделено и легко управляется. 

Если библиотека Python устанавливается без виртуального

окружения, то она устанавливается *глобально*. Обычно такую

установку 

должен 

выполнять 

администратор 

или

пользователь **root**, и тогда библиотека Python станет доступна

для всех пользователей и проектов на данном компьютере. К

счастью, создать виртуальное окружение легко:

$ virtualenv scrapingEnv

Эта команда создает новое окружение с именем

scrapingEnv. Чтобы использовать данное окружение, его

необходимо активировать:

$ cd scrapingEnv/

$ source bin/activate

После активации окружения его имя появится в командной

строке, подсказывая, в каком именно окружении вы

работаете. Все устанавливаемые библиотеки и запускаемые

скрипты будут находиться только в этом виртуальном

окружении. 

В окружении scrapingEnv можно установить и

использовать BeautifulSoup, например, так:

\(scrapingEnv\)ryan$ 

pip 

install

beautifulsoup4

\(scrapingEnv\)ryan$ python

> from bs4 import BeautifulSoup

> 

Выйти из окружения можно с помощью команды

deactivate, после чего библиотеки, которые были

установлены внутри виртуального окружения, станут

недоступными:

\(scrapingEnv\)ryan$ deactivate

ryan$ python

> from bs4 import BeautifulSoup

Traceback \(most recent call last\):

File "<stdin>", line 1, in <module> ImportError: No module named 'bs4' 

Хранение всей библиотеки с разделением по проектам

позволит легко заархивировать всю папку окружения и

отправить ее кому-нибудь. Если на компьютере получателя

установлена та же версия Python, что и у вас, то ваш код

будет работать в его виртуальном окружении, не требуя

дополнительной установки каких-либо библиотек. 

Я не буду настаивать, чтобы вы применяли виртуальное

окружение при выполнении всех примеров этой книги, однако помните: вы можете воспользоваться им в любой

момент, просто заранее его активировав. 

**Работа с BeautifulSoup**

Из всех объектов библиотеки BeautifulSoup чаще всего

используется собственно, сам BeautifulSoup. Посмотрим, как

он работает, изменив пример, приведенный в начале главы: from urllib.request import urlopen

from bs4 import BeautifulSoup

![Image 7](images/000061.png)

 

html 

=

urlopen\('http://www.pythonscraping.com/pages/pa

ge1.html'\)

bs = BeautifulSoup\(html.read\(\), 'html.parser'\)

print\(bs.h1\)

Результат выглядит так:

<h1>An Interesting Title</h1> 

Обратите внимание: этот код возвращает только первый

попавшийся ему на странице экземпляр тега h1. По

существующему соглашению на странице может быть только

один тег h1, однако принятые в Интернете соглашения часто

нарушаются. Поэтому следует помнить, что таким образом

будет получен только первый экземпляр тега и не обязательно

тот, который вы ищете. 

Как и в предыдущих примерах веб-скрапинга, мы

импортируем функцию urlopen и вызываем html.read\(\), чтобы получить контент страницы в формате HTML. Помимо

текстовой строки, BeautifulSoup также может принимать

файловый объект, непосредственно возвращаемый функцией

urlopen. Чтобы получить этот объект, не нужно вызывать

функцию .read\(\):

bs = BeautifulSoup\(html, 'html.parser'\)

Здесь контент HTML-файла преобразуется в объект

BeautifulSoup, имеющий следующую структуру:

**html ** 

<html><head>...</head><body>...</body> 

</html> 

![Image 8](images/000014.png)

![Image 9](images/000042.png)

![Image 10](images/000071.png)

![Image 11](images/000021.png)

![Image 12](images/000050.png)

![Image 13](images/000013.png)

![Image 14](images/000027.png)

— **head ** <head><title>A Useful Page<title> 

</head> 

— **title ** <title>A Useful Page</title> 

— **body ** <body><h1>An Int...</h1><div>Lorem ip...</div></body> 

— **h1 ** <h1>An Interesting Title</h1> 

— **div ** <div>Lorem Ipsum dolor...</div> Обратите внимание: тег h1, извлеченный из кода страницы, находится 

на 

втором 

уровне 

структуры 

объекта

BeautifulSoup \(html body h1\). Однако, извлекая h1 из

объекта, мы обращаемся к этому тегу напрямую:

bs.h1

На практике все следующие вызовы функций приведут к

одинаковым результатам:

bs.html.body.h1

bs.body.h1

bs.html.h1

При 

создании 

объекта 

BeautifulSoup 

функции

передаются два аргумента:

bs = BeautifulSoup\(html.read\(\), 'html.parser'\)

Первый аргумент — это текст в формате HTML, на основе

которого строится объект, а второй — синтаксический

анализатор, который BeautifulSoup будет использовать для

построения объекта. В большинстве случаев не имеет

значения, какой именно синтаксический анализатор будет

применяться. 

Анализатор html.parser входит в состав Python 3 и не

требует дополнительной настройки перед использованием. За

редким исключением, в этой книге я буду применять именно

его. 

Еще 

один 

популярный 

анализатор 

— 

lxml

\(**http://lxml.de/parsing.html**\). Он устанавливается через pip: $ pip3 install lxml

Для того чтобы использовать lxml в BeautifulSoup, нужно изменить имя синтаксического анализатора в знакомой

нам строке:

bs = BeautifulSoup\(html.read\(\), 'lxml'\)

Преимущество lxml, по сравнению с html.parser, состоит

в том, что lxml в целом лучше справляется с «грязным» или

искаженным HTML-кодом. Анализатор lxml прощает

неточности и исправляет такие проблемы, как незакрытые и

неправильно вложенные теги, а также отсутствующие теги

head или body. Кроме того, lxml работает несколько быстрее, чем html.parser, хотя при веб-скрапинге скорость

анализатора не всегда является преимуществом, поскольку

почти всегда главное узкое место — скорость самого сетевого

соединения. 

Недостатками анализатора lxml является то, что его

необходимо специально устанавливать и он зависит от

сторонних C-библиотек. Это может вызвать проблемы

портируемости; кроме того, html.parser проще в

использовании. 

Еще один популярный синтаксический анализатор HTML

называется html5lib. Подобно lxml, он чрезвычайно лоялен к

ошибкам и прилагает еще больше усилий к исправлению

некорректного HTML-кода. Он также имеет внешние

зависимости и работает медленнее, чем lxml и html.parser. 

Тем не менее выбор html5lib может быть оправданным при

работе с «грязными» или написанными вручную HTML-страницами. 

Чтобы использовать этот анализатор, нужно установить его

и передать объекту BeautifulSoup строку html5lib: bs = BeautifulSoup\(html.read\(\), 'html5lib'\)

Надеюсь, благодаря этой краткой дегустации BeautifulSoup вы составили представление о возможностях и удобстве

данной библиотеки. В сущности, она позволяет извлечь любую

информацию из любого файла в формате HTML \(или XML\), если содержимое данного файла заключено в идентифицирующий тег или этот тег хотя бы присутствует в принципе. В

главе 2 мы подробно рассмотрим более сложные вызовы

функций библиотеки, а также регулярные выражения и

способы их использования с помощью BeautifulSoup для

извлечения информации с сайтов. 

**Надежное соединение и обработка исключений**

Сеть — «грязное» место. Данные плохо отформатированы, сайты то и дело «падают», а разработчики страниц постоянно

забывают ставить закрывающие теги. Один из самых

неприятных моментов, связанных с веб-скрапингом, — уйти

спать и оставить работающий скрапер, рассчитывая назавтра

иметь все данные в вашей базе, а утром обнаружить, что

скрапер столкнулся с ошибкой в каком-то непредсказуемом

формате данных и почти сразу прекратил работу, стоило вам

отвернуться от экрана. В подобных случаях возникает соблазн

проклясть разработчика, который создал тот сайт \(и выбрал

странный формат представления данных\). Однако на самом

деле если кому и стоит дать пинка, то это вам самим, поскольку

именно вы не предусмотрели исключение\! 

Рассмотрим первую строку нашего скрапера, сразу после

операторов импорта, и подумаем, как можно обрабатывать

любые исключения, которые могли бы здесь возникнуть: html 

=

urlopen\('http://www.pythonscraping.com/pages/pa

ge1.html'\)

Здесь могут случиться две основные неприятности:

• на сервере нет такой страницы \(или при ее получении

произошла ошибка\); 

• нет такого сервера. 

В первой ситуации будет возвращена ошибка HTTP. Это

может быть 404 Page Not Found, 500 Internal Server Error и т.п. 

Во всех таких случаях функция urlopen выдаст обобщенное

исключение HTTPError. Его можно обработать следующим

образом:

from urllib.request import urlopen

from urllib.error import HTTPError

 

try:

 

 

 

 

html 

=

urlopen\('http://www.pythonscraping.com/pages/pa

ge1.html'\)

except HTTPError as e:

print\(e\)

\# Вернуть ноль, прекратить работу или

\# выполнить еще какой-нибудь "план Б". 

else:

\# Продолжить выполнение программы. 

\# Примечание: если при перехвате исключений

\# программа прерывает работу или происходит

возврат

\# из функции, то оператор else не нужен. 

Теперь в случае возвращения кода HTTP-ошибки выводится

сообщение о ней и остальная часть программы, которая

находится в ветви else, не выполняется. 

Если не найден весь сервер \(например, сервер по адресу

**http://www.pythonscra ping.com** отключен или URL указан с

ошибкой\), то функция urlopen возвращает URLError. Эта

ошибка говорит о том, что ни один из указанных серверов не

доступен. Поскольку именно удаленный сервер отвечает за

возвращение кодов состояния HTTP, ошибка HTTPError не

может быть выдана и вместо нее следует обрабатывать более

серьезную ошибку URLError. Для этого можно добавить в

программу такую проверку:

from urllib.request import urlopen

from urllib.error import HTTPError

from urllib.error import URLError

 

try:

 

 

 

 

html 

=

urlopen\('https://pythonscrapingthisurldoesnotex

ist.com'\)

except HTTPError as e:

print\(e\)

except URLError as e:

print\('The server could not be found\!'\)

else:

print\('It Worked\!'\)

Конечно, даже если страница успешно получена с сервера, все равно остается проблема с ее контентом, который не всегда

соответствует ожидаемому. Всякий раз, обращаясь к тегу в

объекте BeautifulSoup, разумно добавить проверку того, существует ли этот тег. При попытке доступа к

несуществующему тегу BeautifulSoup возвращает объект

None. Проблема в том, что попытка обратиться к тегу самого

объекта None приводит к возникновению ошибки

AttributeError. 

Следующая строка \(в которой nonExistentTag —

несуществующий тег, а не имя реальной функции

BeautifulSoup\) возвращает объект None:

print\(bs.nonExistentTag\)

Этот объект вполне доступен для обработки и проверки. 

Проблема возникает в том случае, если продолжать его

использовать без проверки и попытаться вызвать для объекта

None другую функцию:

print\(bs.nonExistentTag.someTag\)

Эта функция вернет исключение:

AttributeError: 'NoneType' object has no

attribute 'someTag' 

Как же застраховаться от этих ситуаций? Проще всего —

явно проверить обе ситуации:

try:

badContent = bs.nonExistingTag.anotherTag

except AttributeError as e:

print\('Tag was not found'\)

else:

if badContent == None:

print \('Tag was not found'\)

else:

print\(badContent\)

Такие проверка и обработка каждой ошибки поначалу могут

показаться трудоемкими, однако если немного упорядочить

код, то его станет проще писать \(и, что еще важнее, гораздо

проще читать\). Вот, например, все тот же наш скрапер, написанный немного по-другому:

from urllib.request import urlopen

from urllib.error import HTTPError

from bs4 import BeautifulSoup

 

def getTitle\(url\):

try:

html = urlopen\(url\)

except HTTPError as e:

return None

try:

bs = BeautifulSoup\(html.read\(\), 

'html.parser'\)

title = bs.body.h1

except AttributeError as e:

return None

return title



title 

=

getTitle\('http://www.pythonscraping.com/pages/p

age1.html'\)

if title == None:

print\('Title could not be found'\)

else:

print\(title\)

В этом примере мы создаем функцию getTitle, которая

возвращает либо заголовок страницы, либо, если получить его

не удалось, — объект None. Внутри getTitle мы, как в

предыдущем примере, проверяем наличие HTTPError и

инкапсулируем две строки BeautifulSoup внутри оператора try. 

Ошибка AttributeError может возникнуть в любой из этих

строк \(если сервер не найден, то html вернет объект None, а

html.read\(\) выдаст AttributeError\). Фактически внутри

оператора try можно разместить любое количество строк или

вообще вызвать другую функцию, которая будет генерировать

AttributeError в любой момент. 

При написании скраперов важно продумать общий шаблон

кода, который бы обрабатывал исключения, но при этом был

бы читабельным. Вы также, вероятно, захотите использовать

код многократно. Наличие обобщенных функций, таких как

getSiteHTML и getTitle \(в сочетании с тщательной

обработкой исключений\), позволяет быстро — и надежно —

собирать данные с веб-страниц в Сети. 

**Глава 2. Углубленный синтаксический анализ**

**HTML-кода**

Однажды Микеланджело спросили, как ему удалось создать

такой шедевр, как «Давид». Известен его ответ: «Это легко. Вы

просто срезаете ту часть камня, которая не похожа на Давида». 

Большинство веб-скраперов мало напоминают мраморные

статуи, однако при извлечении информации из сложных веб-страниц стоит придерживаться аналогичного подхода. 

Существует множество способов отбрасывать контент, не

похожий на тот, что вы ищете, до тех пор, пока не доберетесь

до нужной информации. В этой главе вы узнаете, как

выполнять анализ сложных HTML-страниц, чтобы извлекать из

них только необходимую вам информацию. 

**Иногда молоток не требуется**

Столкнувшись с гордиевыми узлами тегов, многие испытывают

острое желание углубиться в них с помощью многострочных

операторов в расчете извлечь ценную информацию. Однако

следует помнить: безрассудное наслаивание методов, описанных в этом разделе, может привести к тому, что код

будет трудно отлаживать, или он станет постоянно сбоить, или

же и то и другое. Прежде чем начать, рассмотрим несколько

способов, которые позволяют вообще обойтись без

углубленного синтаксического анализа HTML-кода\! 

Предположим, 

мы 

ищем 

некий 

контент: 

имя, 

статистические данные или блок текста. Возможно, этот

контент похоронен под 20 тегами в каше из HTML-кода и не

отличается никакими особыми HTML-тегами или атрибутами. 

Допустим, мы решили отбросить осторожность и написать

примерно следующее в попытке извлечь нужные данные: bs.find\_all\('table'\)\[4\].find\_all\('tr'\)

\[2\].find\('td'\).find\_all\('div'\)\[1\].find\('a'\)

Выглядит не слишком красиво. Но дело не только в

эстетике: стоит администратору сайта внести малейшее

изменение, и весь наш веб-скрапер сломается. А если

разработчик сайта решит добавить еще одну таблицу или еще

один столбец данных? Или разместит в верхней части

страницы еще один компонент \(с несколькими тегами div\)? 

Показанная выше строка кода нестабильна: она опирается на

то, что структура сайта никогда не изменится. 

Что же делать? 

• Найдите ссылку **Print This Page** \(Распечатать эту страницу\) или, возможно, мобильную версию сайта с более удачным

HTML-форматированием \(по дробнее о том, как выдать себя

за мобильное устройство и получить мобильную версию

сайта, см. в главе 14\). 

• Поищите информацию в файле JavaScript. Учтите, что для

этого 

вам 

может 

понадобиться 

исследовать

импортированные файлы JavaScript. Например, однажды я

получила с сайта адреса улиц \(вместе с широтой и долготой\) в виде аккуратно отформатированного массива, заглянув в

JavaScript-код встроенной карты Google, на которой были

точно отмечены все адреса. 

• Это больше касается заголовков, однако информация может

находиться в URL самой страницы. 

• Если информация, которую вы ищете, по какой-либо

причине является уникальной для данного сайта, — вам не

повезло. В противном случае попробуйте найти другие

источники, из которых можно было бы получить эту

информацию. Нет ли другого сайта с теми же данными? 

Возможно, на нем приводятся данные, скопированные или

агрегированные с другого сайта? 

Когда речь идет о скрытых или плохо отформатированных

данных, особенно важно не зарываться в код, не загонять себя

в кроличью нору, из которой потом можно и не выбраться. 

Лучше сделайте глубокий вдох и подумайте: нет ли других

вариантов? 

На тот случай, если вы уверены, что альтернатив не

существует, в остальной части этой главы описываются

стандартные и нестандартные способы выбора тегов по их

расположению, контексту, атрибутам и содержимому. 

Представленные здесь методы, при условии правильного

использования, способны значительно облегчить написание

более стабильных и надежных веб-краулеров. 

**Еще одна тарелка BeautifulSoup**

В главе 1 мы кратко рассмотрели установку и запуск

BeautifulSoup, а также выбор объектов по одному. В этом

разделе обсудим поиск тегов по атрибутам, работу со списками

тегов и навигацию по дереву синтаксического анализа. 

Почти любой сайт, с которым вам придется иметь дело, содержит таблицы стилей. На первый взгляд может показаться, что стили на сайтах предназначены исключительно для показа

сайта пользователю в браузере. Однако это неверно: появление

CSS стало настоящим благом для веб-скраперов. Чтобы

назначать элементам разные стили, CSS опирается на

дифференциацию HTML-элементов, которые в противном

случае имели бы одинаковую разметку. Например, одни теги

могут выглядеть так:

<span class="green"></span> А другие — так:

<span class="red"></span> 

Веб-скраперы легко различают эти два тега по их классам; например, с по мощью BeautifulSoup веб-скрапер может

собрать весь красный текст, игнорируя зеленый. Поскольку CSS

использует 

эти 

идентифицирующие 

атрибуты 

для

соответствующего оформления сайтов, мы можем быть

практически уверены в том, что на большинстве современных

сайтов будет много атрибутов class и id. 

Создадим пример веб-скрапера, который сканирует

страницу, 

расположенную 

по 

адресу

**http://www.pythonscraping.com/pages/warandpeace.html**. 

На этой странице строки, в которых содержатся реплики

персонажей, выделены красным цветом, а имена персонажей

— зеленым. В следующем примере исходного кода страницы

показаны теги span, которым присвоены соответствующие

классы CSS:

<span class="red">Heavens\! what a virulent attack\!</span> replied

<span class="green">the prince</span>, not in the least disconcerted

by this reception. 

С помощью программы, аналогичной той, которая была

рассмотрена в главе 1, можно прочитать всю страницу и

создать на ее основе объект BeautifulSoup:

from urllib.request import urlopen

from bs4 import BeautifulSoup

 

html 

=

urlopen\('http://www.pythonscraping.com/pages/pa

ge1.html'\)

bs = BeautifulSoup\(html.read\(\), 'html.parser'\)

С помощью этого объекта BeautifulSoup можно вызвать

функцию find\_all и извлечь Python-список всех имен

персонажей, полученных путем выбора текста из тегов

<spanclass="green"></span> \(find\_all — очень гибкая

функция, которую мы будем широко использовать в этой

книге\):

nameList 

= 

bs.find\_all\('span', 

\{'class':'green'\}\)

for name in nameList:

print\(name.get\_text\(\)\)

Результатом выполнения этого кода должен стать список

всех персонажей «Войны и мира» в порядке их появления в

тексте. Так что же здесь происходит? Раньше мы вызывали

функцию bs.имяТега и получали первое появление тега на

странице. 

Теперь 

мы 

вызываем 

функцию

bs.find\_all\(имяТега,атрибутыТега\), чтобы получить не

только первый тег, а список всех тегов, присутствующих на

странице. 

![Image 15](images/000062.png)

Получив список персонажей, программа перебирает все

имена в списке и использует функцию name.get\_text\(\), чтобы очистить контент от тегов. 

**Когда использовать get\_text\(\), а когда — сохранять теги**

Функция .get\_text\(\) удаляет из документа, с которым вы

работаете, все теги и возвращает строку, содержащую только

текст в кодировке Unicode. Например, при работе с большим

блоком текста, содержащим много гиперссылок, абзацев и

других тегов, все эти теги будут удалены, останется только текст. 

Учтите, что в объекте BeautifulSoup гораздо проще найти

нужное, чем в текстовом фрагменте. Вызов .get\_text\(\) всегда

должен быть последним, что вы делаете непосредственно

перед выводом результата на экран, сохранением или

манипулированием готовыми данными. 

Как правило, следует как можно дольше сохранять структуру

тегов документа. 

**Функции find\(\) и find\_all\(\)**

Функции BeautifulSoup find\(\) и find\_all\(\) вы, скорее всего, будете использовать чаще других. С помощью этих функций

можно легко фильтровать HTML-страницы, чтобы выделить

списки нужных тегов или найти отдельный тег по

всевозможным атрибутам. 

Эти две функции очень похожи, о чем свидетельствуют их

определения в документации BeautifulSoup:

find\_all\(tag, attributes, recursive, text, 

limit, keywords\)

find\(tag, 

attributes, 

recursive, 

text, 

keywords\)

Скорее всего, в 95 % случаев вы будете использовать только

первые два аргумента: tag и attributes. Однако мы

подробно рассмотрим все аргументы. 

Аргумент tag нам уже встречался; мы можем передать

функции строку, содержащую имя тега, или даже Python-список имен тегов. Например, следующий код возвращает

список всех тегов заголовков, встречающихся в документе2:

.find\_all\(\['h1','h2','h3','h4','h5','h6'\]\)

Аргумент attribute принимает Python-словарь атрибутов

и ищет теги, которые содержат любой из этих атрибутов. 

Например, следующая функция ищет в HTML-документе теги

span с классом green *или* red:

.find\_all\('span', \{'class':\{'green', 'red'\}\}\)

Аргумент recursive логический. Насколько глубоко вы

хотите исследовать документ? Если recursive присвоено

значение True, то функция find\_all ищет теги, соответствующие заданным параметрам, в дочерних

элементах и их потомках. Если же значение этого аргумента

равно False, то функция будет просматривать только теги

верхнего уровня документа. По умолчанию find\_all работает

рекурсивно \(recurive имеет значение True\); обычно лучше

оставить все как есть, за исключением ситуаций, когда вы

точно знаете, что делаете, и нужно обеспечить высокую

производительность. 

Аргумент text необычен из-за отношения не к свойствам

тегов, а к их текстовому контенту. Так, чтобы узнать, сколько

раз на странице встречается слово the prince, заключенное в

теги, можно заменить функцию .find\_all\(\) из предыдущего

примера на следующие строки:

nameList = bs.find\_all\(text='the prince'\)

print\(len\(nameList\)\)

Результатом будет число 7. 

Аргумент limit по понятным причинам используется

только в методе find\_all; функция find эквивалентна

вызову find\_all со значением limit, равным 1. Этот

аргумент можно использовать в тех случаях, когда вы хотите

извлечь только первые *x* элементов, присутствующих на

странице. Однако следует учитывать, что вы получите первые

элементы в порядке их появления на странице, и это вовсе не

обязательно будут те элементы, которые вам нужны. 

Аргумент keyword позволяет выбрать теги, содержащие

определенный атрибут или набор атрибутов. Например: title = bs.find\_all\(id='title', class\_='text'\)

Этот код возвращает первый тег со словом text в атрибуте

class\_ и словом title в атрибуте id. Обратите внимание: по

соглашению всем значениям атрибута id на странице следует

быть уникальными. Поэтому на практике такая строка может

быть не особенно полезна и должна быть эквивалентна

следующей:

title = bs.find\(id='title'\)

**Аргумент keyword и атрибут class** В определенных ситуациях аргумент keyword может быть

полезен. Однако технически, как свойство объекта

BeautifulSoup, он избыточен. Помните: все, что можно

сделать с помощью keyword, также можно выполнить

методами, описанными далее в этой главе \(см. разделы

«Регулярные выражения» на с. 46 и «Лямбда-выражения» на

с. 52\). 

Например, следующие две строки кода работают одинаково: bs.find\_all\(id='text'\)

bs.find\_all\('', \{'id':'text'\}\)

Кроме того, используя keyword, вы периодически будете

сталкиваться с проблемами, особенно при поиске элементов

по атрибуту class, поскольку class в Python является

защищенным ключевым словом. Другими словами, class —

зарезервированное слово Python, которое нельзя применять

в качестве имени переменной или аргумента \(это не имеет

никакого отношения к обсуждавшемуся ранее аргументу

keyword 

функции 

BeautifulSoup.find\_all\(\)\). 

Например, попытка выполнить следующий код повлечет

синтаксическую ошибку из-за нестандартного использования

слова class:

bs.find\_all\(class='green'\)

Вместо этого можно применить несколько неуклюжее

решение BeautifulSoup с добавлением подчеркивания:

bs.find\_all\(class\_='green'\)

Кроме того, можно заключить слово class в кавычки: bs.find\_all\('', \{'class':'green'\}\)

В этот момент у вас может возникнуть вопрос: «Постойте, но ведь я уже знаю, как получить тег со списком атрибутов: нужно передать в функцию атрибуты в виде словарного

списка\!»

Напомню: передача списка тегов в .find\_all\(\) в виде

списка атрибутов действует как фильтр «или» \(функция

выбирает тег, присутствующий в списке: тег1, тег2, тег3 и

т.д.\). Если список тегов достаточно длинный, то можно

получить множество ненужных данных. Аргумент keyword позволяет добавить к этому списку дополнительный фильтр

«и». 

**Другие объекты BeautifulSoup**

До сих пор в этой книге нам встречались два типа объектов

библиотеки BeautifulSoup:

• *объекты* BeautifulSoup 

— 

экземпляры, 

которые 

в

предыдущих примерах кода встречались в виде переменной

bs; 

*• объекты* Tag — в виде списков или отдельных элементов, как

результаты вызовов функций find и find\_all для объекта

BeautifulSoup или полученные при проходе по структуре

объекта BeautifulSoup:

bs.div.h1

Однако в библиотеке есть еще два объекта, которые

используются реже, но все же о них важно знать:

• *объекты* NavigableString — служат для представления не

самих тегов, а текста внутри тегов \(некоторые функции

принимают и создают не объекты тегов, а объекты

NavigableString\); 

• *объекты* Comment — применяются для поиска HTML-комментариев, заключенных в теги комментариев, <\!--

например,так-->. 

Из всей библиотеки BeautifulSoup вам придется иметь дело

только с этими четырьмя объектами \(на момент написания

данной книги\). 

**Навигация по деревьям**

Функция find\_all выполняет поиск тегов по их именам и

атрибутам. Но как быть, если нужно найти тег по его

расположению в документе? Здесь нам пригодится навигация

по дереву. В главе 1 мы рассмотрели навигацию по дереву

BeautifulSoup только в одном направлении:

bs.tag.subTag.anotherSubTag

Теперь рассмотрим навигацию по деревьям HTML-кода во

всех направлениях: вверх, по горизонтали и диагонали. В

качестве образца для веб-скрапинга мы будем использовать

наш весьма сомнительный интернет-магазин, размещенный

по 

адресу 

**http://www.pythonscraping.com/pages/page3.html**, показанный на рис. 2.1. 

![Image 16](images/000012.png)

 

**Рис. 2.1. ** Снимок экрана с сайта http://www.pythonscraping.com/pages/page3.html HTML-код этой страницы, представленный в виде дерева

\(некоторые теги для краткости опущены\), выглядит так: HTML

— body

— div.wrapper

— h1

— div.content

— table\#giftList

— tr

— th

— th

— th

— th

— tr.gift\#gift1

— td

— td

— span.excitingNote

— td

— td

— img

— ...другие строки таблицы... 

— div.footer

Мы будем использовать эту HTML-структуру в качестве

примера в нескольких следующих разделах. 

**Работа с детьми и другими потомками**

В информатике и некоторых разделах математики часто

приходится слышать о детях, с которыми проделывают

ужасные вещи: перемещают, сохраняют, удаляют и даже

убивают. К счастью, в этом разделе мы будем их всего лишь

выбирать\! 

В BeautifulSoup, как и во многих других библиотеках, существует различие между *детьми* и *потомками*: как и в

генеалогическом древе любого человека, дети всегда

располагаются ровно на один уровень ниже родителей, тогда

как потомки могут находиться на любом уровне дерева ниже

родителя. Скажем, теги tr являются детьми тега table, а теги

tr, th, td, img и span — потомками тега table \(по крайней

мере, в нашем примере\). Все дети — потомки, но не все

потомки — дети. 

В целом функции BeautifulSoup всегда имеют дело с

потомками тега, выбранного в данный момент. Например, функция bs.body.h1 выбирает первый тег h1, который

является потомком тега body. Она не найдет теги, расположенные за пределами body. 

Аналогично функция bs.div.find\_all\('img'\) найдет

первый тег div в до кументе, а затем извлечет список всех

тегов img, которые являются потомками этого тега div. 

Получить только тех потомков, которые являются детьми, можно с помощью тега .children:

from urllib.request import urlopen

from bs4 import BeautifulSoup

 

html 

=

urlopen\('http://www.pythonscraping.com/pages/pa

ge3.html'\)

bs = BeautifulSoup\(html, 'html.parser'\)

 

for 

child 

in 

bs.find\('table', 

\{'id':'giftList'\}\).children:

print\(child\)

Данный код выводит список всех строк таблицы giftList, в том числе начальную строку с заголовками столбцов. Если

вместо функции children\(\) в этом коде использовать

функцию desndants\(\), то она найдет в таблице и выведет

примерно два десятка тегов, включая img, span и отдельные

теги td. Определенно имеет смысл различать детей и

потомков\! 

**Работа с братьями и сестрами**

Функция 

next\_siblings\(\) 

библиотеки 

BeautifulSoup

упрощает сбор данных из таблиц, особенно если в таблице есть

![Image 17](images/000022.png)

заголовки:

from urllib.request import urlopen

from bs4 import BeautifulSoup

 

html 

=

urlopen\('http://www.pythonscraping.com/pages/pa

ge3.html'\)

bs = BeautifulSoup\(html, 'html.parser'\)

 

for 

sibling 

in 

bs.find\('table', 

\{'id':'giftList'\}\).tr.next\_siblings:

print\(sibling\)

Этот код должен выводить все строки таблицы, кроме

первой с заголовком. Почему пропускается строка заголовка? 

Потому что объект не может быть сиблингом сам себе. Каждый

раз, когда составляется список сиблингов \(братьев и сестер\) объекта, сам объект не включается в этот список. Как следует

из названия, данная функция выбирает только *следующих* по

списку сиблингов. Например, если выбрать строку, расположенную в середине таблицы, и вызвать для нее

функцию next\_siblings, то функция вернет только тех

сиблингов, которые идут в списке после данной строки. Таким

образом, выбрав строку заголовка и вызвав функцию

next\_siblings, мы получим все строки таблицы, кроме

самой строки заголовка. 

**Конкретизируйте свой выбор**

Приведенный выше код будет работать ничуть не хуже, если

выбрать первую строку таблицы как bs.table.tr или даже просто

bs.tr. Однако в коде я не поленилась выразить все это в более

длинной форме:

bs.find\('table',\{'id':'giftList'\}\).tr

Даже если на странице только одна таблица \(или другой

интересующий нас тег\), можно легко что-то упустить. Кроме

того, макеты страниц постоянно меняются. Элемент

определенного типа, который когда-то стоял на странице

первым, может в любой момент стать вторым или третьим. 

Чтобы сделать скрапер более надежным, лучше делать выбор

тегов как можно более точным. По возможности используйте

атрибуты тегов. 

У функции next\_siblings есть парная функция

previous\_siblings. Она часто бывает полезна, если в конце

списка одноуровневых тегов, который вы хотели бы получить, есть легко выбираемый тег. 

И конечно же, существуют функции next\_sibling и

previous\_sibling, которые выполняют почти то же, что и

next\_siblings 

и 

previous\_siblings, 

но 

только

возвращают не список тегов, а лишь один тег. 

**Работа с родителями**

При сборе данных со страниц вы, скорее всего, быстро

поймете, что выбирать родительский тег необходимо реже, чем

детей или сиблингов. Как правило, просмотр HTML-страницы с

целью поиска данных мы начинаем с тегов верхнего уровня, 

![Image 18](images/000058.png)

![Image 19](images/000030.png)

![Image 20](images/000067.png)

![Image 21](images/000015.png)

после чего ищем способ углубиться в нужный фрагмент

данных. Однако иногда встречаются странные ситуации, когда

приходится использовать функции поиска родительских

элементов .parent и .parents из библиотеки BeautifulSoup. 

Например:

from urllib.request import urlopen

from bs4 import BeautifulSoup

 

html 

=

urlopen\('http://www.pythonscraping.com/pages/pa

ge3.html'\)

bs = BeautifulSoup\(html, 'html.parser'\)

print\(bs.find\('img', 

\{'src':'../img/gifts/img1.jpg'\}\)

.parent.previous\_sibling.get\_text\(\)\)

Этот код будет выводить цену объекта, изображенного на

картинке ../img/gifts/img1.jpg \(в данном случае цена

составляет 15 долларов\). 

Как это работает? Ниже представлена древовидная

структура фрагмента HTML-страницы, с которой мы работаем, и пошаговый алгоритм:

<tr> 

— td

— td

— td 

— "$15.00" 

— td 

— <img src="../img/gifts/img1.jpg"> 

![Image 22](images/000047.png)

![Image 23](images/000074.png)

![Image 24](images/000026.png)

![Image 25](images/000054.png)

 

Выбираем 

тег 

изображения 

с 

атрибутом

src="../img/gifts/img1.jpg". 

Выбираем родителя этого тега \(в данном случае тег td\). 

С помощью функции previous\_sibling выбираем

предыдущего сиблинга этого тега td \(в данном случае тег td, который содержит цену продукта в долларах\). 

Выбираем текст, содержащийся в этом теге, — "$15.00". 

**Регулярные выражения**

Как говорится в старой шутке по информатике, «допустим, есть

некая проблема, которую мы хотим решить с помощью

регулярных выражений. Теперь у нас две проблемы». 

К сожалению, изучение регулярных выражений \(часто

сокращаемых до *regex*\) часто сводится к пространным

таблицам случайных символов, сочетания которых выглядят

как абракадабра. Это отпугивает многих людей, а потом они

решают рабочие задачи и пишут ненужные сложные функции

поиска и фильтрации, хотя можно было бы обойтись всего

одной строкой с регулярным выражением\! 

К счастью для вас, регулярные выражения не так уж трудно

быстро освоить. Чтобы их изучить, достаточно рассмотреть

всего несколько простых примеров и поэкспериментировать с

ними. 

